<<<<<<< HEAD
---
title: "ExamFeatEngineering"
output: html_document
date: "2024-04-30"
---

#ML2 Alina part

```{r dataload}
library(readxl)
Data <- read_excel("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/ML2 - Machine Learning 2/ML2EXAM/Data.xls")
empFactor <- Data
```

```{r}
#install.packages("visdat")
```


```{r packages}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations
# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```


```{r data structure - overview}
library(dplyr)
# Convert all character columns in the data frame 'emp' to factors
empFactor <- empFactor %>%
  mutate(across(where(is.character), factor))
glimpse(empFactor)
```
character string (<chr>): these should be transformed into categorical variables
numeric <dbl>

```{r Missing overview}
library(visdat)
sum(is.na(empFactor))
vis_miss(empFactor, cluster = TRUE) #visdat library
plot_missing(empFactor)
```
- no NA's in variables: reviewId, reviewDateTime, ratingOverall, ratingWorkLifeBalance, ratingCultureAndValues, -	ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits, lengthOfEmployment
- NA's from 0% - 5%: employmentStatus
- NA's from 5% - 10%: jobTitle.text
- NA's from 10% - 40%: ratingRecommendToFriend, isCurrentJob, location.name
- NA's from 40% -> : jobEndingYear


## reviewId: Unique identifier for each review

```{r reviewId}
str(empFactor$reviewId)
empFactor$reviewId <- NULL
```

## reviewDateTime: Timestamp of when the review was submitted

```{r reviewDateTime }
str(empFactor$reviewDateTime)
```
follows the standard ISO 8601 format: YYYY-MM-DDTHH:MM:SS.fff, where:
YYYY represents the year,
MM the month,
DD the day,
T is a separator (indicating the start of the time portion),
HH the hour (in 24-hour format),
MM the minutes,
SS the seconds,
fff the milliseconds.
- given this interpretation, the information after the T-separator is evaluated as being redundant. Therefore the variable is being transformed into a numeric obtaining the Year variable

```{r Create year}
library(lubridate) 
# Convert 'reviewDateTime' from character to POSIXct format (if not already)
empFactor$reviewDateTime <- ymd_hms(empFactor$reviewDateTime)

# Extract the year and convert it to numeric format
empFactor$reviewYear <- year(empFactor$reviewDateTime)
```
reviewYear is created being a numeric with range 2013->2024.

```{r Factorize reviewYear}
empFactor$reviewYear <- factor(empFactor$reviewYear)
```

```{r EDA reviewYear }
library(tidyverse)
library(DataExplorer)
library(ggplot2)

# Create a bar plot
ggplot(empFactor, aes(x = as.factor(reviewYear))) +
  geom_bar(stat = "count", fill = "blue", color = "black") +  # Count is default, explicitly stating for clarity
  labs(x = "Review Year", y = "Number of Reviews", title = "Distribution of Reviews by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better visibility if needed

summary(empFactor$reviewYear)
```
The reviewDateTime variable has been transformed into
reviewYear
- a categorical variable having the vast majority of observations from the year 2023. The next coming years 2022 and 2024 do also represent a fair amount of the observations. The years including 2021 and before, are bearing a very minor part of the years, as these reviews could be consideres deleted.

## ratingOverall: Overall rating given to the company by the employee
This is the variable to predict based on the other features

```{r Factorize/ordering ratingOverall}
empFactor$ratingOverall <- factor(empFactor$ratingOverall)
summary(empFactor$ratingOverall)
# Convert to an ordinal factor
empFactor$ratingOverall <- factor(empFactor$ratingOverall, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
```
Making the variable ordinal scaled: ordered = TRUE

```{r Histogram ratingOverall }
histogram(empFactor$ratingOverall)

# Load the ggplot2 library
library(ggplot2)

# Create a histogram of ratingOverall
ggplot(empFactor, aes(x = ratingOverall)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black", binwidth = 1) +
  labs(x = "Overall Rating", y = "Frequency", title = "Distribution of Overall Ratings") +
  theme_minimal() +
  scale_x_discrete(limits = c("1", "2", "3", "4", "5"))  # Ensure that all rating levels are shown even if some have 0 counts
```
Discuss wether ratingOverall yields the best insights being numeric or being a factor?
Do we want to obtain RMSE measures or Accuracy measures? 

COMMON ANSWER: make into ordinal scale

#DEALING WITH MISSING

```{r isCurrentJob replacing NAs with 0}
library(dplyr)
# Replace NA values with 0s
empFactor <- empFactor %>% 
  mutate(isCurrentJob = replace_na(isCurrentJob, 0))
empFactor$isCurrentJob <- factor(empFactor$isCurrentJob)
summary(empFactor$isCurrentJob)
```
isCurrentJob:
- the 39% missing values are being evaluated as "Informative missing values" (Kuhn and Johnson 2013). As the variable only contains 1s and NAs, these NAs are indeed an informative missing value which requires being transformed into 0s. 

Deleting variables: jobEndingYear,jobTitle.text,location.name

jobEndingYear
- deleting this variable due to the 61% Missings. As support for this deletion, the variable isCurrentJob is evaluated as having an adequate extent of information given in jobEndingYear. 

jobTitle.text
- Deleting this variable as we have a huge amount of different jobtitles, namely 2733 various instances, whereas these do not contribute with anything but noise to the dataset.

location.name
- Deleting this variable as we have a huge amount of different location anmes, namely 1201 various instances, whereas these do not contribute with anything but noise to the dataset.

```{r Delete jobEndingYear;jobTitle.text;location.name }
empFactor <- subset(empFactor, select = -c(jobEndingYear))
empFactor <- subset(empFactor, select = -c(jobTitle.text))
empFactor <- subset(empFactor, select = -c(location.name))
```

Deleting NAs for the two variables  and prevent making imputations, because it is assumed that it is easier to impute a value there is numeric compared to a variable there are based on your feelings and opinions
```{r Delete NAs ratingCeo;ratingBusinessOutlook;employmentStatus;ratingRecommendToFriend }
empFactor <- empFactor[!is.na(empFactor$ratingCeo), ]
empFactor <- empFactor[!is.na(empFactor$ratingBusinessOutlook), ]
empFactor <- empFactor[!is.na(empFactor$employmentStatus), ]
empFactor <- empFactor[!is.na(empFactor$ratingRecommendToFriend), ]

sum(is.na(empFactor))
```

From Exploratory Data Analysis is seen in the dataset, that 310 respondents is scoring ratingOverall with a value from 3-5 and at the same score 
- ratingCareerOpportunities,
- ratingCompensationAndBenefits,
- ratingCultureAndValues,
- ratingDiversityAndInclusion,
- ratingSeniorLeadership,
- ratingWorkLifeBalance
to zero. This is assessed being a flawed respondent, as it is assumed that this respondent does not take a stand on the above listed question-categories but still rates the ratingOverall with high scores.
```{r deleting instances having 0s in several categorical 0-5 and ratingOverall 3-5}
# Since there's still some giving 0's another subset of data will be made, deleting all the rows where 
# the respondant answered 0
empFactor <- empFactor[!(empFactor$ratingCareerOpportunities == 0 |
                         empFactor$ratingCompensationAndBenefits == 0 |
                         empFactor$ratingCultureAndValues == 0 |
                         empFactor$ratingDiversityAndInclusion == 0 |
                         empFactor$ratingSeniorLeadership == 0 |
                         empFactor$ratingWorkLifeBalance == 0), ]
plot_histogram(empFactor)
```

ratingCeo, ratingBusinessOutlook, ratingRecommendToFriend, employmentStatus
- factorizing these variables into categorical variables.

ratingWorkLifeBalance, ratingCultureAndValues, ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits
- Transforming these variables into ordinal scaled categorical variables as these levels are assessed having an ranking feature embedded.

lengthOfEmployment
- scaling lenght of employment

```{r Factorize/Ordering/Scaling variables}
empFactor$ratingCeo <- factor(empFactor$ratingCeo)
empFactor$ratingBusinessOutlook <- factor(empFactor$ratingBusinessOutlook)
empFactor$ratingRecommendToFriend <- factor(empFactor$ratingRecommendToFriend)
empFactor$employmentStatus <- factor(empFactor$employmentStatus)

empFactor$ratingWorkLifeBalance <- factor(empFactor$ratingWorkLifeBalance, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCultureAndValues <- factor(empFactor$ratingCultureAndValues, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingDiversityAndInclusion <- factor(empFactor$ratingDiversityAndInclusion, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingSeniorLeadership <- factor(empFactor$ratingSeniorLeadership, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCareerOpportunities <- factor(empFactor$ratingCareerOpportunities, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCompensationAndBenefits <- factor(empFactor$ratingCompensationAndBenefits, levels = c(1, 2, 3, 4, 5), ordered = TRUE)

empFactor$lengthOfEmployment <- scale(empFactor$lengthOfEmployment)

glimpse(empFactor)
```

# MODEL DEVELOPMENT

```{r Boysen}
library(rsample)
set.seed(123) # Set a random seed for replication purposes
job_split <- initial_split(emp, prop = 0.80, strata = "ratingOverall")
job_train  <- training(job_split)
job_test   <- testing(job_split)
```


```{r Ralles draft1}
#Tree model
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(ratingOverall ~., job_train,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model
```


```{r Ralles draft1 - Tree}
library(caTools)
real.pred <- job_test$ratingOverall 
tree.class.pred <- predict(tree.model, 
                           job_test, type = "raw") 
tree.scoring <- predict(tree.model, 
                        job_test, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 
tree.auc
```
Output shows the distinction between how the different classes are predicted in comparison to each other

```{r Ch 8_Lab Rcode.R}
library(tree)
# recode sales as a binary var and incorporate to the dataset
#High=ifelse(Carseats$Sales<=8,"0","1")
#Carseats=data.frame(Carseats,High)
#str(Carseats)
#Carseats$High = as.factor(Carseats$High)

# set a classsification tree
tree.carseats = tree(High ~ .-Sales,Carseats, split = c("deviance", "gini"))
summary(tree.carseats)


# plot tree, disply labels (text) and category names (pretty)
plot(tree.carseats)
text(tree.carseats,pretty=0)

# read the tree
tree.carseats

# estimate the test error 
RNGkind("L'Ecuyer-CMRG")
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuraccy: (87+56)/200 = 0.71; test error = 0.29
# it you get slightly different results, it is because of the split

# pruning the tree
RNGkind("L'Ecuyer-CMRG")
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass) # FUN: classsification error rate guides the cross-validation and pruning process; default: deviance 
names(cv.carseats)
cv.carseats
# in the output, despite the name, $dev is the cross-validation error rate
# size = numer of terminal nodes of each tree considered
# $k = cost-complexity parameter (alpha in the slides)

par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# the trees with 19 terminal nodes results in the lowest cross-validation error rate.
# this is the most complex model;  
# as an example, if we wish to prune the tree 
# let us select best = 13 

# plot pruned tree
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats)
text(prune.carseats,pretty=0)

# estimate test error 
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuracy: (78+62)/200 =0.7; test error rate = 0.3
# here, pruning did not improve the accuracy.

# ps. our solution is not the same as in the textbook because of the random split.
# the instability of tree solutions when working with small datasets is acknowledged.
```

```{r Tree model}
# 3) Decision trees 
# training parameters
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(best_seller ~., train.data,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model

# confusion matrix + KAPPA 
real.pred <- test.data$best_seller 
tree.class.pred <- predict(tree.model, 
                           test.data, type = "raw") 
tree.scoring <- predict(tree.model, 
                        test.data, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 

modelComparison = rbind(modelComparison, 
                        data.frame(model = 'Basic Tree', 
                                   accuracy = tree.conf[[3]][[1]], 
                                   kappa = tree.conf[[3]][[2]], 
                                   precision = tree.conf[[4]][[5]], 
                                   recall_sensitivity = tree.conf[[4]][[6]], 
                                   specificity = tree.conf[[4]][[2]], 
                                   auc = tree.auc))
```


```{r Random Forest}
# 2) random forest
rf.model <- train(ratingOverall ~ ., job_train,
                  method = "rf", 
                  ntree = 1000,
                  metric = "Kappa",
                  trControl = train.param,
                  tune.grid = expand.grid(.mtry=c(1:28)))

rf.model

# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
rfmodel.class.pred <- predict(rf.model, 
                              job_test, 
                              type = "raw") 
rfmodel.scoring <- predict(rf.model, 
                           job_test, 
                           type = "prob") [, "1"] 
rfmodel.conf <- confusionMatrix(data = rfmodel.class.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall") 

# ROC and AUC
rf.auc = colAUC(rfmodel.scoring, real.pred, plotROC = TRUE)
```


```{r Random Forest}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'RF tree', 
                                   accuracy = rfmodel.conf[[3]][[1]], 
                                   kappa = rfmodel.conf[[3]][[2]], 
                                   precision = rfmodel.conf[[4]][[5]], 
                                   recall_sensitivity = rfmodel.conf[[4]][[6]], 
                                   specificity = rfmodel.conf[[4]][[2]], 
                                   auc = rf.auc))
```



```{r XGBoost }
model.xgboost <- train(ratingOverall ~ ., job_train,
                       method = "xgbTree",
                       metric = "Kappa",
                       tuneGrid = expand.grid(max_depth=3:6,
                                              gamma=c(0, 1, 2, 3, 5), 
                                              eta=c(0.03, 0.06, 0.1, 0.2), 
                                              nrounds=300,
                                              subsample=0.5, 
                                              colsample_bytree=0.1, 
                                              min_child_weight = 1),
                       trControl = train.param)
model.xgboost



# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
xgb.class.pred <- predict(model.xgboost, 
                          job_test, 
                          type = "raw") 
xgb.scoring <- predict(model.xgboost, 
                       job_test, 
                       type = "prob") [, "1"] 
xgb.conf <- confusionMatrix(data = xgb.class.pred, 
                            reference = real.pred, 
                            positive = "1", 
                            mode = "prec_recall") 

# ROC and AUC
xgb.auc = colAUC(xgb.scoring, real.pred, plotROC = TRUE)
```

```{r XGBoost}
xgb.conf
```


```{r XGBoost}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'xgb tree', 
                                   accuracy = xgb.conf[[3]][[1]], 
                                   kappa = xgb.conf[[3]][[2]], 
                                   precision = xgb.conf[[4]][[5]], 
                                   recall_sensitivity = xgb.conf[[4]][[6]], 
                                   specificity = xgb.conf[[4]][[2]], 
                                   auc = xgb.auc))
```

```{r SVM }
library(caret)
# Tune and fit an SVM with a radial basis kernel (C and sigma as hyperparameters)
# below we use caret’s train() function to tune and train an SVM using the radial basis 
# kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.

# Tune an SVM with radial basis kernel 
set.seed(1854)  
churn_svm <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# Plot results
ggplot(churn_svm) + theme_light()


# Print results
churn_svm$results 
# notice the default is accuracy
```


```{r SVM with trControl}
# RE-run with trContol to get the class probabilities for AUC/ROC     
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
set.seed(5628)  
churn_svm_auc <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC",  # explicitly set area under ROC curve as criteria        
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(churn_svm_auc)
# interpret
```

=======
---
title: "ExamFeatEngineering"
output: html_document
date: "2024-04-30"
---

#ML2 Alina part

```{r dataload}
library(readxl)
Data <- read_excel("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/ML2 - Machine Learning 2/ML2EXAM/Data.xls")
empFactor <- Data
```

```{r}
#install.packages("visdat")
```


```{r packages}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations
# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```


```{r data structure - overview}
library(dplyr)
# Convert all character columns in the data frame 'emp' to factors
empFactor <- empFactor %>%
  mutate(across(where(is.character), factor))
glimpse(empFactor)
```
character string (<chr>): these should be transformed into categorical variables
numeric <dbl>

```{r Missing overview}
library(visdat)
sum(is.na(empFactor))
vis_miss(empFactor, cluster = TRUE) #visdat library
plot_missing(empFactor)
```
- no NA's in variables: reviewId, reviewDateTime, ratingOverall, ratingWorkLifeBalance, ratingCultureAndValues, -	ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits, lengthOfEmployment
- NA's from 0% - 5%: employmentStatus
- NA's from 5% - 10%: jobTitle.text
- NA's from 10% - 40%: ratingRecommendToFriend, isCurrentJob, location.name
- NA's from 40% -> : jobEndingYear


## reviewId: Unique identifier for each review

```{r reviewId}
str(empFactor$reviewId)
empFactor$reviewId <- NULL
```

## reviewDateTime: Timestamp of when the review was submitted

```{r reviewDateTime }
str(empFactor$reviewDateTime)
```
follows the standard ISO 8601 format: YYYY-MM-DDTHH:MM:SS.fff, where:
YYYY represents the year,
MM the month,
DD the day,
T is a separator (indicating the start of the time portion),
HH the hour (in 24-hour format),
MM the minutes,
SS the seconds,
fff the milliseconds.
- given this interpretation, the information after the T-separator is evaluated as being redundant. Therefore the variable is being transformed into a numeric obtaining the Year variable

```{r Create year}
library(lubridate) 
# Convert 'reviewDateTime' from character to POSIXct format (if not already)
empFactor$reviewDateTime <- ymd_hms(empFactor$reviewDateTime)

# Extract the year and convert it to numeric format
empFactor$reviewYear <- year(empFactor$reviewDateTime)
```
reviewYear is created being a numeric with range 2013->2024.

```{r Factorize reviewYear}
empFactor$reviewYear <- factor(empFactor$reviewYear)
```

```{r EDA reviewYear }
library(tidyverse)
library(DataExplorer)
library(ggplot2)

# Create a bar plot
ggplot(empFactor, aes(x = as.factor(reviewYear))) +
  geom_bar(stat = "count", fill = "blue", color = "black") +  # Count is default, explicitly stating for clarity
  labs(x = "Review Year", y = "Number of Reviews", title = "Distribution of Reviews by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better visibility if needed

summary(empFactor$reviewYear)
```
The reviewDateTime variable has been transformed into
reviewYear
- a categorical variable having the vast majority of observations from the year 2023. The next coming years 2022 and 2024 do also represent a fair amount of the observations. The years including 2021 and before, are bearing a very minor part of the years, as these reviews could be consideres deleted.

## ratingOverall: Overall rating given to the company by the employee
This is the variable to predict based on the other features

```{r Factorize/ordering ratingOverall}
empFactor$ratingOverall <- factor(empFactor$ratingOverall)
summary(empFactor$ratingOverall)
# Convert to an ordinal factor
empFactor$ratingOverall <- factor(empFactor$ratingOverall, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
```
Making the variable ordinal scaled: ordered = TRUE

```{r Histogram ratingOverall }
histogram(empFactor$ratingOverall)

# Load the ggplot2 library
library(ggplot2)

# Create a histogram of ratingOverall
ggplot(empFactor, aes(x = ratingOverall)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black", binwidth = 1) +
  labs(x = "Overall Rating", y = "Frequency", title = "Distribution of Overall Ratings") +
  theme_minimal() +
  scale_x_discrete(limits = c("1", "2", "3", "4", "5"))  # Ensure that all rating levels are shown even if some have 0 counts
```
Discuss wether ratingOverall yields the best insights being numeric or being a factor?
Do we want to obtain RMSE measures or Accuracy measures? 

COMMON ANSWER: make into ordinal scale

#DEALING WITH MISSING

```{r isCurrentJob replacing NAs with 0}
library(dplyr)
# Replace NA values with 0s
empFactor <- empFactor %>% 
  mutate(isCurrentJob = replace_na(isCurrentJob, 0))
empFactor$isCurrentJob <- factor(empFactor$isCurrentJob)
summary(empFactor$isCurrentJob)
```
isCurrentJob:
- the 39% missing values are being evaluated as "Informative missing values" (Kuhn and Johnson 2013). As the variable only contains 1s and NAs, these NAs are indeed an informative missing value which requires being transformed into 0s. 

Deleting variables: jobEndingYear,jobTitle.text,location.name

jobEndingYear
- deleting this variable due to the 61% Missings. As support for this deletion, the variable isCurrentJob is evaluated as having an adequate extent of information given in jobEndingYear. 

jobTitle.text
- Deleting this variable as we have a huge amount of different jobtitles, namely 2733 various instances, whereas these do not contribute with anything but noise to the dataset.

location.name
- Deleting this variable as we have a huge amount of different location anmes, namely 1201 various instances, whereas these do not contribute with anything but noise to the dataset.

```{r Delete jobEndingYear;jobTitle.text;location.name }
empFactor <- subset(empFactor, select = -c(jobEndingYear))
empFactor <- subset(empFactor, select = -c(jobTitle.text))
empFactor <- subset(empFactor, select = -c(location.name))
```

Deleting NAs for the two variables  and prevent making imputations, because it is assumed that it is easier to impute a value there is numeric compared to a variable there are based on your feelings and opinions
```{r Delete NAs ratingCeo;ratingBusinessOutlook;employmentStatus;ratingRecommendToFriend }
empFactor <- empFactor[!is.na(empFactor$ratingCeo), ]
empFactor <- empFactor[!is.na(empFactor$ratingBusinessOutlook), ]
empFactor <- empFactor[!is.na(empFactor$employmentStatus), ]
empFactor <- empFactor[!is.na(empFactor$ratingRecommendToFriend), ]

sum(is.na(empFactor))
```

From Exploratory Data Analysis is seen in the dataset, that 310 respondents is scoring ratingOverall with a value from 3-5 and at the same score 
- ratingCareerOpportunities,
- ratingCompensationAndBenefits,
- ratingCultureAndValues,
- ratingDiversityAndInclusion,
- ratingSeniorLeadership,
- ratingWorkLifeBalance
to zero. This is assessed being a flawed respondent, as it is assumed that this respondent does not take a stand on the above listed question-categories but still rates the ratingOverall with high scores.
```{r deleting instances having 0s in several categorical 0-5 and ratingOverall 3-5}
# Since there's still some giving 0's another subset of data will be made, deleting all the rows where 
# the respondant answered 0
empFactor <- empFactor[!(empFactor$ratingCareerOpportunities == 0 |
                         empFactor$ratingCompensationAndBenefits == 0 |
                         empFactor$ratingCultureAndValues == 0 |
                         empFactor$ratingDiversityAndInclusion == 0 |
                         empFactor$ratingSeniorLeadership == 0 |
                         empFactor$ratingWorkLifeBalance == 0), ]
plot_histogram(empFactor)
```

ratingCeo, ratingBusinessOutlook, ratingRecommendToFriend, employmentStatus
- factorizing these variables into categorical variables.

ratingWorkLifeBalance, ratingCultureAndValues, ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits
- Transforming these variables into ordinal scaled categorical variables as these levels are assessed having an ranking feature embedded.

lengthOfEmployment
- scaling lenght of employment

```{r Factorize/Ordering/Scaling variables}
empFactor$ratingCeo <- factor(empFactor$ratingCeo)
empFactor$ratingBusinessOutlook <- factor(empFactor$ratingBusinessOutlook)
empFactor$ratingRecommendToFriend <- factor(empFactor$ratingRecommendToFriend)
empFactor$employmentStatus <- factor(empFactor$employmentStatus)

empFactor$ratingWorkLifeBalance <- factor(empFactor$ratingWorkLifeBalance, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCultureAndValues <- factor(empFactor$ratingCultureAndValues, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingDiversityAndInclusion <- factor(empFactor$ratingDiversityAndInclusion, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingSeniorLeadership <- factor(empFactor$ratingSeniorLeadership, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCareerOpportunities <- factor(empFactor$ratingCareerOpportunities, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCompensationAndBenefits <- factor(empFactor$ratingCompensationAndBenefits, levels = c(1, 2, 3, 4, 5), ordered = TRUE)

empFactor$lengthOfEmployment <- scale(empFactor$lengthOfEmployment)

glimpse(empFactor)
```

# MODEL DEVELOPMENT

```{r Boysen}
library(rsample)
set.seed(123) # Set a random seed for replication purposes
job_split <- initial_split(emp, prop = 0.80, strata = "ratingOverall")
job_train  <- training(job_split)
job_test   <- testing(job_split)
```


```{r Ralles draft1}
#Tree model
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(ratingOverall ~., job_train,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model
```


```{r Ralles draft1 - Tree}
library(caTools)
real.pred <- job_test$ratingOverall 
tree.class.pred <- predict(tree.model, 
                           job_test, type = "raw") 
tree.scoring <- predict(tree.model, 
                        job_test, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 
tree.auc
```
Output shows the distinction between how the different classes are predicted in comparison to each other

```{r Ch 8_Lab Rcode.R}
library(tree)
# recode sales as a binary var and incorporate to the dataset
#High=ifelse(Carseats$Sales<=8,"0","1")
#Carseats=data.frame(Carseats,High)
#str(Carseats)
#Carseats$High = as.factor(Carseats$High)

# set a classsification tree
tree.carseats = tree(High ~ .-Sales,Carseats, split = c("deviance", "gini"))
summary(tree.carseats)


# plot tree, disply labels (text) and category names (pretty)
plot(tree.carseats)
text(tree.carseats,pretty=0)

# read the tree
tree.carseats

# estimate the test error 
RNGkind("L'Ecuyer-CMRG")
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuraccy: (87+56)/200 = 0.71; test error = 0.29
# it you get slightly different results, it is because of the split

# pruning the tree
RNGkind("L'Ecuyer-CMRG")
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass) # FUN: classsification error rate guides the cross-validation and pruning process; default: deviance 
names(cv.carseats)
cv.carseats
# in the output, despite the name, $dev is the cross-validation error rate
# size = numer of terminal nodes of each tree considered
# $k = cost-complexity parameter (alpha in the slides)

par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# the trees with 19 terminal nodes results in the lowest cross-validation error rate.
# this is the most complex model;  
# as an example, if we wish to prune the tree 
# let us select best = 13 

# plot pruned tree
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats)
text(prune.carseats,pretty=0)

# estimate test error 
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuracy: (78+62)/200 =0.7; test error rate = 0.3
# here, pruning did not improve the accuracy.

# ps. our solution is not the same as in the textbook because of the random split.
# the instability of tree solutions when working with small datasets is acknowledged.
```

```{r Tree model}
# 3) Decision trees 
# training parameters
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(best_seller ~., train.data,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model

# confusion matrix + KAPPA 
real.pred <- test.data$best_seller 
tree.class.pred <- predict(tree.model, 
                           test.data, type = "raw") 
tree.scoring <- predict(tree.model, 
                        test.data, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 

modelComparison = rbind(modelComparison, 
                        data.frame(model = 'Basic Tree', 
                                   accuracy = tree.conf[[3]][[1]], 
                                   kappa = tree.conf[[3]][[2]], 
                                   precision = tree.conf[[4]][[5]], 
                                   recall_sensitivity = tree.conf[[4]][[6]], 
                                   specificity = tree.conf[[4]][[2]], 
                                   auc = tree.auc))
```


```{r Random Forest}
# 2) random forest
rf.model <- train(ratingOverall ~ ., job_train,
                  method = "rf", 
                  ntree = 1000,
                  metric = "Kappa",
                  trControl = train.param,
                  tune.grid = expand.grid(.mtry=c(1:28)))

rf.model

# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
rfmodel.class.pred <- predict(rf.model, 
                              job_test, 
                              type = "raw") 
rfmodel.scoring <- predict(rf.model, 
                           job_test, 
                           type = "prob") [, "1"] 
rfmodel.conf <- confusionMatrix(data = rfmodel.class.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall") 

# ROC and AUC
rf.auc = colAUC(rfmodel.scoring, real.pred, plotROC = TRUE)
```


```{r Random Forest}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'RF tree', 
                                   accuracy = rfmodel.conf[[3]][[1]], 
                                   kappa = rfmodel.conf[[3]][[2]], 
                                   precision = rfmodel.conf[[4]][[5]], 
                                   recall_sensitivity = rfmodel.conf[[4]][[6]], 
                                   specificity = rfmodel.conf[[4]][[2]], 
                                   auc = rf.auc))
```



```{r XGBoost }
model.xgboost <- train(ratingOverall ~ ., job_train,
                       method = "xgbTree",
                       metric = "Kappa",
                       tuneGrid = expand.grid(max_depth=3:6,
                                              gamma=c(0, 1, 2, 3, 5), 
                                              eta=c(0.03, 0.06, 0.1, 0.2), 
                                              nrounds=300,
                                              subsample=0.5, 
                                              colsample_bytree=0.1, 
                                              min_child_weight = 1),
                       trControl = train.param)
model.xgboost



# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
xgb.class.pred <- predict(model.xgboost, 
                          job_test, 
                          type = "raw") 
xgb.scoring <- predict(model.xgboost, 
                       job_test, 
                       type = "prob") [, "1"] 
xgb.conf <- confusionMatrix(data = xgb.class.pred, 
                            reference = real.pred, 
                            positive = "1", 
                            mode = "prec_recall") 

# ROC and AUC
xgb.auc = colAUC(xgb.scoring, real.pred, plotROC = TRUE)
```

```{r XGBoost}
xgb.conf
```


```{r XGBoost}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'xgb tree', 
                                   accuracy = xgb.conf[[3]][[1]], 
                                   kappa = xgb.conf[[3]][[2]], 
                                   precision = xgb.conf[[4]][[5]], 
                                   recall_sensitivity = xgb.conf[[4]][[6]], 
                                   specificity = xgb.conf[[4]][[2]], 
                                   auc = xgb.auc))
```

```{r SVM }
library(caret)
# Tune and fit an SVM with a radial basis kernel (C and sigma as hyperparameters)
# below we use caret’s train() function to tune and train an SVM using the radial basis 
# kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.

# Tune an SVM with radial basis kernel 
set.seed(1854)  
churn_svm <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# Plot results
ggplot(churn_svm) + theme_light()


# Print results
churn_svm$results 
# notice the default is accuracy
```


```{r SVM with trControl}
# RE-run with trContol to get the class probabilities for AUC/ROC     
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
set.seed(5628)  
churn_svm_auc <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC",  # explicitly set area under ROC curve as criteria        
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(churn_svm_auc)
# interpret
```

>>>>>>> 63dc1c65cf50349f1b453a9fccae7d737922855f
