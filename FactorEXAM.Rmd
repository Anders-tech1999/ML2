<<<<<<< HEAD
---
title: "ExamFeatEngineering"
output: html_document
date: "2024-04-30"
---

#ML2 Alina part

```{r dataload}
library(readxl)
Data <- read_excel("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/ML2 - Machine Learning 2/ML2EXAM/Data.xls")
empFactor <- Data
```

```{r}
#install.packages("visdat")
```


```{r packages}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations
# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```


```{r data structure - overview}
library(dplyr)
# Convert all character columns in the data frame 'emp' to factors
empFactor <- empFactor %>%
  mutate(across(where(is.character), factor))
glimpse(empFactor)
```
character string (<chr>): these should be transformed into categorical variables
numeric <dbl>

```{r Missing overview}
library(visdat)
sum(is.na(empFactor))
vis_miss(empFactor, cluster = TRUE) #visdat library
plot_missing(empFactor)
```
- no NA's in variables: reviewId, reviewDateTime, ratingOverall, ratingWorkLifeBalance, ratingCultureAndValues, -	ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits, lengthOfEmployment
- NA's from 0% - 5%: employmentStatus
- NA's from 5% - 10%: jobTitle.text
- NA's from 10% - 40%: ratingRecommendToFriend, isCurrentJob, location.name
- NA's from 40% -> : jobEndingYear


## reviewId: Unique identifier for each review

```{r reviewId}
str(empFactor$reviewId)
empFactor$reviewId <- NULL
```

## reviewDateTime: Timestamp of when the review was submitted

```{r reviewDateTime }
str(empFactor$reviewDateTime)
```
follows the standard ISO 8601 format: YYYY-MM-DDTHH:MM:SS.fff, where:
YYYY represents the year,
MM the month,
DD the day,
T is a separator (indicating the start of the time portion),
HH the hour (in 24-hour format),
MM the minutes,
SS the seconds,
fff the milliseconds.
- given this interpretation, the information after the T-separator is evaluated as being redundant. Therefore the variable is being transformed into a numeric obtaining the Year variable

```{r Create year}
library(lubridate) 
# Convert 'reviewDateTime' from character to POSIXct format (if not already)
empFactor$reviewDateTime <- ymd_hms(empFactor$reviewDateTime)

# Extract the year and convert it to numeric format
empFactor$reviewYear <- year(empFactor$reviewDateTime)
```
reviewYear is created being a numeric with range 2013->2024.

```{r Factorize reviewYear}
empFactor$reviewYear <- factor(empFactor$reviewYear)
```

```{r delete origin variable}
empFactor$reviewDateTime <- NULL 
```

```{r EDA reviewYear }
library(tidyverse)
library(DataExplorer)
library(ggplot2)

# Create a bar plot
ggplot(empFactor, aes(x = as.factor(reviewYear))) +
  geom_bar(stat = "count", fill = "blue", color = "black") +  # Count is default, explicitly stating for clarity
  labs(x = "Review Year", y = "Number of Reviews", title = "Distribution of Reviews by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better visibility if needed

summary(empFactor$reviewYear)
```
The reviewDateTime variable has been transformed into
reviewYear
- a categorical variable having the vast majority of observations from the year 2023. The next coming years 2022 and 2024 do also represent a fair amount of the observations. The years including 2021 and before, are bearing a very minor part of the years, as these reviews could be consideres deleted.

## ratingOverall: Overall rating given to the company by the employee
This is the variable to predict based on the other features

```{r Factorize/ordering ratingOverall}
empFactor$ratingOverall <- factor(empFactor$ratingOverall)
summary(empFactor$ratingOverall)
# Convert to an ordinal factor
empFactor$ratingOverall <- factor(empFactor$ratingOverall, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
```
Making the variable ordinal scaled: ordered = TRUE

```{r Histogram ratingOverall }
histogram(empFactor$ratingOverall)

# Load the ggplot2 library
library(ggplot2)

# Create a histogram of ratingOverall
ggplot(empFactor, aes(x = ratingOverall)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black", binwidth = 1) +
  labs(x = "Overall Rating", y = "Frequency", title = "Distribution of Overall Ratings") +
  theme_minimal() +
  scale_x_discrete(limits = c("1", "2", "3", "4", "5"))  # Ensure that all rating levels are shown even if some have 0 counts
```
Discuss wether ratingOverall yields the best insights being numeric or being a factor?
Do we want to obtain RMSE measures or Accuracy measures? 

Discretizing ratingOverall into 2 new variables
```{r low and highratingOverall}
empFactor$HighratingOverall_binary <- factor(ifelse(empFactor$ratingOverall < 3, "0", "1"))

head(empFactor$HighratingOverall_binary)
```
REMEMBER to use ONLY ONE FORMAT of the ratingOverall variable

```{r if asked for numeric ratingOverall}
empFactor$numericratingOverall <- as.numeric(empFactor$ratingOverall)
```
REMEMBER to use ONLY ONE FORMAT of the ratingOverall variable


#DEALING WITH MISSING

```{r isCurrentJob replacing NAs with 0}
library(dplyr)
# Replace NA values with 0s
empFactor <- empFactor %>% 
  mutate(isCurrentJob = replace_na(isCurrentJob, 0))
empFactor$isCurrentJob <- factor(empFactor$isCurrentJob)
summary(empFactor$isCurrentJob)
```
isCurrentJob:
- the 39% missing values are being evaluated as "Informative missing values" (Kuhn and Johnson 2013). As the variable only contains 1s and NAs, these NAs are indeed an informative missing value which requires being transformed into 0s. 

Deleting variables: jobEndingYear,jobTitle.text,location.name

jobEndingYear
- deleting this variable due to the 61% Missings. As support for this deletion, the variable isCurrentJob is evaluated as having an adequate extent of information given in jobEndingYear. 

jobTitle.text
- Deleting this variable as we have a huge amount of different jobtitles, namely 2733 various instances, whereas these do not contribute with anything but noise to the dataset.

location.name
- Deleting this variable as we have a huge amount of different location anmes, namely 1201 various instances, whereas these do not contribute with anything but noise to the dataset.

```{r Delete jobEndingYear;jobTitle.text;location.name }
empFactor <- subset(empFactor, select = -c(jobEndingYear))
empFactor <- subset(empFactor, select = -c(jobTitle.text))
empFactor <- subset(empFactor, select = -c(location.name))
```

Deleting NAs for the two variables  and prevent making imputations, because it is assumed that it is easier to impute a value there is numeric compared to a variable there is based on your feelings and opinions
```{r Delete NAs ratingCeo;ratingBusinessOutlook;employmentStatus;ratingRecommendToFriend }
empFactor <- empFactor[!is.na(empFactor$ratingCeo), ]
empFactor <- empFactor[!is.na(empFactor$ratingBusinessOutlook), ]
empFactor <- empFactor[!is.na(empFactor$employmentStatus), ]
empFactor <- empFactor[!is.na(empFactor$ratingRecommendToFriend), ]

sum(is.na(empFactor))
```

From Exploratory Data Analysis is seen in the dataset, that 310 respondents is scoring ratingOverall with a value from 3-5 and at the same score 
- ratingCareerOpportunities,
- ratingCompensationAndBenefits,
- ratingCultureAndValues,
- ratingDiversityAndInclusion,
- ratingSeniorLeadership,
- ratingWorkLifeBalance
to zero. This is assessed being a flawed respondent, as it is assumed that this respondent does not take a stand on the above listed question-categories but still rates the ratingOverall with high scores.
```{r deleting instances having 0s in several categorical 0-5 and ratingOverall 3-5}
# Since there's still some giving 0's another subset of data will be made, deleting all the rows where 
# the respondant answered 0
empFactor <- empFactor[!(empFactor$ratingCareerOpportunities == 0 |
                         empFactor$ratingCompensationAndBenefits == 0 |
                         empFactor$ratingCultureAndValues == 0 |
                         empFactor$ratingDiversityAndInclusion == 0 |
                         empFactor$ratingSeniorLeadership == 0 |
                         empFactor$ratingWorkLifeBalance == 0), ]
plot_histogram(empFactor)
```

ratingCeo, ratingBusinessOutlook, ratingRecommendToFriend, employmentStatus
- factorizing these variables into categorical variables.

ratingWorkLifeBalance, ratingCultureAndValues, ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits
- Transforming these variables into ordinal scaled categorical variables as these levels are assessed having an ranking feature embedded.

lengthOfEmployment
- scaling lenght of employment

```{r Factorize/Ordering/Scaling variables}
empFactor$ratingCeo <- factor(empFactor$ratingCeo)
empFactor$ratingBusinessOutlook <- factor(empFactor$ratingBusinessOutlook)
empFactor$ratingRecommendToFriend <- factor(empFactor$ratingRecommendToFriend)
empFactor$employmentStatus <- factor(empFactor$employmentStatus)

empFactor$ratingWorkLifeBalance <- factor(empFactor$ratingWorkLifeBalance, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCultureAndValues <- factor(empFactor$ratingCultureAndValues, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingDiversityAndInclusion <- factor(empFactor$ratingDiversityAndInclusion, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingSeniorLeadership <- factor(empFactor$ratingSeniorLeadership, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCareerOpportunities <- factor(empFactor$ratingCareerOpportunities, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCompensationAndBenefits <- factor(empFactor$ratingCompensationAndBenefits, levels = c(1, 2, 3, 4, 5), ordered = TRUE)

#empFactor$lengthOfEmployment <- scale(empFactor$lengthOfEmployment)

dim(empFactor)
glimpse(empFactor)
```

```{r}
empFactorBinary <- empFactor
empFactorBinary$ratingOverall <- NULL

dim(empFactorBinary)
glimpse(empFactorBinary)
```


# Data Preparation

```{r datasplit standard}
library(rsample)
set.seed(123) # Set a random seed for replication purposes
split <- initial_split(empFactor, prop = 0.70, strata = "ratingOverall")
train  <- training(split)
test   <- testing(split)
dim(train)
dim(test)
```

```{r Recipe standard }
#Creating the blueprint
recipe <- recipe(ratingOverall ~ ., data = empFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(), threshold = 0.05, other = "Other") %>%  #lumping if needed
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) # using NZV last removes X factor levels with near zero variance
recipe

#Preparing the blueprint based on training data
prepare <- prep(recipe, training = subtrain)
prepare

#baking: applying the recipe to new data / test data
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
```

If computation is heavy
```{r downsizing datasplit }
set.seed(123)
random_indices <- sample(1:nrow(empFactorBinary), 1000)
subempFactor <- empFactorBinary[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "HighratingOverall_binary") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r Recipe subset }
#Creating the blueprint
subrecipe <- recipe(HighratingOverall_binary ~ ., data = subempFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(), threshold = 0.05, other = "Other") %>%  #lumping if needed
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) # using NZV last removes X factor levels with near zero variance
subrecipe

#Preparing the blueprint based on training data
subprepare <- prep(subrecipe, training = subtrain)
subprepare

#baking: applying the recipe to new data / test data
subbaked_train <- bake(subprepare, new_data = subtrain)
subbaked_test <- bake(subprepare, new_data = subtest)
```

```{r dimensions baked train/test}
#New datasets
dim(subbaked_train)
dim(subbaked_test)
```

#Model Development

## 8.3.1 Fitting Classification Trees (ISLRp.353)'

In this case the whole dataset is used.

```{r Fit tree}
#install.packages("ISLR2")
library(tree)
library(ISLR2)
#using carseats as pseudocode
tree.carseats <- tree(HighratingOverall_binary ~ ., empFactorBinary)
summary(tree.carseats)
```
Variables actually used in tree construction:
[1] "ratingRecommendToFriend"   "ratingSeniorLeadership"   
[3] "ratingCareerOpportunities" "ratingCultureAndValues"

Number of terminal nodes:  6
- Terminal nodes, or leaf nodes, are the end points of the tree where predictions are made. 
- The number of terminal nodes can give you an idea of the complexity of the model: more nodes generally mean a more complex model.

Residual mean deviance:  0.3889 = 3153 / 8107 
- a small deviance indicates a tree that provides a good fit to the
(training) data. 
- the residual mean deviance reported is simply the deviance divided by n − |T0|

Misclassification error rate: 0.08776 = 712 / 8113 
- training error rate is 0.08776, which means that approximately 8.776% of the predictions made by the tree were incorrect. 
- this is calculated as 712 misclassified observations out of a total of 813 observations used in the model.

use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
```{r tree plot}
plot(tree.carseats)
text(tree.carseats , pretty = 0)
```
Rating Recommend To Friend: NEGATIVE
- the root node of the tree, where the first decision is made. It indicates that the initial split in the dataset was made based on the predictor "ratingRecommendToFriend," specifically when the rating is NEGATIVE. 
- this decision or split was deemed most informative in predicting the target variable at this stage.

Deeper in the tree other variables are presented with decisions

```{r Tree metrics and measures}
tree.carseats
```
1) root 8113 7307.0 1 ( 0.166523 0.833477 )
8113: all observations in df
7307.0: deviance
1: the majority class (which is Highoverallrating)
( 0.166523 0.833477 ): probability of Highoverallrating

2) ratingRecommendToFriend: NEGATIVE 2495 3457.0 0 ( 0.511824 0.488176 )  
2): first split
ratingRecommendToFriend: NEGATIVE: the tree first splits the data based on whether this rating is NEGATIVE
2495: observation quantity, has fewer compared to the root because it only includes cases where the condition is true.
3457.0: deviance here is smaller compared to the root, indicating a better fit for these observations under this condition.

4) ratingSeniorLeadership: 1 1237 1368.0 0 ( 0.758286 0.241714 )
- subsequent nodes are based on other ratings like "ratingSeniorLeadership" and "ratingCareerOpportunities," with specific levels indicated (e.g., "1" or "2,3,4,5").

We extract this above table to gain accurate information about the decision tree nodes


Evaluating performance of a classification tree on these data, the test error is estimated. Splitting the observations into a training set and a test set, building the tree using the training set, and evaluate its performance on the test data. 
The predict() function can be used for this purpose. In the case
of a classification tree, the argument type = "class" instructs R to return the actual class prediction. 
```{r Tree test predictions}
set.seed(123)

treetrain <- sample(1:nrow(empFactorBinary), 800)
empFactorBinary.test <- empFactorBinary[-treetrain , ]
High.test <- empFactorBinary$HighratingOverall_binary[-treetrain]

tree.empFactorBinary <- tree(HighratingOverall_binary ~ ., data = empFactorBinary, subset = treetrain)
tree.pred <- predict(tree.empFactorBinary, newdata = empFactorBinary.test, type = "class")

table(tree.pred, High.test)
((948+5663)/(948+5663+445+257))
```
correct predictions for 90,4% of the locations in the test data set

###8.3.1 Pruning
Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α)

```{r Prune output}
set.seed(123)
cv.carseats <- cv.tree(tree.carseats , FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```
The output is assessed by comparing the node "size" to the deviance of these exact nodes
- in this case nodes 6 and 3 have the same deviance 753
- Proceeding to gain test error measures

```{r Prune plot: node size + k complexity parameter}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```
Choosing the Optimal Tree Size (deviance///size): 
- these plots are used to select the optimal tree size (number of terminal nodes). You typically look for the tree size corresponding to the lowest point on the deviance plot or the smallest tree size

Complexity Parameter (k): 
- the plot of k vs. deviance helps in understanding how sensitive the tree is to the pruning process. A sharp increase in deviance as k increases indicates a point beyond which the tree loses significant predictive accuracy

```{r}
prune.carseats <- prune.misclass(tree.carseats , best = 3)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
```

```{r}
tree.pred <- predict(prune.carseats , empFactorBinary.test,type = "class")
table(tree.pred, High.test)
((840+5844)/(840+5844+264+367))
```
correct predictions for 91,37% of the locations in the test data set WITH PRUNING the decision tree. The test accuracy gains 0.0097 percentagepoints by using pruning having 3 terminal nodes instead of 6. (0.9137-0.9040 = 0.0097)






#From Classification_2024

```{r Ralles draft1}
#Tree model
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(ratingOverall ~., job_train,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model
```


```{r Ralles draft1 - Tree}
library(caTools)
real.pred <- job_test$ratingOverall 
tree.class.pred <- predict(tree.model, 
                           job_test, type = "raw") 
tree.scoring <- predict(tree.model, 
                        job_test, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 
tree.auc
```
Output shows the distinction between how the different classes are predicted in comparison to each other

```{r Ch 8_Lab Rcode.R}
library(tree)
# recode sales as a binary var and incorporate to the dataset
#High=ifelse(Carseats$Sales<=8,"0","1")
#Carseats=data.frame(Carseats,High)
#str(Carseats)
#Carseats$High = as.factor(Carseats$High)

# set a classsification tree
tree.carseats = tree(High ~ .-Sales,Carseats, split = c("deviance", "gini"))
summary(tree.carseats)


# plot tree, disply labels (text) and category names (pretty)
plot(tree.carseats)
text(tree.carseats,pretty=0)

# read the tree
tree.carseats

# estimate the test error 
RNGkind("L'Ecuyer-CMRG")
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuraccy: (87+56)/200 = 0.71; test error = 0.29
# it you get slightly different results, it is because of the split

# pruning the tree
RNGkind("L'Ecuyer-CMRG")
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass) # FUN: classsification error rate guides the cross-validation and pruning process; default: deviance 
names(cv.carseats)
cv.carseats
# in the output, despite the name, $dev is the cross-validation error rate
# size = numer of terminal nodes of each tree considered
# $k = cost-complexity parameter (alpha in the slides)

par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# the trees with 19 terminal nodes results in the lowest cross-validation error rate.
# this is the most complex model;  
# as an example, if we wish to prune the tree 
# let us select best = 13 

# plot pruned tree
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats)
text(prune.carseats,pretty=0)

# estimate test error 
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuracy: (78+62)/200 =0.7; test error rate = 0.3
# here, pruning did not improve the accuracy.

# ps. our solution is not the same as in the textbook because of the random split.
# the instability of tree solutions when working with small datasets is acknowledged.
```

```{r Tree model}
# 3) Decision trees 
# training parameters
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(best_seller ~., train.data,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model

# confusion matrix + KAPPA 
real.pred <- test.data$best_seller 
tree.class.pred <- predict(tree.model, 
                           test.data, type = "raw") 
tree.scoring <- predict(tree.model, 
                        test.data, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 

modelComparison = rbind(modelComparison, 
                        data.frame(model = 'Basic Tree', 
                                   accuracy = tree.conf[[3]][[1]], 
                                   kappa = tree.conf[[3]][[2]], 
                                   precision = tree.conf[[4]][[5]], 
                                   recall_sensitivity = tree.conf[[4]][[6]], 
                                   specificity = tree.conf[[4]][[2]], 
                                   auc = tree.auc))
```


```{r Random Forest}
# 2) random forest
rf.model <- train(ratingOverall ~ ., job_train,
                  method = "rf", 
                  ntree = 1000,
                  metric = "Kappa",
                  trControl = train.param,
                  tune.grid = expand.grid(.mtry=c(1:28)))

rf.model

# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
rfmodel.class.pred <- predict(rf.model, 
                              job_test, 
                              type = "raw") 
rfmodel.scoring <- predict(rf.model, 
                           job_test, 
                           type = "prob") [, "1"] 
rfmodel.conf <- confusionMatrix(data = rfmodel.class.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall") 

# ROC and AUC
rf.auc = colAUC(rfmodel.scoring, real.pred, plotROC = TRUE)
```


```{r Random Forest}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'RF tree', 
                                   accuracy = rfmodel.conf[[3]][[1]], 
                                   kappa = rfmodel.conf[[3]][[2]], 
                                   precision = rfmodel.conf[[4]][[5]], 
                                   recall_sensitivity = rfmodel.conf[[4]][[6]], 
                                   specificity = rfmodel.conf[[4]][[2]], 
                                   auc = rf.auc))
```



```{r XGBoost }
model.xgboost <- train(ratingOverall ~ ., job_train,
                       method = "xgbTree",
                       metric = "Kappa",
                       tuneGrid = expand.grid(max_depth=3:6,
                                              gamma=c(0, 1, 2, 3, 5), 
                                              eta=c(0.03, 0.06, 0.1, 0.2), 
                                              nrounds=300,
                                              subsample=0.5, 
                                              colsample_bytree=0.1, 
                                              min_child_weight = 1),
                       trControl = train.param)
model.xgboost



# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
xgb.class.pred <- predict(model.xgboost, 
                          job_test, 
                          type = "raw") 
xgb.scoring <- predict(model.xgboost, 
                       job_test, 
                       type = "prob") [, "1"] 
xgb.conf <- confusionMatrix(data = xgb.class.pred, 
                            reference = real.pred, 
                            positive = "1", 
                            mode = "prec_recall") 

# ROC and AUC
xgb.auc = colAUC(xgb.scoring, real.pred, plotROC = TRUE)
```

```{r XGBoost}
xgb.conf
```


```{r XGBoost}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'xgb tree', 
                                   accuracy = xgb.conf[[3]][[1]], 
                                   kappa = xgb.conf[[3]][[2]], 
                                   precision = xgb.conf[[4]][[5]], 
                                   recall_sensitivity = xgb.conf[[4]][[6]], 
                                   specificity = xgb.conf[[4]][[2]], 
                                   auc = xgb.auc))
```

```{r SVM }
library(caret)
# Tune and fit an SVM with a radial basis kernel (C and sigma as hyperparameters)
# below we use caret’s train() function to tune and train an SVM using the radial basis 
# kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.

# Tune an SVM with radial basis kernel 
set.seed(1854)  
churn_svm <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# Plot results
ggplot(churn_svm) + theme_light()


# Print results
churn_svm$results 
# notice the default is accuracy
```


```{r SVM with trControl}
# RE-run with trContol to get the class probabilities for AUC/ROC     
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
set.seed(5628)  
churn_svm_auc <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC",  # explicitly set area under ROC curve as criteria        
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(churn_svm_auc)
# interpret
```
