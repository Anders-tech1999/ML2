<<<<<<< HEAD

#ML2 Alina part

```{r dataload}
library(readxl)
Data <- read_excel("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/ML2 - Machine Learning 2/ML2EXAM/Data.xls")
empFactor <- Data
```

```{r}
#install.packages("visdat")
```


```{r packages}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations
# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks

library(DataExplorer) # for data exploration
library(caTools)      # Data splits
library(rsample)      # for resampling procedures
library(Hmisc)        # Missing data handling
library(Metrics)#      # Performance measures
library(reshape2)
library(rpart)        # for fitting decision trees
library(ipred)        # for fitting bagged decision trees #used in bagging in addition to random forest e.g. to compare bagging vs rf (but RF will dominate)
library(randomForest) # for fitting bagged decision trees
library(ranger)       # a c++ implementation of random forest 
library(h2o)          # a java-based random forest
library(modeldata)#    # for data set Job attrition
library(kernlab)      # also for fitting SVMs 
library(pdp)          # for partial dependence plots, etc.
library(forecast)#     # Library for performance evaluation
library(gbm)          # For Gradient Boosting models
library(pROC)         # For AUC
library(ROCR)         # For ROC AUC
library(kernlab)
```


```{r data structure - overview}
library(dplyr)
# Convert all character columns in the data frame 'emp' to factors
empFactor <- empFactor %>%
  mutate(across(where(is.character), factor))
glimpse(empFactor)
```
character string (<chr>): these should be transformed into categorical variables
numeric <dbl>

```{r Missing overview}
library(visdat)
sum(is.na(empFactor))
vis_miss(empFactor, cluster = TRUE) #visdat library
plot_missing(empFactor)
```
- no NA's in variables: reviewId, reviewDateTime, ratingOverall, ratingWorkLifeBalance, ratingCultureAndValues, -	ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits, lengthOfEmployment
- NA's from 0% - 5%: employmentStatus
- NA's from 5% - 10%: jobTitle.text
- NA's from 10% - 40%: ratingRecommendToFriend, isCurrentJob, location.name
- NA's from 40% -> : jobEndingYear


## reviewId: Unique identifier for each review

```{r reviewId}
str(empFactor$reviewId)
empFactor$reviewId <- NULL
```

## reviewDateTime: Timestamp of when the review was submitted

```{r reviewDateTime }
str(empFactor$reviewDateTime)
```
follows the standard ISO 8601 format: YYYY-MM-DDTHH:MM:SS.fff, where:
YYYY represents the year,
MM the month,
DD the day,
T is a separator (indicating the start of the time portion),
HH the hour (in 24-hour format),
MM the minutes,
SS the seconds,
fff the milliseconds.
- given this interpretation, the information after the T-separator is evaluated as being redundant. Therefore the variable is being transformed into a numeric obtaining the Year variable

```{r Create year}
library(lubridate) 
# Convert 'reviewDateTime' from character to POSIXct format (if not already)
empFactor$reviewDateTime <- ymd_hms(empFactor$reviewDateTime)

# Extract the year and convert it to numeric format
empFactor$reviewYear <- year(empFactor$reviewDateTime)
```
reviewYear is created being a numeric with range 2013->2024.

```{r Factorize reviewYear}
empFactor$reviewYear <- factor(empFactor$reviewYear)
```

```{r delete origin variable}
empFactor$reviewDateTime <- NULL 
```

```{r EDA reviewYear }
library(tidyverse)
library(DataExplorer)
library(ggplot2)

# Create a bar plot
ggplot(empFactor, aes(x = as.factor(reviewYear))) +
  geom_bar(stat = "count", fill = "blue", color = "black") +  # Count is default, explicitly stating for clarity
  labs(x = "Review Year", y = "Number of Reviews", title = "Distribution of Reviews by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better visibility if needed

summary(empFactor$reviewYear)
```
The reviewDateTime variable has been transformed into
reviewYear
- a categorical variable having the vast majority of observations from the year 2023. The next coming years 2022 and 2024 do also represent a fair amount of the observations. The years including 2021 and before, are bearing a very minor part of the years, as these reviews could be consideres deleted.

## ratingOverall: Overall rating given to the company by the employee
This is the variable to predict based on the other features

Making the variable ordinal scaled: ordered = TRUE
```{r Factorize/ordering ratingOverall}
empFactor$ratingOverall <- factor(empFactor$ratingOverall)
summary(empFactor$ratingOverall)
# Convert to an ordinal factor
empFactor$ratingOverall <- factor(empFactor$ratingOverall, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
```

```{r Histogram ratingOverall }
histogram(empFactor$ratingOverall)

# Load the ggplot2 library
library(ggplot2)

# Create a histogram of ratingOverall
ggplot(empFactor, aes(x = ratingOverall)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black", binwidth = 1) +
  labs(x = "Overall Rating", y = "Frequency", title = "Distribution of Overall Ratings") +
  theme_minimal() +
  scale_x_discrete(limits = c("1", "2", "3", "4", "5"))  # Ensure that all rating levels are shown even if some have 0 counts
```
Discuss wether ratingOverall yields the best insights being numeric or being a factor?
Do we want to obtain RMSE measures or Accuracy measures? 

Discretizing ratingOverall into new variable
```{r low and highratingOverall}
empFactor$HighratingOverallBinary <- factor(ifelse(empFactor$ratingOverall < 3, "0", "1"))

head(empFactor$HighratingOverallBinary)

#Deleting on er the other DV
#empFactor$ratingOverall <- NULL 
#empFactor$HighratingOverall_binary <- NULL 
```
REMEMBER to use ONLY ONE FORMAT of the ratingOverall variable

```{r}
empFactor  <- empFactor %>%
  mutate(ratingOverall = case_when(
    ratingOverall >= 1 & ratingOverall <= 3 ~ 1,
    ratingOverall == 4 ~ 2,
    ratingOverall == 5 ~ 3,
    TRUE ~ NA_real_
  ))
```


#DEALING WITH MISSING

```{r isCurrentJob replacing NAs with 0}
library(dplyr)
# Replace NA values with 0s
empFactor <- empFactor %>% 
  mutate(isCurrentJob = replace_na(isCurrentJob, 0))
empFactor$isCurrentJob <- factor(empFactor$isCurrentJob)
summary(empFactor$isCurrentJob)
```
isCurrentJob:
- the 39% missing values are being evaluated as "Informative missing values" (Kuhn and Johnson 2013). As the variable only contains 1s and NAs, these NAs are indeed an informative missing value which requires being transformed into 0s. 

Deleting variables: jobEndingYear,jobTitle.text,location.name

jobEndingYear
- deleting this variable due to the 61% Missings. As support for this deletion, the variable isCurrentJob is evaluated as having an adequate extent of information given in jobEndingYear. 

jobTitle.text
- Deleting this variable as we have a huge amount of different jobtitles, namely 2733 various instances, whereas these do not contribute with anything but noise to the dataset.

```{r count of unique jobTitle.text}
unique_job_titles <- unique(empFactor$jobTitle.text)

# Count the number of unique job titles
number_of_unique_job_titles <- length(unique_job_titles)
number_of_unique_job_titles
```

location.name
- Deleting this variable as we have a huge amount of different location anmes, namely 1201 various instances, whereas these do not contribute with anything but noise to the dataset.
```{r count of unique location.name}
unique_location.name <- unique(empFactor$location.name)

# Count the number of unique job titles
number_of_unique_location.name <- length(unique_location.name)
number_of_unique_location.name
```

```{r Delete jobEndingYear;jobTitle.text;location.name }
empFactor <- subset(empFactor, select = -c(jobEndingYear))
empFactor <- subset(empFactor, select = -c(jobTitle.text))
empFactor <- subset(empFactor, select = -c(location.name))
```

Deleting NAs for the two variables  and prevent making imputations, because it is assumed that it is easier to impute a value there is numeric compared to a variable there is based on your feelings and opinions
```{r Delete NAs ratingCeo;ratingBusinessOutlook;employmentStatus;ratingRecommendToFriend }
empFactor <- empFactor[!is.na(empFactor$ratingCeo), ]
empFactor <- empFactor[!is.na(empFactor$ratingBusinessOutlook), ]
empFactor <- empFactor[!is.na(empFactor$employmentStatus), ]
empFactor <- empFactor[!is.na(empFactor$ratingRecommendToFriend), ]

sum(is.na(empFactor))
```

ratingCeo, ratingBusinessOutlook, ratingRecommendToFriend, employmentStatus
- factorizing these variables into categorical variables.

ratingWorkLifeBalance, ratingCultureAndValues, ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits
- Transforming these variables into ordinal scaled categorical variables as these levels are assessed having an ranking feature embedded.

lengthOfEmployment
- scaling lenght of employment

```{r Factorize/Ordering/Scaling variables}
empFactor$ratingCeo <- factor(empFactor$ratingCeo)

empFactor$ratingBusinessOutlook <- factor(empFactor$ratingBusinessOutlook)

empFactor$ratingRecommendToFriend <- factor(empFactor$ratingRecommendToFriend)

empFactor$employmentStatus <- factor(empFactor$employmentStatus)
###
empFactor$ratingWorkLifeBalance <- factor(empFactor$ratingWorkLifeBalance, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

empFactor$ratingCultureAndValues <- factor(empFactor$ratingCultureAndValues, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

empFactor$ratingDiversityAndInclusion <- factor(empFactor$ratingDiversityAndInclusion, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

empFactor$ratingSeniorLeadership <- factor(empFactor$ratingSeniorLeadership, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

empFactor$ratingCareerOpportunities <- factor(empFactor$ratingCareerOpportunities, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

empFactor$ratingCompensationAndBenefits <- factor(empFactor$ratingCompensationAndBenefits, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

dim(empFactor)
glimpse(empFactor)
```

```{r Creating Binary df}
empFactorBinary <- empFactor
empFactorBinary$ratingOverall <- NULL

empFactor$HighratingOverallBinary <- NULL

dim(empFactorBinary)
glimpse(empFactorBinary)
```


```{r Binary df IF ASKED FOR NUMERIC IVs}
#IF ASKED FOR NUMERIC PREDICTORS
empFactorBinary$ratingBusinessOutlook <- as.numeric(empFactorBinary$ratingBusinessOutlook)

empFactorBinary$ratingWorkLifeBalance <- as.numeric(empFactorBinary$ratingWorkLifeBalance)

empFactorBinary$ratingCultureAndValues <- as.numeric(empFactorBinary$ratingCultureAndValues)

empFactorBinary$ratingDiversityAndInclusion <- as.numeric(empFactorBinary$ratingDiversityAndInclusion)

empFactorBinary$ratingSeniorLeadership <- as.numeric(empFactorBinary$ratingSeniorLeadership)

empFactorBinary$ratingCareerOpportunities <- as.numeric(empFactorBinary$ratingSeniorLeadership)

empFactorBinary$ratingCompensationAndBenefits <- as.numeric(empFactorBinary$ratingCompensationAndBenefits)

empFactorBinary$isCurrentJob <- as.numeric(empFactorBinary$isCurrentJob)

empFactorBinary$reviewYear <- as.numeric(empFactorBinary$reviewYear)

```

```{r Creating DVnumeric df}
empFactorDVNumeric <- empFactor

empFactorDVNumeric$numericratingOverall <- as.numeric(empFactorDVNumeric$ratingOverall)

empFactorDVNumeric$ratingOverall <- NULL
empFactorDVNumeric$HighratingOverallBinary <- NULL

dim(empFactorDVNumeric)
glimpse(empFactorDVNumeric)
```

```{r}
sum(is.na(empFactor))
sum(is.na(empFactorBinary))
sum(is.na(empFactorDVNumeric))
```


# Data Preparation

```{r datasplit standard}
library(rsample)
set.seed(123) # Set a random seed for replication purposes
split <- initial_split(empFactor, prop = 0.70, strata = "ratingOverall")
train  <- training(split)
test   <- testing(split)
dim(train)
dim(test)
```

```{r Recipe standard }
#Creating the blueprint
recipe <- recipe(ratingOverall ~ ., data = empFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(), threshold = 0.05, other = "Other") %>%  #lumping if needed
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) # using NZV last removes X factor levels with near zero variance
recipe

#Preparing the blueprint based on training data
prepare <- prep(recipe, training = train)
prepare

#baking: applying the recipe to new data / test data
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
```

```{r dim standard recipe}
dim(baked_train)
dim(baked_test)
```


If computation is heavy
Remember to CHOOSE dataset in 
 - random_indices <- sample(1:nrow(EMPFACTORBINARY), 1000)
 - subempFactor <- EMPFACTORBINARY[random_indices, ]
as well as DV in
 - subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "HIGHRATINGOVERALLBINARY")

```{r downsizing datasplit DVBinary }
library(rsample)
set.seed(123)
random_indices <- sample(1:nrow(empFactorBinary), 1000)
subempFactor <- empFactorBinary[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "HighratingOverallBinary") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r downsizing datasplit 5-level DV }
set.seed(123)
random_indices <- sample(1:nrow(empFactor), 1000)
subempFactor <- empFactor[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "ratingOverall") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r downsizing datasplit DVNumeric }
set.seed(123)
random_indices <- sample(1:nrow(empFactorDVNumeric), 1000)
subempFactor <- empFactorDVNumeric[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "numericratingOverall") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

REMEMBER TO CHANGE DV IN RECIPE
```{r Recipe subset }
#Creating the blueprint
subrecipe <- recipe(HighratingOverallBinary ~ ., data = subempFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(),threshold=0.05,other = "Other") %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) #removes factor levels with nzv
subrecipe

#Preparing the blueprint based on training data
subprepare <- prep(subrecipe, training = subtrain)
subprepare

#baking: applying the recipe to new data / test data
subbaked_train <- bake(subprepare, new_data = subtrain)
subbaked_test <- bake(subprepare, new_data = subtest)
```

```{r dimensions baked train/test}
#New datasets
dim(subbaked_train)
dim(subbaked_test)
glimpse(subbaked_train)
```

```{r Unbalancing trainset IF ASKED}
# unbalanced
prop.table(table(subbaked_train$HighratingOverallBinary))

# oversampling
train.data <- ovun.sample(HighratingOverallBinary ~ ., data=subbaked_train, p=0.5, seed=2, method="over")$data

prop.table(table(subbaked_train$HighratingOverallBinary)) # more balanced after oversampling
prop.table(table(subbaked_test$HighratingOverallBinary))
# reflection here (why is balancing necessary; 
# how it helps later in evaluating accuracy; why not used for testing data)
```




#Model Development


## 8.4.9 EXE 9 - fit Classification Trees
EXE 9 - Classification tree - 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. 
This problem involves the OJ data set which is part of the ISLR2 package.
- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
- (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
- (d) Create a plot of the tree, and interpret the results.
- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
- (f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
- (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
- (h) Which tree size corresponds to the lowest cross-validated classification error rate?
- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?


- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

DOING THIS WITH SUBTRAIN SUBTEST

```{r datasplit train test}
dim(subbaked_train)
dim(subbaked_test)
```

- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. 
Use the summary() function to produce summary statistics about the tree, and describe the results obtained. 
What is the training error rate? How many terminal nodes does the tree have?

```{r}
library(tree)
oj.tree = tree(HighratingOverallBinary ~ ., subbaked_train)
summary(oj.tree)
# Notice in the output: The tree only uses two variables: LoyalCH and PriceDiff.
# It has 7 terminal nodes. Training error rate (misclassification error) for the tree is 0.155
```
Classification tree:
Variables actually used in tree construction:
[1] "ratingRecommendToFriend_POSITIVE"
[2] "ratingWorkLifeBalance_1"         
[3] "ratingCareerOpportunities_1"     
[4] "ratingCompensationAndBenefits_3" 
[5] "ratingSeniorLeadership_3"        
[6] "ratingDiversityAndInclusion_4"   
[7] "ratingCultureAndValues_3"        
[8] "ratingCareerOpportunities_3"     
[9] "ratingCultureAndValues_1"

Number of terminal nodes:  12
- Terminal nodes, or leaf nodes, are the end points of the tree where predictions are made. 
- The number of terminal nodes can give you an idea of the complexity of the model: more nodes generally mean a more complex model.

Residual mean deviance: 0.2759 = 189.6 / 687 
- a small deviance indicates a tree that provides a good fit to the
(training) data. 
- the residual mean deviance reported is simply the deviance divided by n − |T0|

Misclassification error rate: 0.06724 = 47 / 699 
- training error rate is 0.06724, which means that approximately 7.143% of the predictions made by the tree were incorrect. 
- this is calculated as 47 misclassified observations out of a total of 699 observations used in the model.


- (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
```{r Tree metrics and measures}
oj.tree
# Let's pick terminal node labeled “10)”
   # - The splitting variable at this node is PriceDiff
   # - The splitting value of this node is 0.065. 
   # - There are 79 points in the subtree below this node. 
   # -The deviance for all points contained in region below this node is 76.79. 
   # - A star (*) in the line denotes that this is in fact a terminal node. 
   # - The prediction at this node is Sales=MM. About 19% points in this node have CH as value of Sales.
   #   Remaining 81% points have MM as value of Sales. 
```
1) root 672 575.70 1 ( 0.15327 0.84673 )  
672: all observations in df
575.70: deviance
1: the majority class (which is Highoverallrating)
( 0.15327 0.84673 ): probability of Highoverallrating

2) ratingRecommendToFriend:NEGATIVE 198 274.50 1 (0.49495 0.50505 )
2): first split
ratingRecommendToFriend: NEGATIVE: the tree first splits the data based on whether this rating is NEGATIVE
198: observation quantity, has fewer compared to the root because it only includes cases where the condition is true.
274.50: deviance here is smaller compared to the root, indicating a better fit for these observations under this condition.

4) ratingWorkLifeBalance: 1,2 134 171.00 0 ( 0.66418 0.33582 )
- subsequent nodes are based on other ratings like "ratingSeniorLeadership" and "ratingCareerOpportunities," with specific levels indicated (e.g., "1" or "2,3,4,5").

We extract this above table to gain accurate information about the decision tree nodes


- (d) Create a plot of the tree, and interpret the results.

use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
```{r Tree Plot}
plot(oj.tree)
text(oj.tree, pretty = 0)
```
Rating Recommend To Friend: NEGATIVE
- the root node of the tree, where the first decision is made. It indicates that the initial split in the dataset was made based on the predictor "ratingRecommendToFriend," specifically when the rating is NEGATIVE. 
- this decision or split was deemed most informative in predicting the target variable at this stage.

Deeper in the tree, other variables are presented with decisions


- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

Evaluating performance of a classification tree on these data, the test error is estimated. Splitting the observations into a training set and a test set, building the tree using the training set, and evaluate its performance on the test data. 
The predict() function can be used for this purpose. In the case
of a classification tree, the argument type = "class" instructs R to return the actual class prediction. 
```{r}
oj.pred = predict(oj.tree, subbaked_test, type = "class")
table(subbaked_test$HighratingOverallBinary, oj.pred)

testerror = (21+252)/(21+252+25+1)
testerror
```
correct predictions for 91,3% of the locations in the test data set

###8.4.9 Prune + CV
- (f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.

Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α)

```{r Prune output}
cv.oj = cv.tree(oj.tree, FUN = prune.tree)
cv.oj
```
The output is assessed by comparing the node "size" to the deviance of these exact nodes
- in this case nodes 4 has the lowest deviance 332.6914  
- Proceeding to gain test error measures


- (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r Prune plot: node size + k complexity parameter}
par(mfrow = c(1, 2))
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
plot(cv.oj$k, cv.oj$dev, type = "b", xlab = "Complex Parameter k", ylab = "Deviance")
```
Choosing the Optimal Tree Size (deviance///size): 
- these plots are used to select the optimal tree size (number of terminal nodes). You typically look for the tree size corresponding to the lowest point on the deviance plot or the smallest tree size

Complexity Parameter (k): 
- the plot of k vs. deviance helps in understanding how sensitive the tree is to the pruning process. A sharp increase in deviance as k increases indicates a point beyond which the tree loses significant predictive accuracy

Based on both plots and size///deviance comparisons:
- 4 nodes are chosen as the optimal Tree complexity

- (h) Which tree size corresponds to the lowest cross-validated classification error rate?

```{r choose quantity of Tree nodes}
#Based on both plots and size///deviance comparisons:
#- 4 nodes are chosen as the optimal Tree complexity
```

- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

```{r PRune Tree plot}
oj.pruned = prune.tree(oj.tree, best = 4)
plot(oj.pruned)
text(oj.pruned, pretty = 0)
```


- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?

```{r PruneTree test predictions }
summary(oj.pruned)
```
The pruned training error 0.3879 is 0.0137 worse than the unpruned training error 0.3143 (0.3879-0.3143 = 0.0137)

- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r Unprune Tree accuracy}
pred.unpruned = predict(oj.tree, subbaked_test, type = "class")
table(subbaked_test$HighratingOverallBinary, pred.unpruned)
testMSEu= (21+254)/(21+254+25+1)
testMSEu
```

```{r Prune Tree accuracy}
pred.pruned = predict(oj.pruned, subbaked_test, type = "class")
table(subbaked_test$HighratingOverallBinary, pred.pruned)
testMSEp= (21+252)/(21+252+21+3)
testMSEp
```
The pruned Tree performs the best prediction

## Alina Simple Tree
```{r}
train.param <- trainControl(method = "cv", number = 5)

tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(HighratingOverallBinary ~., subbaked_train,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 
tree.model
```

```{r}
# confusion matrix + KAPPA 
real.pred <- subbaked_test$HighratingOverallBinary 
tree.class.pred <- predict(tree.model, 
                           subbaked_test, type = "raw") 
tree.scoring <- predict(tree.model, 
                        subbaked_test, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall")
tree.conf
```


```{r}
# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 
```





## 8.4.11 EXE 11 - Boosting DV binary DOES NOT WORK

GO TO LauB BOOSTING

EXE 11 - focus on Boosting - Binary classification problem - 5822 real customer records, (Purchase) indicates whether the customer purchased a caravan insurance policy or not.
- This question uses the Caravan data set.
- (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
- (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
- (c) Use the boosting model to predict the response on the test data.
Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. 
What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

- (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
```{r train for boosting}
dim(subbaked_train)
dim(subbaked_test)
```

- (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. 
Which predictors appear to be the most important?

I get the following error:

"Error in plot.window(xlim, ylim, log = log, ...) : 
  need finite 'xlim' values"
  
When trying to run Boosting...

```{r ERROR Fit Boosting model 1000 trees + skrinkage 0.01}
library(gbm)
set.seed(123)
boost.caravan = gbm(HighratingOverall_binary ~ ., subtrain, n.trees = 500, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)

# Concl: PPERSAUT, MKOOP KLA and MOPLHOOG 
# (that is,Contribution car policies, Purchasing power class, High level education) 
# are three most important variables

summary(Caravan.train$PPERSAUT)
summary(Caravan.train$MKOOPKLA)
summary(Caravan.train$MOPLHOOG)

plot(boost.caravan, i="PPERSAUT")
plot(boost.caravan, i="MKOOPKLA")
plot(boost.caravan, i="MOPLHOOG")
```


- (c) Use the boosting model to predict the response on test data.
Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. 
What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r ERROR boost predicting on test}
boost.prob = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response") # note we use type="response" to retain probabilities ; also, we need to mention again "n.trees = 1000". 
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)

 34/(137 + 34) 
## Precision (TP/P*) is 0.1988. Thus, about 20% of people predicted to make purchase actually end up making one.

# Logistic regression 
lm.caravan = glm(Purchase ~ ., data = Caravan.train, family = binomial)
lm.prob = predict(lm.caravan, Caravan.test, type = "response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, lm.pred)

58/(350 + 58)
## [1] 0.1422. 
## About 14 % of people predicted to make purchase using logistic  regression actually end up making one. 
# The precision is lower than for boosting.

# Note: In applications one can use other popular performance measures (AUC,recall, ...) as discussed in the first part of the curriculum (ISL p. 147-149). 
```




## Alina Random forest
```{r}
train.param <- trainControl(method = "cv", number = 5)

rf.model <- train(HighratingOverallBinary ~ ., subbaked_train,
                  method = "rf", 
                  ntree = 1000,
                  metric = "Kappa",
                  trControl = train.param,
                  tune.grid = expand.grid(.mtry=c(1:28)))

rf.model
```


```{r}
# confusion matrix + KAPPA 
real.pred <- subbaked_test$HighratingOverallBinary 
rfmodel.class.pred <- predict(rf.model, 
                              subbaked_test, 
                              type = "raw") 
rfmodel.scoring <- predict(rf.model, 
                           subbaked_test, 
                           type = "prob") [, "1"] 
rfmodel.conf <- confusionMatrix(data = rfmodel.class.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall") 
rfmodel.conf
```


```{r}
# ROC and AUC
rf.auc = colAUC(rfmodel.scoring, real.pred, plotROC = TRUE) 
```




##LauB RF Random Forest

```{r Define train CV}
ctrl <- trainControl(method = "cv", number = 5, savePredictions = "final")
```

```{r Random Forest }
rf_model <- train(HighratingOverallBinary ~ ., data = subbaked_train, method = "rf", trControl = ctrl)
summary(rf_model)
```
TRUST the process, we only need the confusion matrix!

```{r predictions RF + Accuracy}
# Make predictions
pred_rf <- predict(rf_model, newdata = subbaked_test)

# Evaluate the models' performance
confusionMatrix(pred_rf, subbaked_test$HighratingOverallBinary, positive = "1")
```
Accuracy : 0.9136  

95% CI : (0.876, 0.9428)
 - Range in which the true accuracy of the model is expected to fall 95% of the time.

No Information Rate : 0.8472
 - Accuracy that could be achieved by always predicting the most frequent class.

Kappa : 0.611
 - Measure of agreement between the predicted and true classes, adjusted for chance.
 - Values range from -1 (complete disagreement) to 1 (perfect agreement), with 0 indicating chance agreement.

McNemar's Test P-Value: 0.0032637
 - Statistical test for significant differences between paired proportions (e.g., classifier's performance on different classes).
 - low p-value indicates that the differences are statistically significant.

Sensitivity :  0.98039
 - Ability of the model to correctly identify positive instances.

Specificity : 0.54348      
-  - Ability of the model to correctly identify negative instances.


##LauB RF Random Forest PART2

```{r n features}
# Random Forest
# number of features
n_features <- length(setdiff(names(subbaked_train), "HighratingOverallBinary"))
n_features #13
```


```{r  }
# train a default random forest model using ranger library
library(ranger)
rf1 <- ranger(
  HighratingOverallBinary ~ ., 
  data = subbaked_train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)

# get Out-of-bag RMSE (OOB RMSE)
(default_rmse <- sqrt(rf1$prediction.error)) 
# OOB RMSE = 0.2593047
```


```{r Hyperparameters mtry node.size sample.fraction }
# Tunable hyperparameters that we should consider when training a RF model (the tuneable hyperparameters are "mtry", "node.size", "sample.fraction")
# create hyperparameter grid
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .5)),
  min.node.size = c(1, 3, 5, 10, 15, 20),   #Minimum number of oberservations in each node size #Bigger node size = less computational time, but higher error.
  replace = c(TRUE, FALSE), #bagging with replacement or without, default is with replacement, but ranger allows both
  sample.fraction = c(.5, .63, .8), #Decides the fraction size of each bag used
  rmse = NA                                               
)
```


```{r Hypergrid search }
# execute full cartesian grid search (where we assess every combination of hyperparameters of interest)
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = HighratingOverallBinary ~ ., 
    data            = subbaked_train, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    num.threads     = 8,
    verbose         = TRUE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)
```
Interpretation: 
 - the top 10 models have these features e.g., mtry = 2 (out of 45 predictors), 
 - node-size 5 (it splits until theres 5 observation in the terminal node)
 - replacement is better in half of the models here, as replace = FALSE for top 10 models (Replace = false = Replacement is best)
 - sample fraction is better if we only consider 80% of the original data, 
 - rmse perc_gain = percentage rmse minimized by using this model

TYPE IN THE GRID SEARCH IN THE FOLLOWING
```{r Hypergrid search }
#Train the optimal model, changing values in accordance with the best model obtained from the top 10 above
OptimalModel <- ranger(
  formula = HighratingOverallBinary ~.,
  data = subbaked_train,
  num.trees = 450,
  mtry = 2,
  min.node.size = 5,
  replace = T,
  sample.fraction = 0.8,
  verbose = T,
  seed = 123
)

#Making out of sample predictions
p <- predict(OptimalModel, subbaked_test)
pred <- p$predictions
OptimalModel
```
rmse <- accuracy(pred, baked_test$X0) #stoppet med at virke efter første gang!? use caret package if problems. (giver det overhovedet mening at beregne RMSE for logistic problems?)
rmse #0.7375415 Mean Error


```{r Feature Importance + impurity }
# Feature Importance plot + comparison between impurity and permutation
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = HighratingOverallBinary ~ ., 
  data = subbaked_train, 
  num.trees = 450,  # notice the model is re-run with the optimal hyperparam identified before
  mtry = 2,
  min.node.size =5,
  sample.fraction = 0.8,
  replace = T,
  importance = "impurity",  # based on impurity (Impurity importance, also known as Gini importance, is the default importance measure used in random forests. The impurity-based importance assesses the predictive power of each predictor by measuring how much splitting on that predictor improves the overall purity or homogeneity of the resulting nodes.)
  respect.unordered.factors = "order",
  verbose = T,
  seed  = 123
)
rf_impurity
```


```{r Feature importance permutation }
# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = HighratingOverallBinary ~ ., 
  data = subtrain, 
  num.trees = 450,  # notice the model is re-run with the optimal hyperparam identified before
  mtry = 2,
  min.node.size = 5,
  sample.fraction = 0.8,
  replace = T,
  importance = "permutation",  # based on permutation (It measures the decrease in model performance (e.g., accuracy or mean squared error) when the values of a predictor are randomly permuted while keeping other predictors unchanged)
  respect.unordered.factors = "order",
  verbose = T,
  seed  = 123
) 
rf_permutation
```
Typically, you will not see the same variable importance order between the two options; however, you will often see similar  variables at the top of the plots (and also the bottom).
 - We use impurity, as it is the standard for RF models.

```{r Feature importance Plot impurity }
p1 <- vip::vip(rf_impurity, num_features = 13, bar = FALSE)
#p2 <- vip::vip(rf_permutation, num_features = 13, bar = FALSE)

p1 #because of default for RF
#gridExtra::grid.arrange(p1, p2, nrow = 1)
```


```{r RF Confusion Matrix }
confusionMatrix(
  pred, subbaked_test$HighratingOverallBinary, positive = "1")
```
Accuracy : 0.9136  

95% CI : (0.876, 0.9428)
 - Range in which the true accuracy of the model is expected to fall 95% of the time.

No Information Rate : 0.8472
 - Accuracy that could be achieved by always predicting the most frequent class.

Kappa : 0.611
 - Measure of agreement between the predicted and true classes, adjusted for chance.
 - Values range from -1 (complete disagreement) to 1 (perfect agreement), with 0 indicating chance agreement.

McNemar's Test P-Value: 0.0032637
 - Statistical test for significant differences between paired proportions (e.g., classifier's performance on different classes).
 - low p-value indicates that the differences are statistically significant.

Sensitivity : 0.9804         
 - Ability of the model to correctly identify positive instances.

Specificity : 0.5435                 
 - Ability of the model to correctly identify negative instances.


##LauB Gradient Boosting Model (GBM) 

```{r Gradient Boosting CV}
# Gradient boosting with caret
# Set up the train control object
ctrl <- trainControl(
  method = "cv",       # Cross-validation method
  number = 3,          # Number of folds
  verboseIter = TRUE   # Print progress during training
)

# Train the gradient boosting model
gbmfit <- train(
  HighratingOverallBinary ~ ., # Formula for the model
  data = subbaked_train,        # Training data
  method = "gbm",            # Gradient boosting method
  trControl = ctrl,          # Train control object
  verbose = FALSE            # Suppress verbose output
)
```
Fitting n.trees = 150, interaction.depth = 1, shrinkage = 0.1, n.minobsinnode = 10 on full training set

```{r Confusionmatrix Gradient Boost}
# Getting accuracy and kappa from the model trained using caret
# Obtain predictions from the trained model
gbm_pred <- predict(gbmfit, newdata = subbaked_test)

# Create a confusion matrix
confusion_mat <- confusionMatrix(gbm_pred, subbaked_test$HighratingOverallBinary, positive = "1")
confusion_mat
```
Accuracy : 0.9169            

95% CI : (0.8798, 0.9455)
 - Range in which the true accuracy of the model is expected to fall 95% of the time.

No Information Rate : 0.8472
 - Accuracy that could be achieved by always predicting the most frequent class.

Kappa : 0.6372  
 - Measure of agreement between the predicted and true classes, adjusted for chance.
 - Values range from -1 (complete disagreement) to 1 (perfect agreement), with 0 indicating chance agreement.

McNemar's Test P-Value: 0.0163951 
 - Statistical test for significant differences between paired proportions (e.g., classifier's performance on different classes).
 - low p-value indicates that the differences are statistically significant.

Sensitivity : 0.9765         
 - Ability of the model to correctly identify positive instances.

Specificity : 0.5870                 
 - Ability of the model to correctly identify negative instances.


```{r Calculate AUC }
# Load the pROC package
#install.packages("pROC")
library(pROC)

# Obtain predicted probabilities from your gradient boosting model
predicted_probs <- predict(gbmfit, newdata = subbaked_test, type = "prob")

# Extract the predicted probabilities for the positive class
positive_probs <- predicted_probs[, "1"]

# Create a binary outcome vector (1 for positive class, 0 for negative class)
outcome <- ifelse(subbaked_test$HighratingOverallBinary == "1", 1, 0)

# Calculate the ROC curve and AUC
library(caTools)
roc_obj <- roc(outcome, positive_probs)
plot(roc_obj)
colAUC(predicted_probs[ , 1], as.character(subbaked_test$HighratingOverallBinary), plotROC = T)
```
get the probabilities from the first model we trained
auc_value <- auc(roc_obj) 0.9495737
 - Area Under the Curve is 94,96%

The ROC curve is a graphical representation of the models sensitivity and 1-specificity and to evaluate the performance of a model
predicting a binary variable we can evaluate the Area Under the Curve which represent a number between 0 and 1, where the closer to 1 the better the performance of the model and an AUC of 0.5 is equal to a random guess.

```{r Calculate AUC }
# Confusion Matrix
predicted_val <- predict(gbmfit, subbaked_test)
confusionMatrix(
  predicted_val, subbaked_test$HighratingOverallBinary, positive = "1")
```
Discussion: 
 - The accuracy of the model is 74.42%, which means the model predicts the true positives or true negative correctly 74.42% of the time. We can compare this to the NIR of 70%, which is the ratio of Yes and No in the testing data set. This means that if we simply predicted "no" for all observations, we would still have an accuracy of 70%, why our goal is to maximize the accuracy rate above the no information baseline. The model predicts slightly better with an accuracy of 74.42

sensitivity (True Positive Rate) is an indication of how well the model is recalling the positive class "Yes" and is 33.33% which is a bad performance of recalling the positive class.

specificity is an indication of how well the model is predicting the negative class "No" and is 92%, which is a good performance of predicting the negative class

precision of the model (Pos Pred Value) is a value that represents the precision or the proportion of positive predictions that are  actually true positives which is at 63.83% meaning that 57% of the time, the model correctly predicts a subscribed client.

## Alina XGBoost
```{r}
train.param <- trainControl(method = "cv", number = 3) #3 for computation

model.xgboost <- train(HighratingOverallBinary ~ ., subbaked_train,
                method = "xgbTree",
                metric = "Kappa",
                tuneGrid = expand.grid(max_depth=3:6,
                                      gamma=c(0, 1, 2, 3, 5), 
                                      eta=c(0.03, 0.06, 0.1, 0.2), 
                                      nrounds=300,
                                      subsample=0.5, 
                                      colsample_bytree=0.1, 
                                      min_child_weight = 1),
                            trControl = train.param)
model.xgboost
```


```{r}
# confusion matrix + KAPPA 
real.pred <- subbaked_test$HighratingOverallBinary 
xgb.class.pred <- predict(model.xgboost, 
                          subbaked_test, 
                          type = "raw") 
xgb.scoring <- predict(model.xgboost, 
                       subbaked_test, 
                       type = "prob") [, "1"] 
xgb.conf <- confusionMatrix(data = xgb.class.pred, 
                            reference = real.pred, 
                            positive = "1", 
                            mode = "prec_recall")
xgb.conf
```


```{r}
# ROC and AUC
xgb.auc = colAUC(xgb.scoring, real.pred, plotROC = TRUE) 
```



##LauB Extreme Gradient Boosting (XGBoost) 

```{r Fit XGBoost}
set.seed(123)
train.param <- trainControl(method = "cv", number = 3) 

tune.grid.xgb <- expand.grid(nrounds=300, 
                                 max_depth=c(3,6,9), 
                                 gamma=c(0,1,5,10),
                                 eta=c(0.01, 0.03, 0.1, 0.3),
                                 subsample=0.5, 
                                 colsample_bytree=0.1, 
                                 min_child_weight = 1)

xgbcaret <- train(HighratingOverallBinary ~ ., subbaked_train,
                     method = "xgbTree",
                     tuneGrid = tune.grid.xgb,
                     trControl = train.param)

xgbcaret
xgbcaret$bestTune
xgbcaret$results
```
eta   max_depth  gamma  Accuracy   Kappa    
0.01  3           0     0.9327724  0.7090448


```{r XGBoost plot}
plot(xgbcaret)
```
Shrinkage (Learning Rate):
 - parameter controls the step size at each iteration while moving toward a minimum of the loss function. Common values range from 0 to 0.3.
 - Lower values make the model more robust to overfitting but require more trees.
 
Max Depth:
 - Maximum depth of a tree. This parameter controls the complexity of the model.
 - Higher values allow the model to learn more complex patterns but can lead to overfitting.
 
Minimum Loss Reduction (Gamma):
 - parameter specifies the minimum loss reduction required to make a further partition on a leaf node of the tree.
 - Higher values lead to fewer splits, thus making the model simpler.
 
SUMMARY
 - The plot provides a comprehensive overview of how different hyperparameters interact and affect the model's accuracy. Based on this plot, you can infer that:
 - A maximum depth of 6 or 9 
 - with a shrinkage value around 0.1 and 
 - a gamma value of 0 or 1 tends to perform well.
High gamma values generally reduce accuracy, especially at higher shrinkage values.

```{r XGBoost Variable Importance}
#Feature importance
vip::vip(xgbcaret) 
#Most important features (top 5) are X18, X13, X19, X20 and X16
```
indicates how much each feature contributes to the model's prediction


```{r Predicting XGBoost }
# Obtain predicted probabilities from the XG boost model
predicted_probsXG <- predict(xgbcaret, newdata = subbaked_test, type = "prob")

# Extract the predicted probabilities for the positive class
positive_probsXG <- predicted_probsXG[, "1"]

# Create a binary outcome vector (1 for positive class, 0 for negative class)
outcomeXG <- ifelse(subbaked_test$HighratingOverallBinary == "1", 1, 0)

# Calculate the ROC curve and AUC
roc_objXG <- roc(outcomeXG, positive_probsXG)
roc_objXG
```
Area Under the Curve is 0.9338


```{r Predicting XGBoost }
colAUC(predicted_probsXG[ , 1], as.character(subbaked_test$HighratingOverallBinary), plotROC = T) 
```
The ROC curve is a graphical representation of the models sensitivity and 1-specificity and to evaluate the performance of a model
 - predicting a binary variable we can evaluate the Area Under the Curve which represent a number between 0 and 1, where the closer to 1 the better the performance of the model and an AUC of 0.5 is equal to a random guess.
Hence the AUC of almost 76% is considered a decent performance.

```{r RMSE XGBoost }
#RMSE (does not make sense to calculate on logistic?)
rmse <- RMSE(positive_probsXG, outcomeXG) #0.4126
1-rmse #0.5873
```


```{r Confusionmatrix }
# Confusion Matrix
predicted_val <- predict(xgbcaret, subbaked_test)
confusionMatrix(
  predicted_val, subbaked_test$HighratingOverallBinary, positive = "1")
```
Previous Accuracy : 0.9169
Previous Kappa : 0.6372          

Discussion: 
 - The accuracy of the model is 74.42%, which means the model predicts the true positives or true negative correctly 74.42% of the time. We can compare this to the NIR of 70%, which is the ratio of Yes and No in the testing data set. This means that if we simply predicted "no" for all observations, we would still have an accuracy of 70%, why our goal is to maximize the accuracy rate above the no information baseline. The model predicts slightly better with an accuracy of 74.42

sensitivity (True Positive Rate) is an indication of how well the model is recalling the positive class "Yes" and is 33.33% which is a bad performance of recalling the positive class.

specificity is an indication of how well the model is predicting the negative class "No" and is 92%, which is a good performance of predicting the negative class

precision of the model (Pos Pred Value) is a value that represents the precision or the proportion of positive predictions that are  actually true positives which is at 63.83% meaning that 57% of the time, the model correctly predicts a subscribed client.



#SVM HOM

```{r helper packages}
# Helper packages
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)  # for data splitting
library(modeldata) # for data set Job attrition


# Modeling packages
library(caret)    # meta-engine for SVMs
library(kernlab)  # also for fitting SVMs 

# Model interpretability packages
library(pdp)      # for partial dependence plots, etc.
library(vip)      # for variable importance plots
```

```{r datasplit}
subbaked_trainSVM <- subbaked_train
subbaked_testSVM <- subbaked_test
dim(subbaked_trainSVM)
dim(subbaked_testSVM)
prop.table(table(subbaked_trainSVM$HighratingOverallBinary)) 
```

```{r rescaling DV as getting strange format error}
levels(subbaked_trainSVM$HighratingOverallBinary) <- make.names(levels(subbaked_trainSVM$HighratingOverallBinary), unique=TRUE)

levels(subbaked_trainSVM$HighratingOverallBinary)
```

Tuning and fitting SVM with a radial basis kernel (C and sigma as hyperparameters)
- below we use caret’s train() function to tune and train an SVM using the radial basis 
- kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.
```{r Tune SVM - radial kernel}
set.seed(123)  
churn_svm <- train(
  HighratingOverallBinary ~ ., 
  data = subbaked_trainSVM,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method="cv",number = 10),#first draft
  tuneLength = 10
)
```

```{r Plot SVM}
ggplot(churn_svm) + theme_light()

# Print results
churn_svm$bestTune 
churn_svm$results 
# notice the default is accuracy
```
from Plotting and results from radial kernel SVM, smaller values of cost is beneficial for the accuracy on this dataset case.
- the cost = 0.50 yields: 
- the highest accuracy measure 0.9384
- a sigma value of 0.01541844

```{r}
confusionMatrix(churn_svm)
```
On SVM without probability classes
- Accuracy: 0.9385

We calculate the probability classes to be able to obtain AUC/ROC measurements
```{r Class probabilities to obtain AUC/ROC}
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)
```


```{r Tune SVM with ROC/AUC}
# Tune an SVM
set.seed(123)  
churn_svm_auc <- train(
  HighratingOverallBinary ~ ., 
  data = subbaked_trainSVM,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC", #explicitly set area under ROC curve as criteria
  trControl = ctrl,
  tuneLength = 10
)
```


```{r SVM AUC Confusionmatrix}
confusionMatrix(churn_svm_auc)
# interpret
```
On SVM with probability classes
- Accuracy: 0.9328
This Accuracy is 0.0057 less than the Accuracy obtained without class probabilities.
(0.9385 - 0.9328 = 0.0057)

```{r SVM AUC besttune + results}
churn_svm_auc$bestTune
churn_svm_auc$results
```
from results from radial kernel SVM taking class probabilities into account, smaller values of cost is beneficial for the accuracy on this dataset case.
- the cost = 0.25 yields: 
- the highest ROC measure 0.9574
- a sigma value of 0.01541844
- sensitivity (true positive rate): 0.7372727
- specificity (true negative rate): 0.9679944

This model does a better job predicting 0's compared to 1's due to the sensitivity and specificity rates.

###SVM HOM - Feature Importance

Creating a prediction wrapper function where the reference class of interest is chosen.
Evaluating whether it is preferable to focus on the employees having 
- low scores on HighratingOverallBinary = 0
- high scores on HighratingOverallBinary= 1

From a business perspective, the employees being unsatisfied is the ones we want to focus on at first: Choose 0 as reference
IMPORTANT^^^^^^^^^^^^
```{r Feature Importance - reference class}
#install.packages("kernlab")
library(kernlab)
prob_0 <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "X0"]
}
```

```{r VIP plot}
set.seed(123)  # for reproducibility
vip::vip(churn_svm_auc, method = "permute", event_level = "second", nsim = 5, train = subbaked_trainSVM, target = "HighratingOverallBinary", metric = "roc_auc", reference_class = "X0", pred_wrapper = prob_0)
```
Feature Importance:
 - the permutation importance method shows how much each feature contributes to the model’s performance by measuring the drop in ROC AUC when each feature is randomly shuffled.
 - Features with higher importance scores have a more significant impact on the model’s ability to predict the target variable accurately.

x-axis 
 - represents the importance score of each feature. 
 - Positive values indicate features that increase the model's performance when included
 - negative values suggest a decrease in performance when the feature is permuted.


### SVM HOM - PDP 

use the pdp package to construct PDPs for the top four features according to the permutation-based (ombytning af elementer i en given mængde/kombinationsmulighed/omplacering) variable importance scores (notice we set prob = TRUE in the call to pdp::partial() so that the feature effect plots are on the probability scale; see ?pdp::partial for details). 
Additionally, since the predicted probabilities from our model come in two columns (No/0 and Yes/1), we specify which.class = 1 so that our interpretation is in reference to predicting NO:

CHANGE VARIABLES IN REGARD TO THE VIP PLOT

```{r}
#install.packages("pdp")
library(pdp)
library(gridExtra)
#construct PDP (feature effect plots are on the probability scale)
features <- c("ratingSeniorLeadership_4", "reviewYear_X2023", "ratingSeniorLeadership_2", "reviewYear_X2022")

pdps <- lapply(features, function(x) {
  partial(churn_svm_auc, 
          pred.var = x, 
          which.class = 1, # since the predicted probabilities from our model come in two columns (No=1 and Yes=2), we specify which.class = 1 so that our interpretation is in reference to predicting No
          prob = TRUE, 
          plot = TRUE, 
          plot.engine = "ggplot2") +
    coord_flip()
})
grid.arrange(grobs = pdps,  ncol = 2)
# interpret
```
PDPs show the marginal effect of a single feature on the predicted outcome of a machine learning model while averaging out the effects of all other features.
 - X-axis (yhat): Represents the predicted probability of the outcome (likely the probability of a certain class, e.g., churn).
 - Y-axis (ratingCareerOpportunities): Represents the values of the ratingCareerOpportunities feature.
 - Pattern: The plot shows how changes in ratingCareerOpportunities affect the predicted probability of the outcome. If the line is horizontal, it indicates no effect; otherwise, it shows the nature of the relationship (positive, negative, or non-linear).

#Moving Beyond Linearity

 - go to empNumeric for GAM logistic regression

##LauB GLM Generalized Linear Model


```{r}
dim(subbaked_train)
dim(subbaked_test)
glimpse(subbaked_train)
```


Define train CV
```{r Define train CV}
ctrl <- trainControl(method = "cv", number = 5, savePredictions = "final")
```



```{r Fit GLM }
glm_model <- train(HighratingOverallBinary ~ ., data = subbaked_train, method = "glm", trControl = ctrl)
```
Function Call: Unfortunately, the function call details are marked as NULL, which typically indicates missing output or that the original call wasn't captured.
MIGHT CONVERT INTO NUMERIC TO GET SOMETHING?
```{r Summary GLM }
summary(glm_model)
```

```{r predictions GLM}
# Make predictions
pred_glm <- predict(glm_model, newdata = subbaked_test)

# Evaluate the models' performance
confusionMatrix(pred_glm, subbaked_test$HighratingOverallBinary)
```
Get interpretation from above somewhere^^^


## HOM 7.6 MARS: Multivariate Additive Regression Spline

The MARS method and algorithm can be extended to handle classification problems and GLMs in general.

```{r datasplit}
dim(subbaked_train)
dim(subbaked_test)
glimpse(subbaked_train)
```

Creating hypergrid
Tuning hyperparametrs:
 - the maximum degree of interactions (degree)  
 - the number of terms (i.e., hinge functions determined by the optimal number of knots across all features) (nprune)
 - performing a grid search to identify the optimal combination of hyperparameters that minimize the cv prediction error
```{r Hyper grid}
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)
hyper_grid
```

```{r CV model}
library(earth)
# cross validated model
tuned_mars <- train(
  x = subset(subbaked_train, select = -HighratingOverallBinary),
  y = subbaked_train$HighratingOverallBinary,
  method = "earth",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = hyper_grid
)
```

```{r besttune MARS}
tuned_mars$bestTune
tuned_mars$results
```

```{r Plot CV on Degree//Terms}
ggplot(tuned_mars)
```
The besttune and plot shows:
Maximum degree of interactions (degree)
- the maximum degree for optimizing accuracy is 3 degrees/interactions

Number of terms (i.e., hinge functions determined by the optimal number of knots across all features) (nprune)
- the number of terms optimizing the accuracy are 23

Model Complexity:
 - the optimal number of terms (23) indicates the model complexity.  - having too few terms might underfit, while too many might overfit. The plot helps in understanding the trade-off.

Interaction Terms:
 - degree 1 means that the model does not include interaction terms.  - If higher degrees showed significant improvements in accuracy, you might consider including interactions, but in this case, degree 1 is optimal


## Alina Logit regression
```{r}
train.param <- trainControl(method = "cv", number = 10)

log.model <- train(HighratingOverallBinary ~.,
                   data = subbaked_train, 
                   trControl = train.param,
                   method = "glm",
                   metric = "kappa",
                   preProcess = c("nzv", "center", "scale"))
log.model
```

```{r}
summary(log.model)
```


```{r}
# confusion matrix + KAPPA 
real.pred <- subbaked_test$HighratingOverallBinary
log.pred  <- predict(log.model, 
                     subbaked_test, 
                     type = "raw") # predicted class
log.scoring <- predict(log.model, 
                       subbaked_test, 
                       type = "prob") [,"1"] # predicted probability
log.conf   <- confusionMatrix(data = log.pred, 
                              reference = real.pred, 
                              positive = "1", 
                              mode = "prec_recall")
log.conf
```


```{r}
# ROC and AUC
log.auc = colAUC(log.scoring , 
                 real.pred, 
                 plotROC = TRUE)
log.auc
```

## Alina GLM Regularized (Elastic Net)
```{r}
train.param <- trainControl(method = "cv", number = 10)

glmnet.model <- train(HighratingOverallBinary ~ ., 
                      data = subbaked_train, 
                      trControl = train.param,
                      method = "glmnet",
                      metric = "kappa",
                      preProcess = c("nzv", "center", "scale"),
                      tuneGrid = expand.grid(alpha = seq(0, 1, length=5), lambda = seq(0.0001, 1, length = 20)))
```


```{r}
# confusion matrix + KAPPA 
real.pred   <- subbaked_test$HighratingOverallBinary 
glmnet.pred    <- predict(glmnet.model, 
                          subbaked_test, 
                          type = "raw") # predicted class
glmnet.scoring <- predict(glmnet.model, 
                          subbaked_test, 
                          type = "prob")[,"1"] # predicted probability
glmnet.conf   <- confusionMatrix(data = glmnet.pred, 
                                 reference = real.pred, 
                                 positive = "1", 
                                 mode = "prec_recall")
glmnet.conf
```


```{r}
# ROC and AUC
glmnet.auc = colAUC(glmnet.scoring , 
                    real.pred, 
                    plotROC = TRUE) 
glmnet.auc

# the predictors
coef(glmnet.model$finalModel)
```

## Alina GLM Regularized (Ridge)
```{r}
train.param <- trainControl(method = "cv", number = 10)

ridge.model <- train(HighratingOverallBinary~., 
                     data = subbaked_train, 
                     trControl = train.param,
                     method = "glmnet",
                     metric = "Kappa",
                     preProcess = c("nzv", "center", "scale"),
                     tuneGrid = expand.grid(alpha = 0, lambda = seq(0.0001, 1, length = 20)))
```


```{r}
# confusion matrix + KAPPA 
real.pred   <- subbaked_test$HighratingOverallBinary 
ridge.pred    <- predict(ridge.model, 
                         subbaked_test, 
                         type = "raw") 
ridge.scoring <- predict(ridge.model, 
                         subbaked_test, 
                         type = "prob")[,"1"] 

ridge.conf   <- confusionMatrix(data = ridge.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall")
ridge.conf
```

```{r}
# ROC and AUC
ridge.auc = colAUC(ridge.scoring , 
                   real.pred, 
                   plotROC = TRUE) 
```


## Alina GLM Regularized (Lasso)
```{r}
train.param <- trainControl(method = "cv", number = 10)

lasso.model <- train(HighratingOverallBinary ~., 
                     data = subbaked_train, 
                     trControl = train.param,
                     method = "glmnet",
                     metric = "Kappa",
                     preProcess = c("nzv", "center", "scale"),
                     tuneGrid = expand.grid(alpha = 1,lambda = seq(0.0001, 1, length = 20)))
coef(glmnet.model$finalModel)
```

NOT WORKING BECAUSE THE CODE IS EXTRACTING A VARIABLE????
```{r}
# confusion matrix + KAPPA 
real.pred   <- subbaked_test$HighratingOverallBinary 
lasso.pred    <- predict(lasso.model, 
                         subbaked_test[,-HighratingOverallBinary], 
                         type = "raw") 
lasso.scoring <- predict(lasso.model, 
                         subbaked_test, 
                         type = "prob")[, "1"] 
lasso.conf   <- confusionMatrix(data = lasso.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall")
lasso.conf
```


```{r}
# ROC and AUC
lasso.auc = colAUC(lasso.scoring, real.pred, plotROC = TRUE) 
lasso.auc
```





