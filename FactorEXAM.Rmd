<<<<<<< HEAD
---
title: "ExamFeatEngineering"
output: html_document
date: "2024-04-30"
---

#ML2 Alina part

```{r dataload}
library(readxl)
Data <- read_excel("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/ML2 - Machine Learning 2/ML2EXAM/Data.xls")
empFactor <- Data
```

```{r}
#install.packages("visdat")
```


```{r packages}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations
# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```


```{r data structure - overview}
library(dplyr)
# Convert all character columns in the data frame 'emp' to factors
empFactor <- empFactor %>%
  mutate(across(where(is.character), factor))
glimpse(empFactor)
```
character string (<chr>): these should be transformed into categorical variables
numeric <dbl>

```{r Missing overview}
library(visdat)
sum(is.na(empFactor))
vis_miss(empFactor, cluster = TRUE) #visdat library
plot_missing(empFactor)
```
- no NA's in variables: reviewId, reviewDateTime, ratingOverall, ratingWorkLifeBalance, ratingCultureAndValues, -	ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits, lengthOfEmployment
- NA's from 0% - 5%: employmentStatus
- NA's from 5% - 10%: jobTitle.text
- NA's from 10% - 40%: ratingRecommendToFriend, isCurrentJob, location.name
- NA's from 40% -> : jobEndingYear


## reviewId: Unique identifier for each review

```{r reviewId}
str(empFactor$reviewId)
empFactor$reviewId <- NULL
```

## reviewDateTime: Timestamp of when the review was submitted

```{r reviewDateTime }
str(empFactor$reviewDateTime)
```
follows the standard ISO 8601 format: YYYY-MM-DDTHH:MM:SS.fff, where:
YYYY represents the year,
MM the month,
DD the day,
T is a separator (indicating the start of the time portion),
HH the hour (in 24-hour format),
MM the minutes,
SS the seconds,
fff the milliseconds.
- given this interpretation, the information after the T-separator is evaluated as being redundant. Therefore the variable is being transformed into a numeric obtaining the Year variable

```{r Create year}
library(lubridate) 
# Convert 'reviewDateTime' from character to POSIXct format (if not already)
empFactor$reviewDateTime <- ymd_hms(empFactor$reviewDateTime)

# Extract the year and convert it to numeric format
empFactor$reviewYear <- year(empFactor$reviewDateTime)
```
reviewYear is created being a numeric with range 2013->2024.

```{r Factorize reviewYear}
empFactor$reviewYear <- factor(empFactor$reviewYear)
```

```{r delete origin variable}
empFactor$reviewDateTime <- NULL 
```

```{r EDA reviewYear }
library(tidyverse)
library(DataExplorer)
library(ggplot2)

# Create a bar plot
ggplot(empFactor, aes(x = as.factor(reviewYear))) +
  geom_bar(stat = "count", fill = "blue", color = "black") +  # Count is default, explicitly stating for clarity
  labs(x = "Review Year", y = "Number of Reviews", title = "Distribution of Reviews by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better visibility if needed

summary(empFactor$reviewYear)
```
The reviewDateTime variable has been transformed into
reviewYear
- a categorical variable having the vast majority of observations from the year 2023. The next coming years 2022 and 2024 do also represent a fair amount of the observations. The years including 2021 and before, are bearing a very minor part of the years, as these reviews could be consideres deleted.

## ratingOverall: Overall rating given to the company by the employee
This is the variable to predict based on the other features

```{r Factorize/ordering ratingOverall}
empFactor$ratingOverall <- factor(empFactor$ratingOverall)
summary(empFactor$ratingOverall)
# Convert to an ordinal factor
empFactor$ratingOverall <- factor(empFactor$ratingOverall, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
```
Making the variable ordinal scaled: ordered = TRUE

```{r Histogram ratingOverall }
histogram(empFactor$ratingOverall)

# Load the ggplot2 library
library(ggplot2)

# Create a histogram of ratingOverall
ggplot(empFactor, aes(x = ratingOverall)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black", binwidth = 1) +
  labs(x = "Overall Rating", y = "Frequency", title = "Distribution of Overall Ratings") +
  theme_minimal() +
  scale_x_discrete(limits = c("1", "2", "3", "4", "5"))  # Ensure that all rating levels are shown even if some have 0 counts
```
Discuss wether ratingOverall yields the best insights being numeric or being a factor?
Do we want to obtain RMSE measures or Accuracy measures? 

Discretizing ratingOverall into new variable
```{r low and highratingOverall}
empFactor$HighratingOverall_binary <- factor(ifelse(empFactor$ratingOverall < 3, "0", "1"))

head(empFactor$HighratingOverall_binary)

#Deleting on er the other DV
#empFactor$ratingOverall <- NULL 
#empFactor$HighratingOverall_binary <- NULL 
```
No need for deleting on or the other yet: creating a new df below!
REMEMBER to use ONLY ONE FORMAT of the ratingOverall variable

```{r if asked for numeric ratingOverall}
empFactor$numericratingOverall <- as.numeric(empFactor$ratingOverall)
```
REMEMBER to use ONLY ONE FORMAT of the ratingOverall variable


#DEALING WITH MISSING

```{r isCurrentJob replacing NAs with 0}
library(dplyr)
# Replace NA values with 0s
empFactor <- empFactor %>% 
  mutate(isCurrentJob = replace_na(isCurrentJob, 0))
empFactor$isCurrentJob <- factor(empFactor$isCurrentJob)
summary(empFactor$isCurrentJob)
```
isCurrentJob:
- the 39% missing values are being evaluated as "Informative missing values" (Kuhn and Johnson 2013). As the variable only contains 1s and NAs, these NAs are indeed an informative missing value which requires being transformed into 0s. 

Deleting variables: jobEndingYear,jobTitle.text,location.name

jobEndingYear
- deleting this variable due to the 61% Missings. As support for this deletion, the variable isCurrentJob is evaluated as having an adequate extent of information given in jobEndingYear. 

jobTitle.text
- Deleting this variable as we have a huge amount of different jobtitles, namely 2733 various instances, whereas these do not contribute with anything but noise to the dataset.

location.name
- Deleting this variable as we have a huge amount of different location anmes, namely 1201 various instances, whereas these do not contribute with anything but noise to the dataset.

```{r Delete jobEndingYear;jobTitle.text;location.name }
empFactor <- subset(empFactor, select = -c(jobEndingYear))
empFactor <- subset(empFactor, select = -c(jobTitle.text))
empFactor <- subset(empFactor, select = -c(location.name))
```

Deleting NAs for the two variables  and prevent making imputations, because it is assumed that it is easier to impute a value there is numeric compared to a variable there is based on your feelings and opinions
```{r Delete NAs ratingCeo;ratingBusinessOutlook;employmentStatus;ratingRecommendToFriend }
empFactor <- empFactor[!is.na(empFactor$ratingCeo), ]
empFactor <- empFactor[!is.na(empFactor$ratingBusinessOutlook), ]
empFactor <- empFactor[!is.na(empFactor$employmentStatus), ]
empFactor <- empFactor[!is.na(empFactor$ratingRecommendToFriend), ]

sum(is.na(empFactor))
```

From Exploratory Data Analysis is seen in the dataset, that 310 respondents is scoring ratingOverall with a value from 3-5 and at the same score 
- ratingCareerOpportunities,
- ratingCompensationAndBenefits,
- ratingCultureAndValues,
- ratingDiversityAndInclusion,
- ratingSeniorLeadership,
- ratingWorkLifeBalance
to zero. This is assessed being a flawed respondent, as it is assumed that this respondent does not take a stand on the above listed question-categories but still rates the ratingOverall with high scores.
```{r deleting instances having 0s in several categorical 0-5 and ratingOverall 3-5}
# Since there's still some giving 0's another subset of data will be made, deleting all the rows where the respondant answered 0
#empFactor <- empFactor[!(empFactor$ratingCareerOpportunities == 0 |
#                         empFactor$ratingCompensationAndBenefits == 0 |
#                         empFactor$ratingCultureAndValues == 0 |
#                         empFactor$ratingDiversityAndInclusion == 0 |
#                         empFactor$ratingSeniorLeadership == 0 |
#                         empFactor$ratingWorkLifeBalance == 0), ]
plot_histogram(empFactor)
```

ratingCeo, ratingBusinessOutlook, ratingRecommendToFriend, employmentStatus
- factorizing these variables into categorical variables.

ratingWorkLifeBalance, ratingCultureAndValues, ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits
- Transforming these variables into ordinal scaled categorical variables as these levels are assessed having an ranking feature embedded.

lengthOfEmployment
- scaling lenght of employment

```{r Factorize/Ordering/Scaling variables}
empFactor$ratingCeo <- factor(empFactor$ratingCeo)
empFactor$ratingBusinessOutlook <- factor(empFactor$ratingBusinessOutlook)
empFactor$ratingRecommendToFriend <- factor(empFactor$ratingRecommendToFriend)
empFactor$employmentStatus <- factor(empFactor$employmentStatus)

empFactor$ratingWorkLifeBalance <- factor(empFactor$ratingWorkLifeBalance, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCultureAndValues <- factor(empFactor$ratingCultureAndValues, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingDiversityAndInclusion <- factor(empFactor$ratingDiversityAndInclusion, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingSeniorLeadership <- factor(empFactor$ratingSeniorLeadership, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCareerOpportunities <- factor(empFactor$ratingCareerOpportunities, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCompensationAndBenefits <- factor(empFactor$ratingCompensationAndBenefits, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

#empFactor$lengthOfEmployment <- scale(empFactor$lengthOfEmployment)

dim(empFactor)
glimpse(empFactor)
```

```{r Creating Binary df}
empFactorBinary <- empFactor
empFactorBinary$ratingOverall <- NULL

dim(empFactorBinary)
glimpse(empFactorBinary)
```

```{r}
sum(is.na(empFactor))
sum(is.na(empFactorBinary))
```


# Data Preparation

```{r datasplit standard}
library(rsample)
set.seed(123) # Set a random seed for replication purposes
split <- initial_split(empFactor, prop = 0.70, strata = "ratingOverall")
train  <- training(split)
test   <- testing(split)
dim(train)
dim(test)
```

```{r Recipe standard }
#Creating the blueprint
recipe <- recipe(ratingOverall ~ ., data = empFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(), threshold = 0.05, other = "Other") %>%  #lumping if needed
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) # using NZV last removes X factor levels with near zero variance
recipe

#Preparing the blueprint based on training data
prepare <- prep(recipe, training = train)
prepare

#baking: applying the recipe to new data / test data
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
```

```{r dim standard recipe}
dim(baked_train)
dim(baked_test)
```


If computation is heavy
```{r downsizing datasplit }
set.seed(123)
random_indices <- sample(1:nrow(empFactorBinary), 1000)
subempFactor <- empFactorBinary[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "HighratingOverall_binary") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r Recipe subset }
#Creating the blueprint
subrecipe <- recipe(HighratingOverall_binary ~ ., data = subempFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(), threshold = 0.05, other = "Other") %>%  #lumping if needed
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) # using NZV last removes X factor levels with near zero variance
subrecipe

#Preparing the blueprint based on training data
subprepare <- prep(subrecipe, training = subtrain)
subprepare

#baking: applying the recipe to new data / test data
subbaked_train <- bake(subprepare, new_data = subtrain)
subbaked_test <- bake(subprepare, new_data = subtest)
```

```{r dimensions baked train/test}
#New datasets
dim(subbaked_train)
dim(subbaked_test)
```

#Model Development

## 8.3.1 Fitting Classification Trees (ISLRp.353)'

In this case the whole dataset empFactor is used.

```{r Fit tree}
#install.packages("ISLR2")
library(tree)
library(ISLR2)
#using carseats as pseudocode
tree.carseats <- tree(HighratingOverall_binary ~ ., empFactorBinary) #consider change this into subtrain + subtest
summary(tree.carseats)
```
Variables actually used in tree construction:
[1] "ratingRecommendToFriend"   "ratingSeniorLeadership"   
[3] "ratingCareerOpportunities" "ratingCultureAndValues"

Number of terminal nodes:  6
- Terminal nodes, or leaf nodes, are the end points of the tree where predictions are made. 
- The number of terminal nodes can give you an idea of the complexity of the model: more nodes generally mean a more complex model.

Residual mean deviance:  0.3889 = 3153 / 8107 
- a small deviance indicates a tree that provides a good fit to the
(training) data. 
- the residual mean deviance reported is simply the deviance divided by n − |T0|

Misclassification error rate: 0.08776 = 712 / 8113 
- training error rate is 0.08776, which means that approximately 8.776% of the predictions made by the tree were incorrect. 
- this is calculated as 712 misclassified observations out of a total of 813 observations used in the model.

use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
```{r tree plot}
plot(tree.carseats)
text(tree.carseats , pretty = 0)
```
Rating Recommend To Friend: NEGATIVE
- the root node of the tree, where the first decision is made. It indicates that the initial split in the dataset was made based on the predictor "ratingRecommendToFriend," specifically when the rating is NEGATIVE. 
- this decision or split was deemed most informative in predicting the target variable at this stage.

Deeper in the tree, other variables are presented with decisions

```{r Tree metrics and measures}
tree.carseats
```
1) root 8113 7307.0 1 ( 0.166523 0.833477 )
8113: all observations in df
7307.0: deviance
1: the majority class (which is Highoverallrating)
( 0.166523 0.833477 ): probability of Highoverallrating

2) ratingRecommendToFriend: NEGATIVE 2495 3457.0 0 ( 0.511824 0.488176 )  
2): first split
ratingRecommendToFriend: NEGATIVE: the tree first splits the data based on whether this rating is NEGATIVE
2495: observation quantity, has fewer compared to the root because it only includes cases where the condition is true.
3457.0: deviance here is smaller compared to the root, indicating a better fit for these observations under this condition.

4) ratingSeniorLeadership: 1 1237 1368.0 0 ( 0.758286 0.241714 )
- subsequent nodes are based on other ratings like "ratingSeniorLeadership" and "ratingCareerOpportunities," with specific levels indicated (e.g., "1" or "2,3,4,5").

We extract this above table to gain accurate information about the decision tree nodes


Evaluating performance of a classification tree on these data, the test error is estimated. Splitting the observations into a training set and a test set, building the tree using the training set, and evaluate its performance on the test data. 
The predict() function can be used for this purpose. In the case
of a classification tree, the argument type = "class" instructs R to return the actual class prediction. 
```{r Tree test predictions}
set.seed(123)

treetrain <- sample(1:nrow(empFactorBinary), 800)
empFactorBinary.test <- empFactorBinary[-treetrain , ]
High.test <- empFactorBinary$HighratingOverall_binary[-treetrain]

tree.empFactorBinary <- tree(HighratingOverall_binary ~ ., data = empFactorBinary, subset = treetrain)
tree.pred <- predict(tree.empFactorBinary, newdata = empFactorBinary.test, type = "class")

table(tree.pred, High.test)
((948+5663)/(948+5663+445+257))
```
correct predictions for 90,4% of the locations in the test data set

###8.3.1 Pruning
Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α)

```{r Prune output}
set.seed(123)
cv.carseats <- cv.tree(tree.carseats , FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```
The output is assessed by comparing the node "size" to the deviance of these exact nodes
- in this case nodes 6 and 3 have the same deviance 753
- Proceeding to gain test error measures

```{r Prune plot: node size + k complexity parameter}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```
Choosing the Optimal Tree Size (deviance///size): 
- these plots are used to select the optimal tree size (number of terminal nodes). You typically look for the tree size corresponding to the lowest point on the deviance plot or the smallest tree size

Complexity Parameter (k): 
- the plot of k vs. deviance helps in understanding how sensitive the tree is to the pruning process. A sharp increase in deviance as k increases indicates a point beyond which the tree loses significant predictive accuracy

```{r Prune Tree plot}
prune.carseats <- prune.misclass(tree.carseats , best = 3)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
```

```{r PruneTree test predictions}
tree.pred <- predict(prune.carseats , empFactorBinary.test,type = "class")
table(tree.pred, High.test)
((840+5844)/(840+5844+264+367))
```
correct predictions for 91,37% of the locations in the test data set WITH PRUNING the decision tree. The test accuracy gains 0.0097 percentagepoints by using pruning having 3 terminal nodes instead of 6. (0.9137-0.9040 = 0.0097)



## 8.4.9 EXE 9 - fit Classification Trees
EXE 9 - Classification tree - 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. 
This problem involves the OJ data set which is part of the ISLR2 package.
- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
- (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
- (d) Create a plot of the tree, and interpret the results.
- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
- (f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
- (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
- (h) Which tree size corresponds to the lowest cross-validated classification error rate?
- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?


- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

DOING THIS WITH SUBTRAIN SUBTEST

```{r datasplit train test}
dim(subtrain)
dim(subtest)
```

- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. 
Use the summary() function to produce summary statistics about the tree, and describe the results obtained. 
What is the training error rate? How many terminal nodes does the tree have?

```{r}
library(tree)
oj.tree = tree(HighratingOverall_binary ~ ., subtrain)
summary(oj.tree)
# Notice in the output: The tree only uses two variables: LoyalCH and PriceDiff.
# It has 7 terminal nodes. Training error rate (misclassification error) for the tree is 0.155
```
Classification tree:
Variables actually used in tree construction:
[1] "ratingRecommendToFriend"      
[2] "ratingWorkLifeBalance"        
[3] "ratingCareerOpportunities"    
[4] "ratingCompensationAndBenefits"
[5] "ratingCultureAndValues"

Number of terminal nodes:  7
- Terminal nodes, or leaf nodes, are the end points of the tree where predictions are made. 
- The number of terminal nodes can give you an idea of the complexity of the model: more nodes generally mean a more complex model.

Residual mean deviance:  0.3143 = 209 / 665
- a small deviance indicates a tree that provides a good fit to the
(training) data. 
- the residual mean deviance reported is simply the deviance divided by n − |T0|

Misclassification error rate: 0.07143 = 48 / 672 
- training error rate is 0.07143, which means that approximately 7.143% of the predictions made by the tree were incorrect. 
- this is calculated as 48 misclassified observations out of a total of 672 observations used in the model.


- (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
```{r Tree metrics and measures}
oj.tree
# Let's pick terminal node labeled “10)”
   # - The splitting variable at this node is PriceDiff
   # - The splitting value of this node is 0.065. 
   # - There are 79 points in the subtree below this node. 
   # -The deviance for all points contained in region below this node is 76.79. 
   # - A star (*) in the line denotes that this is in fact a terminal node. 
   # - The prediction at this node is Sales=MM. About 19% points in this node have CH as value of Sales.
   #   Remaining 81% points have MM as value of Sales. 
```
1) root 672 575.70 1 ( 0.15327 0.84673 )  
672: all observations in df
575.70: deviance
1: the majority class (which is Highoverallrating)
( 0.15327 0.84673 ): probability of Highoverallrating

2) ratingRecommendToFriend:NEGATIVE 198 274.50 1 (0.49495 0.50505 )
2): first split
ratingRecommendToFriend: NEGATIVE: the tree first splits the data based on whether this rating is NEGATIVE
198: observation quantity, has fewer compared to the root because it only includes cases where the condition is true.
274.50: deviance here is smaller compared to the root, indicating a better fit for these observations under this condition.

4) ratingWorkLifeBalance: 1,2 134 171.00 0 ( 0.66418 0.33582 )
- subsequent nodes are based on other ratings like "ratingSeniorLeadership" and "ratingCareerOpportunities," with specific levels indicated (e.g., "1" or "2,3,4,5").

We extract this above table to gain accurate information about the decision tree nodes


- (d) Create a plot of the tree, and interpret the results.

use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
```{r Tree Plot}
plot(oj.tree)
text(oj.tree, pretty = 0)
```
Rating Recommend To Friend: NEGATIVE
- the root node of the tree, where the first decision is made. It indicates that the initial split in the dataset was made based on the predictor "ratingRecommendToFriend," specifically when the rating is NEGATIVE. 
- this decision or split was deemed most informative in predicting the target variable at this stage.

Deeper in the tree, other variables are presented with decisions


- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

Evaluating performance of a classification tree on these data, the test error is estimated. Splitting the observations into a training set and a test set, building the tree using the training set, and evaluate its performance on the test data. 
The predict() function can be used for this purpose. In the case
of a classification tree, the argument type = "class" instructs R to return the actual class prediction. 
```{r}
oj.pred = predict(oj.tree, subtest, type = "class")
table(subtest$HighratingOverall_binary, oj.pred)

testerror = (21+252)/(21+252+25+3)
testerror
```
correct predictions for 90,7% of the locations in the test data set

###8.4.9 Prune + CV
- (f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.

Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α)

```{r Prune output}
cv.oj = cv.tree(oj.tree, FUN = prune.tree)
cv.oj
```
The output is assessed by comparing the node "size" to the deviance of these exact nodes
- in this case nodes 6 has the lowest deviance 278.6684 
- Proceeding to gain test error measures


- (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r Prune plot: node size + k complexity parameter}
par(mfrow = c(1, 2))
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
plot(cv.oj$k, cv.oj$dev, type = "b", xlab = "Complex Parameter k", ylab = "Deviance")
```
Choosing the Optimal Tree Size (deviance///size): 
- these plots are used to select the optimal tree size (number of terminal nodes). You typically look for the tree size corresponding to the lowest point on the deviance plot or the smallest tree size

Complexity Parameter (k): 
- the plot of k vs. deviance helps in understanding how sensitive the tree is to the pruning process. A sharp increase in deviance as k increases indicates a point beyond which the tree loses significant predictive accuracy

Based on both plots and size///deviance comparisons:
- 6 nodes are chosen as the optimal Tree complexity

- (h) Which tree size corresponds to the lowest cross-validated classification error rate?

```{r choose quantity of Tree nodes}
#Based on both plots and size///deviance comparisons:
#- 6 nodes are chosen as the optimal Tree complexity
```

- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

```{r PRune Tree plot}
oj.pruned = prune.tree(oj.tree, best = 6)
plot(oj.pruned)
text(oj.pruned, pretty = 0)
```


- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?

```{r PruneTree test predictions }
summary(oj.pruned)
```
The pruned training error 0.328 is 0.0137 worse than the unpruned training error 0.3143 (0.328-0.3143 = 0.0137)

- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r Unprune Tree accuracy}
pred.unpruned = predict(oj.tree, subtest, type = "class")
table(subtest$HighratingOverall_binary, pred.unpruned)
testMSEu= (21+252)/(21+252+25+3)
testMSEu
```

```{r Prune Tree accuracy}
pred.pruned = predict(oj.pruned, subtest, type = "class")
table(subtest$HighratingOverall_binary, pred.pruned)
testMSEp= (21+252)/(21+252+25+3)
testMSEp
```



## 8.4.11 EXE 11 - Boosting DV binary
EXE 11 - focus on Boosting - Binary classification problem - 5822 real customer records, (Purchase) indicates whether the customer purchased a caravan insurance policy or not.
- This question uses the Caravan data set.
- (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
- (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
- (c) Use the boosting model to predict the response on the test data.
Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. 
What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

- (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
```{r train for boosting}
dim(subtrain)
dim(subtest)
```

- (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. 
Which predictors appear to be the most important?

I get the following error:

"Error in plot.window(xlim, ylim, log = log, ...) : 
  need finite 'xlim' values"
  
When trying to run Boosting...

```{r ERROR Fit Boosting model 1000 trees + skrinkage 0.01}
library(gbm)
set.seed(123)
boost.caravan = gbm(HighratingOverall_binary ~ ., subtrain, n.trees = 500, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)

# Concl: PPERSAUT, MKOOP KLA and MOPLHOOG 
# (that is,Contribution car policies, Purchasing power class, High level education) 
# are three most important variables

summary(Caravan.train$PPERSAUT)
summary(Caravan.train$MKOOPKLA)
summary(Caravan.train$MOPLHOOG)

plot(boost.caravan, i="PPERSAUT")
plot(boost.caravan, i="MKOOPKLA")
plot(boost.caravan, i="MOPLHOOG")
```


- (c) Use the boosting model to predict the response on test data.
Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. 
What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r ERROR boost predicting on test}
boost.prob = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response") # note we use type="response" to retain probabilities ; also, we need to mention again "n.trees = 1000". 
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)

 34/(137 + 34) 
## Precision (TP/P*) is 0.1988. Thus, about 20% of people predicted to make purchase actually end up making one.

# Logistic regression 
lm.caravan = glm(Purchase ~ ., data = Caravan.train, family = binomial)
lm.prob = predict(lm.caravan, Caravan.test, type = "response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, lm.pred)

58/(350 + 58)
## [1] 0.1422. 
## About 14 % of people predicted to make purchase using logistic  regression actually end up making one. 
# The precision is lower than for boosting.

# Note: In applications one can use other popular performance measures (AUC,recall, ...) as discussed in the first part of the curriculum (ISL p. 147-149). 
```






#From Classification_2024

```{r Ralles draft1}
#Tree model
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(ratingOverall ~., job_train,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model
```


```{r Ralles draft1 - Tree}
library(caTools)
real.pred <- job_test$ratingOverall 
tree.class.pred <- predict(tree.model, 
                           job_test, type = "raw") 
tree.scoring <- predict(tree.model, 
                        job_test, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 
tree.auc
```
Output shows the distinction between how the different classes are predicted in comparison to each other

```{r Ch 8_Lab Rcode.R}
library(tree)
# recode sales as a binary var and incorporate to the dataset
#High=ifelse(Carseats$Sales<=8,"0","1")
#Carseats=data.frame(Carseats,High)
#str(Carseats)
#Carseats$High = as.factor(Carseats$High)

# set a classsification tree
tree.carseats = tree(High ~ .-Sales,Carseats, split = c("deviance", "gini"))
summary(tree.carseats)


# plot tree, disply labels (text) and category names (pretty)
plot(tree.carseats)
text(tree.carseats,pretty=0)

# read the tree
tree.carseats

# estimate the test error 
RNGkind("L'Ecuyer-CMRG")
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuraccy: (87+56)/200 = 0.71; test error = 0.29
# it you get slightly different results, it is because of the split

# pruning the tree
RNGkind("L'Ecuyer-CMRG")
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass) # FUN: classsification error rate guides the cross-validation and pruning process; default: deviance 
names(cv.carseats)
cv.carseats
# in the output, despite the name, $dev is the cross-validation error rate
# size = numer of terminal nodes of each tree considered
# $k = cost-complexity parameter (alpha in the slides)

par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# the trees with 19 terminal nodes results in the lowest cross-validation error rate.
# this is the most complex model;  
# as an example, if we wish to prune the tree 
# let us select best = 13 

# plot pruned tree
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats)
text(prune.carseats,pretty=0)

# estimate test error 
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuracy: (78+62)/200 =0.7; test error rate = 0.3
# here, pruning did not improve the accuracy.

# ps. our solution is not the same as in the textbook because of the random split.
# the instability of tree solutions when working with small datasets is acknowledged.
```

```{r Tree model}
# 3) Decision trees 
# training parameters
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(best_seller ~., train.data,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model

# confusion matrix + KAPPA 
real.pred <- test.data$best_seller 
tree.class.pred <- predict(tree.model, 
                           test.data, type = "raw") 
tree.scoring <- predict(tree.model, 
                        test.data, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 

modelComparison = rbind(modelComparison, 
                        data.frame(model = 'Basic Tree', 
                                   accuracy = tree.conf[[3]][[1]], 
                                   kappa = tree.conf[[3]][[2]], 
                                   precision = tree.conf[[4]][[5]], 
                                   recall_sensitivity = tree.conf[[4]][[6]], 
                                   specificity = tree.conf[[4]][[2]], 
                                   auc = tree.auc))
```


```{r Random Forest}
# 2) random forest
rf.model <- train(ratingOverall ~ ., job_train,
                  method = "rf", 
                  ntree = 1000,
                  metric = "Kappa",
                  trControl = train.param,
                  tune.grid = expand.grid(.mtry=c(1:28)))

rf.model

# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
rfmodel.class.pred <- predict(rf.model, 
                              job_test, 
                              type = "raw") 
rfmodel.scoring <- predict(rf.model, 
                           job_test, 
                           type = "prob") [, "1"] 
rfmodel.conf <- confusionMatrix(data = rfmodel.class.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall") 

# ROC and AUC
rf.auc = colAUC(rfmodel.scoring, real.pred, plotROC = TRUE)
```


```{r Random Forest}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'RF tree', 
                                   accuracy = rfmodel.conf[[3]][[1]], 
                                   kappa = rfmodel.conf[[3]][[2]], 
                                   precision = rfmodel.conf[[4]][[5]], 
                                   recall_sensitivity = rfmodel.conf[[4]][[6]], 
                                   specificity = rfmodel.conf[[4]][[2]], 
                                   auc = rf.auc))
```



```{r XGBoost }
model.xgboost <- train(ratingOverall ~ ., job_train,
                       method = "xgbTree",
                       metric = "Kappa",
                       tuneGrid = expand.grid(max_depth=3:6,
                                              gamma=c(0, 1, 2, 3, 5), 
                                              eta=c(0.03, 0.06, 0.1, 0.2), 
                                              nrounds=300,
                                              subsample=0.5, 
                                              colsample_bytree=0.1, 
                                              min_child_weight = 1),
                       trControl = train.param)
model.xgboost



# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
xgb.class.pred <- predict(model.xgboost, 
                          job_test, 
                          type = "raw") 
xgb.scoring <- predict(model.xgboost, 
                       job_test, 
                       type = "prob") [, "1"] 
xgb.conf <- confusionMatrix(data = xgb.class.pred, 
                            reference = real.pred, 
                            positive = "1", 
                            mode = "prec_recall") 

# ROC and AUC
xgb.auc = colAUC(xgb.scoring, real.pred, plotROC = TRUE)
```

```{r XGBoost}
xgb.conf
```


```{r XGBoost}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'xgb tree', 
                                   accuracy = xgb.conf[[3]][[1]], 
                                   kappa = xgb.conf[[3]][[2]], 
                                   precision = xgb.conf[[4]][[5]], 
                                   recall_sensitivity = xgb.conf[[4]][[6]], 
                                   specificity = xgb.conf[[4]][[2]], 
                                   auc = xgb.auc))
```

```{r SVM }
library(caret)
# Tune and fit an SVM with a radial basis kernel (C and sigma as hyperparameters)
# below we use caret’s train() function to tune and train an SVM using the radial basis 
# kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.

# Tune an SVM with radial basis kernel 
set.seed(1854)  
churn_svm <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# Plot results
ggplot(churn_svm) + theme_light()


# Print results
churn_svm$results 
# notice the default is accuracy
```


```{r SVM with trControl}
# RE-run with trContol to get the class probabilities for AUC/ROC     
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
set.seed(5628)  
churn_svm_auc <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC",  # explicitly set area under ROC curve as criteria        
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(churn_svm_auc)
# interpret
```
