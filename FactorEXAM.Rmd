<<<<<<< HEAD
---
title: "ExamFeatEngineering"
output: html_document
date: "2024-04-30"
---

#ML2 Alina part

```{r dataload}
library(readxl)
Data <- read_excel("C:/Users/Bruger/OneDrive - Aarhus universitet/8. semester - BI/ML2 - Machine Learning 2/ML2EXAM/Data.xls")
empFactor <- Data
```

```{r}
#install.packages("visdat")
```


```{r packages}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations
# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```


```{r data structure - overview}
library(dplyr)
# Convert all character columns in the data frame 'emp' to factors
empFactor <- empFactor %>%
  mutate(across(where(is.character), factor))
glimpse(empFactor)
```
character string (<chr>): these should be transformed into categorical variables
numeric <dbl>

```{r Missing overview}
library(visdat)
sum(is.na(empFactor))
vis_miss(empFactor, cluster = TRUE) #visdat library
plot_missing(empFactor)
```
- no NA's in variables: reviewId, reviewDateTime, ratingOverall, ratingWorkLifeBalance, ratingCultureAndValues, -	ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits, lengthOfEmployment
- NA's from 0% - 5%: employmentStatus
- NA's from 5% - 10%: jobTitle.text
- NA's from 10% - 40%: ratingRecommendToFriend, isCurrentJob, location.name
- NA's from 40% -> : jobEndingYear


## reviewId: Unique identifier for each review

```{r reviewId}
str(empFactor$reviewId)
empFactor$reviewId <- NULL
```

## reviewDateTime: Timestamp of when the review was submitted

```{r reviewDateTime }
str(empFactor$reviewDateTime)
```
follows the standard ISO 8601 format: YYYY-MM-DDTHH:MM:SS.fff, where:
YYYY represents the year,
MM the month,
DD the day,
T is a separator (indicating the start of the time portion),
HH the hour (in 24-hour format),
MM the minutes,
SS the seconds,
fff the milliseconds.
- given this interpretation, the information after the T-separator is evaluated as being redundant. Therefore the variable is being transformed into a numeric obtaining the Year variable

```{r Create year}
library(lubridate) 
# Convert 'reviewDateTime' from character to POSIXct format (if not already)
empFactor$reviewDateTime <- ymd_hms(empFactor$reviewDateTime)

# Extract the year and convert it to numeric format
empFactor$reviewYear <- year(empFactor$reviewDateTime)
```
reviewYear is created being a numeric with range 2013->2024.

```{r Factorize reviewYear}
empFactor$reviewYear <- factor(empFactor$reviewYear)
```

```{r delete origin variable}
empFactor$reviewDateTime <- NULL 
```

```{r EDA reviewYear }
library(tidyverse)
library(DataExplorer)
library(ggplot2)

# Create a bar plot
ggplot(empFactor, aes(x = as.factor(reviewYear))) +
  geom_bar(stat = "count", fill = "blue", color = "black") +  # Count is default, explicitly stating for clarity
  labs(x = "Review Year", y = "Number of Reviews", title = "Distribution of Reviews by Year") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x labels for better visibility if needed

summary(empFactor$reviewYear)
```
The reviewDateTime variable has been transformed into
reviewYear
- a categorical variable having the vast majority of observations from the year 2023. The next coming years 2022 and 2024 do also represent a fair amount of the observations. The years including 2021 and before, are bearing a very minor part of the years, as these reviews could be consideres deleted.

## ratingOverall: Overall rating given to the company by the employee
This is the variable to predict based on the other features

```{r Factorize/ordering ratingOverall}
empFactor$ratingOverall <- factor(empFactor$ratingOverall)
summary(empFactor$ratingOverall)
# Convert to an ordinal factor
empFactor$ratingOverall <- factor(empFactor$ratingOverall, levels = c(1, 2, 3, 4, 5), ordered = TRUE)
```
Making the variable ordinal scaled: ordered = TRUE

```{r Histogram ratingOverall }
histogram(empFactor$ratingOverall)

# Load the ggplot2 library
library(ggplot2)

# Create a histogram of ratingOverall
ggplot(empFactor, aes(x = ratingOverall)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black", binwidth = 1) +
  labs(x = "Overall Rating", y = "Frequency", title = "Distribution of Overall Ratings") +
  theme_minimal() +
  scale_x_discrete(limits = c("1", "2", "3", "4", "5"))  # Ensure that all rating levels are shown even if some have 0 counts
```
Discuss wether ratingOverall yields the best insights being numeric or being a factor?
Do we want to obtain RMSE measures or Accuracy measures? 

Discretizing ratingOverall into new variable
```{r low and highratingOverall}
empFactor$HighratingOverallBinary <- factor(ifelse(empFactor$ratingOverall < 3, "0", "1"))

head(empFactor$HighratingOverallBinary)

#Deleting on er the other DV
#empFactor$ratingOverall <- NULL 
#empFactor$HighratingOverall_binary <- NULL 
```
No need for deleting on or the other yet: creating a new df below!
REMEMBER to use ONLY ONE FORMAT of the ratingOverall variable


#DEALING WITH MISSING

```{r isCurrentJob replacing NAs with 0}
library(dplyr)
# Replace NA values with 0s
empFactor <- empFactor %>% 
  mutate(isCurrentJob = replace_na(isCurrentJob, 0))
empFactor$isCurrentJob <- factor(empFactor$isCurrentJob)
summary(empFactor$isCurrentJob)
```
isCurrentJob:
- the 39% missing values are being evaluated as "Informative missing values" (Kuhn and Johnson 2013). As the variable only contains 1s and NAs, these NAs are indeed an informative missing value which requires being transformed into 0s. 

Deleting variables: jobEndingYear,jobTitle.text,location.name

jobEndingYear
- deleting this variable due to the 61% Missings. As support for this deletion, the variable isCurrentJob is evaluated as having an adequate extent of information given in jobEndingYear. 

jobTitle.text
- Deleting this variable as we have a huge amount of different jobtitles, namely 2733 various instances, whereas these do not contribute with anything but noise to the dataset.

location.name
- Deleting this variable as we have a huge amount of different location anmes, namely 1201 various instances, whereas these do not contribute with anything but noise to the dataset.

```{r Delete jobEndingYear;jobTitle.text;location.name }
empFactor <- subset(empFactor, select = -c(jobEndingYear))
empFactor <- subset(empFactor, select = -c(jobTitle.text))
empFactor <- subset(empFactor, select = -c(location.name))
```

Deleting NAs for the two variables  and prevent making imputations, because it is assumed that it is easier to impute a value there is numeric compared to a variable there is based on your feelings and opinions
```{r Delete NAs ratingCeo;ratingBusinessOutlook;employmentStatus;ratingRecommendToFriend }
empFactor <- empFactor[!is.na(empFactor$ratingCeo), ]
empFactor <- empFactor[!is.na(empFactor$ratingBusinessOutlook), ]
empFactor <- empFactor[!is.na(empFactor$employmentStatus), ]
empFactor <- empFactor[!is.na(empFactor$ratingRecommendToFriend), ]

sum(is.na(empFactor))
```

From Exploratory Data Analysis is seen in the dataset, that 310 respondents is scoring ratingOverall with a value from 3-5 and at the same score 
- ratingCareerOpportunities,
- ratingCompensationAndBenefits,
- ratingCultureAndValues,
- ratingDiversityAndInclusion,
- ratingSeniorLeadership,
- ratingWorkLifeBalance
to zero. This is assessed being a flawed respondent, as it is assumed that this respondent does not take a stand on the above listed question-categories but still rates the ratingOverall with high scores.
```{r deleting instances having 0s in several categorical 0-5 and ratingOverall 3-5}
# Since there's still some giving 0's another subset of data will be made, deleting all the rows where the respondant answered 0
#empFactor <- empFactor[!(empFactor$ratingCareerOpportunities == 0 |
#                         empFactor$ratingCompensationAndBenefits == 0 |
#                         empFactor$ratingCultureAndValues == 0 |
#                         empFactor$ratingDiversityAndInclusion == 0 |
#                         empFactor$ratingSeniorLeadership == 0 |
#                         empFactor$ratingWorkLifeBalance == 0), ]
plot_histogram(empFactor)
```

ratingCeo, ratingBusinessOutlook, ratingRecommendToFriend, employmentStatus
- factorizing these variables into categorical variables.

ratingWorkLifeBalance, ratingCultureAndValues, ratingDiversityAndInclusion, ratingSeniorLeadership, ratingCareerOpportunities, ratingCompensationAndBenefits
- Transforming these variables into ordinal scaled categorical variables as these levels are assessed having an ranking feature embedded.

lengthOfEmployment
- scaling lenght of employment

```{r Factorize/Ordering/Scaling variables}
empFactor$ratingCeo <- factor(empFactor$ratingCeo)
empFactor$ratingBusinessOutlook <- factor(empFactor$ratingBusinessOutlook)
empFactor$ratingRecommendToFriend <- factor(empFactor$ratingRecommendToFriend)
empFactor$employmentStatus <- factor(empFactor$employmentStatus)

empFactor$ratingWorkLifeBalance <- factor(empFactor$ratingWorkLifeBalance, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCultureAndValues <- factor(empFactor$ratingCultureAndValues, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingDiversityAndInclusion <- factor(empFactor$ratingDiversityAndInclusion, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingSeniorLeadership <- factor(empFactor$ratingSeniorLeadership, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCareerOpportunities <- factor(empFactor$ratingCareerOpportunities, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)
empFactor$ratingCompensationAndBenefits <- factor(empFactor$ratingCompensationAndBenefits, levels = c(0, 1, 2, 3, 4, 5), ordered = TRUE)

#empFactor$lengthOfEmployment <- scale(empFactor$lengthOfEmployment)

dim(empFactor)
glimpse(empFactor)
```

```{r Creating Binary df}
empFactorBinary <- empFactor
empFactorBinary$ratingOverall <- NULL

empFactor$HighratingOverallBinary <- NULL

dim(empFactorBinary)
glimpse(empFactorBinary)
```

```{r Creating DVnumeric df}
empFactorDVNumeric <- empFactor

empFactorDVNumeric$numericratingOverall <- as.numeric(empFactorDVNumeric$ratingOverall)

empFactorDVNumeric$ratingOverall <- NULL
empFactorDVNumeric$HighratingOverallBinary <- NULL

dim(empFactorDVNumeric)
glimpse(empFactorDVNumeric)
```

```{r}
sum(is.na(empFactor))
sum(is.na(empFactorBinary))
sum(is.na(empFactorDVNumeric))
```


# Data Preparation

```{r datasplit standard}
library(rsample)
set.seed(123) # Set a random seed for replication purposes
split <- initial_split(empFactor, prop = 0.70, strata = "ratingOverall")
train  <- training(split)
test   <- testing(split)
dim(train)
dim(test)
```

```{r Recipe standard }
#Creating the blueprint
recipe <- recipe(ratingOverall ~ ., data = empFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(), threshold = 0.05, other = "Other") %>%  #lumping if needed
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) # using NZV last removes X factor levels with near zero variance
recipe

#Preparing the blueprint based on training data
prepare <- prep(recipe, training = train)
prepare

#baking: applying the recipe to new data / test data
baked_train <- bake(prepare, new_data = train)
baked_test <- bake(prepare, new_data = test)
```

```{r dim standard recipe}
dim(baked_train)
dim(baked_test)
```


If computation is heavy
Remember to CHOOSE dataset in 
 - random_indices <- sample(1:nrow(EMPFACTORBINARY), 1000)
 - subempFactor <- EMPFACTORBINARY[random_indices, ]
as well as DV in
 - subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "HIGHRATINGOVERALLBINARY")

```{r downsizing datasplit DVBinary }
set.seed(123)
random_indices <- sample(1:nrow(empFactorBinary), 1000)
subempFactor <- empFactorBinary[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "HighratingOverallBinary") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r downsizing datasplit 5-level DV }
set.seed(123)
random_indices <- sample(1:nrow(empFactor), 1000)
subempFactor <- empFactor[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "ratingOverall") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r downsizing datasplit DVNumeric }
set.seed(123)
random_indices <- sample(1:nrow(empFactorDVNumeric), 1000)
subempFactor <- empFactorDVNumeric[random_indices, ]

subsplit  <- initial_split(subempFactor, prop = 0.7, strata = "numericratingOverall") #Important to distinguish between binary and 5-level variable!
subtrain <- training(subsplit)
subtest  <- testing(subsplit)

dim(subtrain)
dim(subtest)
```

```{r Recipe subset }
#Creating the blueprint
subrecipe <- recipe(HighratingOverall_binary ~ ., data = subempFactor) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  #step_impute_knn(all_predictors(), neighbors = 5) %>%
  #step_BoxCox(all_outcomes()) %>%
  #step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
  # step_other(all_nominal(),threshold=0.05, other = "Other") %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE) %>% #produces dummy encoding (True is one-hot encoding)
  step_nzv(all_predictors()) #removes factor levels with nzv
subrecipe

#Preparing the blueprint based on training data
subprepare <- prep(subrecipe, training = subtrain)
subprepare

#baking: applying the recipe to new data / test data
subbaked_train <- bake(subprepare, new_data = subtrain)
subbaked_test <- bake(subprepare, new_data = subtest)
```

```{r dimensions baked train/test}
#New datasets
dim(subbaked_train)
dim(subbaked_test)
```

#Model Development

## 8.3.1 Fitting Classification Trees (ISLRp.353)'

In this case the whole dataset empFactor is used.

```{r Fit tree}
#install.packages("ISLR2")
library(tree)
library(ISLR2)
#using carseats as pseudocode
tree.carseats <- tree(HighratingOverall_binary ~ ., empFactorBinary) #consider change this into subtrain + subtest
summary(tree.carseats)
```
Variables actually used in tree construction:
[1] "ratingRecommendToFriend"   "ratingSeniorLeadership"   
[3] "ratingCareerOpportunities" "ratingCultureAndValues"

Number of terminal nodes:  6
- Terminal nodes, or leaf nodes, are the end points of the tree where predictions are made. 
- The number of terminal nodes can give you an idea of the complexity of the model: more nodes generally mean a more complex model.

Residual mean deviance:  0.3889 = 3153 / 8107 
- a small deviance indicates a tree that provides a good fit to the
(training) data. 
- the residual mean deviance reported is simply the deviance divided by n − |T0|

Misclassification error rate: 0.08776 = 712 / 8113 
- training error rate is 0.08776, which means that approximately 8.776% of the predictions made by the tree were incorrect. 
- this is calculated as 712 misclassified observations out of a total of 813 observations used in the model.

use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
```{r tree plot}
plot(tree.carseats)
text(tree.carseats , pretty = 0)
```
Rating Recommend To Friend: NEGATIVE
- the root node of the tree, where the first decision is made. It indicates that the initial split in the dataset was made based on the predictor "ratingRecommendToFriend," specifically when the rating is NEGATIVE. 
- this decision or split was deemed most informative in predicting the target variable at this stage.

Deeper in the tree, other variables are presented with decisions

```{r Tree metrics and measures}
tree.carseats
```
1) root 8113 7307.0 1 ( 0.166523 0.833477 )
8113: all observations in df
7307.0: deviance
1: the majority class (which is Highoverallrating)
( 0.166523 0.833477 ): probability of Highoverallrating

2) ratingRecommendToFriend: NEGATIVE 2495 3457.0 0 ( 0.511824 0.488176 )  
2): first split
ratingRecommendToFriend: NEGATIVE: the tree first splits the data based on whether this rating is NEGATIVE
2495: observation quantity, has fewer compared to the root because it only includes cases where the condition is true.
3457.0: deviance here is smaller compared to the root, indicating a better fit for these observations under this condition.

4) ratingSeniorLeadership: 1 1237 1368.0 0 ( 0.758286 0.241714 )
- subsequent nodes are based on other ratings like "ratingSeniorLeadership" and "ratingCareerOpportunities," with specific levels indicated (e.g., "1" or "2,3,4,5").

We extract this above table to gain accurate information about the decision tree nodes


Evaluating performance of a classification tree on these data, the test error is estimated. Splitting the observations into a training set and a test set, building the tree using the training set, and evaluate its performance on the test data. 
The predict() function can be used for this purpose. In the case
of a classification tree, the argument type = "class" instructs R to return the actual class prediction. 
```{r Tree test predictions}
set.seed(123)

treetrain <- sample(1:nrow(empFactorBinary), 800)
empFactorBinary.test <- empFactorBinary[-treetrain , ]
High.test <- empFactorBinary$HighratingOverall_binary[-treetrain]

tree.empFactorBinary <- tree(HighratingOverall_binary ~ ., data = empFactorBinary, subset = treetrain)
tree.pred <- predict(tree.empFactorBinary, newdata = empFactorBinary.test, type = "class")

table(tree.pred, High.test)
((948+5663)/(948+5663+445+257))
```
correct predictions for 90,4% of the locations in the test data set

###8.3.1 Pruning
Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α)

```{r Prune output}
set.seed(123)
cv.carseats <- cv.tree(tree.carseats , FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```
The output is assessed by comparing the node "size" to the deviance of these exact nodes
- in this case nodes 6 and 3 have the same deviance 753
- Proceeding to gain test error measures

```{r Prune plot: node size + k complexity parameter}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```
Choosing the Optimal Tree Size (deviance///size): 
- these plots are used to select the optimal tree size (number of terminal nodes). You typically look for the tree size corresponding to the lowest point on the deviance plot or the smallest tree size

Complexity Parameter (k): 
- the plot of k vs. deviance helps in understanding how sensitive the tree is to the pruning process. A sharp increase in deviance as k increases indicates a point beyond which the tree loses significant predictive accuracy

```{r Prune Tree plot}
prune.carseats <- prune.misclass(tree.carseats , best = 3)
plot(prune.carseats)
text(prune.carseats , pretty = 0)
```

```{r PruneTree test predictions}
tree.pred <- predict(prune.carseats , empFactorBinary.test,type = "class")
table(tree.pred, High.test)
((840+5844)/(840+5844+264+367))
```
correct predictions for 91,37% of the locations in the test data set WITH PRUNING the decision tree. The test accuracy gains 0.0097 percentagepoints by using pruning having 3 terminal nodes instead of 6. (0.9137-0.9040 = 0.0097)



## 8.4.9 EXE 9 - fit Classification Trees
EXE 9 - Classification tree - 1070 purchases where the customer either purchased Citrus Hill or Minute Maid Orange Juice. 
This problem involves the OJ data set which is part of the ISLR2 package.
- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?
- (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
- (d) Create a plot of the tree, and interpret the results.
- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?
- (f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.
- (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
- (h) Which tree size corresponds to the lowest cross-validated classification error rate?
- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?
- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?


- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

DOING THIS WITH SUBTRAIN SUBTEST

```{r datasplit train test}
dim(subtrain)
dim(subtest)
```

- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. 
Use the summary() function to produce summary statistics about the tree, and describe the results obtained. 
What is the training error rate? How many terminal nodes does the tree have?

```{r}
library(tree)
oj.tree = tree(HighratingOverall_binary ~ ., subtrain)
summary(oj.tree)
# Notice in the output: The tree only uses two variables: LoyalCH and PriceDiff.
# It has 7 terminal nodes. Training error rate (misclassification error) for the tree is 0.155
```
Classification tree:
Variables actually used in tree construction:
[1] "ratingRecommendToFriend"      
[2] "ratingWorkLifeBalance"        
[3] "ratingCareerOpportunities"    
[4] "ratingCompensationAndBenefits"
[5] "ratingCultureAndValues"

Number of terminal nodes:  7
- Terminal nodes, or leaf nodes, are the end points of the tree where predictions are made. 
- The number of terminal nodes can give you an idea of the complexity of the model: more nodes generally mean a more complex model.

Residual mean deviance:  0.3143 = 209 / 665
- a small deviance indicates a tree that provides a good fit to the
(training) data. 
- the residual mean deviance reported is simply the deviance divided by n − |T0|

Misclassification error rate: 0.07143 = 48 / 672 
- training error rate is 0.07143, which means that approximately 7.143% of the predictions made by the tree were incorrect. 
- this is calculated as 48 misclassified observations out of a total of 672 observations used in the model.


- (c) Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.
```{r Tree metrics and measures}
oj.tree
# Let's pick terminal node labeled “10)”
   # - The splitting variable at this node is PriceDiff
   # - The splitting value of this node is 0.065. 
   # - There are 79 points in the subtree below this node. 
   # -The deviance for all points contained in region below this node is 76.79. 
   # - A star (*) in the line denotes that this is in fact a terminal node. 
   # - The prediction at this node is Sales=MM. About 19% points in this node have CH as value of Sales.
   #   Remaining 81% points have MM as value of Sales. 
```
1) root 672 575.70 1 ( 0.15327 0.84673 )  
672: all observations in df
575.70: deviance
1: the majority class (which is Highoverallrating)
( 0.15327 0.84673 ): probability of Highoverallrating

2) ratingRecommendToFriend:NEGATIVE 198 274.50 1 (0.49495 0.50505 )
2): first split
ratingRecommendToFriend: NEGATIVE: the tree first splits the data based on whether this rating is NEGATIVE
198: observation quantity, has fewer compared to the root because it only includes cases where the condition is true.
274.50: deviance here is smaller compared to the root, indicating a better fit for these observations under this condition.

4) ratingWorkLifeBalance: 1,2 134 171.00 0 ( 0.66418 0.33582 )
- subsequent nodes are based on other ratings like "ratingSeniorLeadership" and "ratingCareerOpportunities," with specific levels indicated (e.g., "1" or "2,3,4,5").

We extract this above table to gain accurate information about the decision tree nodes


- (d) Create a plot of the tree, and interpret the results.

use the plot() function to display the tree structure, and the text() function to display the node labels. The argument pretty = 0 instructs R to include the category names for any qualitative predictors, rather than simply displaying a letter for each category
```{r Tree Plot}
plot(oj.tree)
text(oj.tree, pretty = 0)
```
Rating Recommend To Friend: NEGATIVE
- the root node of the tree, where the first decision is made. It indicates that the initial split in the dataset was made based on the predictor "ratingRecommendToFriend," specifically when the rating is NEGATIVE. 
- this decision or split was deemed most informative in predicting the target variable at this stage.

Deeper in the tree, other variables are presented with decisions


- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

Evaluating performance of a classification tree on these data, the test error is estimated. Splitting the observations into a training set and a test set, building the tree using the training set, and evaluate its performance on the test data. 
The predict() function can be used for this purpose. In the case
of a classification tree, the argument type = "class" instructs R to return the actual class prediction. 
```{r}
oj.pred = predict(oj.tree, subtest, type = "class")
table(subtest$HighratingOverall_binary, oj.pred)

testerror = (21+252)/(21+252+25+3)
testerror
```
correct predictions for 90,7% of the locations in the test data set

###8.4.9 Prune + CV
- (f) Apply the cv.tree() function to the training set in order to determine the optimal tree size.

Next, we consider whether pruning the tree might lead to improved results. The function cv.tree() performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument FUN = prune.misclass in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the cv.tree() function, which is deviance. The cv.tree() function reports the number of terminal nodes of each tree considered (size) as well as the corresponding error rate and the value of the cost-complexity parameter used (k, which corresponds to α)

```{r Prune output}
cv.oj = cv.tree(oj.tree, FUN = prune.tree)
cv.oj
```
The output is assessed by comparing the node "size" to the deviance of these exact nodes
- in this case nodes 6 has the lowest deviance 278.6684 
- Proceeding to gain test error measures


- (g) Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r Prune plot: node size + k complexity parameter}
par(mfrow = c(1, 2))
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
plot(cv.oj$k, cv.oj$dev, type = "b", xlab = "Complex Parameter k", ylab = "Deviance")
```
Choosing the Optimal Tree Size (deviance///size): 
- these plots are used to select the optimal tree size (number of terminal nodes). You typically look for the tree size corresponding to the lowest point on the deviance plot or the smallest tree size

Complexity Parameter (k): 
- the plot of k vs. deviance helps in understanding how sensitive the tree is to the pruning process. A sharp increase in deviance as k increases indicates a point beyond which the tree loses significant predictive accuracy

Based on both plots and size///deviance comparisons:
- 6 nodes are chosen as the optimal Tree complexity

- (h) Which tree size corresponds to the lowest cross-validated classification error rate?

```{r choose quantity of Tree nodes}
#Based on both plots and size///deviance comparisons:
#- 6 nodes are chosen as the optimal Tree complexity
```

- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

```{r PRune Tree plot}
oj.pruned = prune.tree(oj.tree, best = 6)
plot(oj.pruned)
text(oj.pruned, pretty = 0)
```


- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?

```{r PruneTree test predictions }
summary(oj.pruned)
```
The pruned training error 0.328 is 0.0137 worse than the unpruned training error 0.3143 (0.328-0.3143 = 0.0137)

- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?
```{r Unprune Tree accuracy}
pred.unpruned = predict(oj.tree, subtest, type = "class")
table(subtest$HighratingOverall_binary, pred.unpruned)
testMSEu= (21+252)/(21+252+25+3)
testMSEu
```

```{r Prune Tree accuracy}
pred.pruned = predict(oj.pruned, subtest, type = "class")
table(subtest$HighratingOverall_binary, pred.pruned)
testMSEp= (21+252)/(21+252+25+3)
testMSEp
```



## 8.4.11 EXE 11 - Boosting DV binary
EXE 11 - focus on Boosting - Binary classification problem - 5822 real customer records, (Purchase) indicates whether the customer purchased a caravan insurance policy or not.
- This question uses the Caravan data set.
- (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
- (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?
- (c) Use the boosting model to predict the response on the test data.
Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. 
What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

- (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.
```{r train for boosting}
dim(subtrain)
dim(subtest)
```

- (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. 
Which predictors appear to be the most important?

I get the following error:

"Error in plot.window(xlim, ylim, log = log, ...) : 
  need finite 'xlim' values"
  
When trying to run Boosting...

```{r ERROR Fit Boosting model 1000 trees + skrinkage 0.01}
library(gbm)
set.seed(123)
boost.caravan = gbm(HighratingOverall_binary ~ ., subtrain, n.trees = 500, shrinkage = 0.01, distribution = "bernoulli")
summary(boost.caravan)

# Concl: PPERSAUT, MKOOP KLA and MOPLHOOG 
# (that is,Contribution car policies, Purchasing power class, High level education) 
# are three most important variables

summary(Caravan.train$PPERSAUT)
summary(Caravan.train$MKOOPKLA)
summary(Caravan.train$MOPLHOOG)

plot(boost.caravan, i="PPERSAUT")
plot(boost.caravan, i="MKOOPKLA")
plot(boost.caravan, i="MOPLHOOG")
```


- (c) Use the boosting model to predict the response on test data.
Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. 
What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r ERROR boost predicting on test}
boost.prob = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response") # note we use type="response" to retain probabilities ; also, we need to mention again "n.trees = 1000". 
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)

 34/(137 + 34) 
## Precision (TP/P*) is 0.1988. Thus, about 20% of people predicted to make purchase actually end up making one.

# Logistic regression 
lm.caravan = glm(Purchase ~ ., data = Caravan.train, family = binomial)
lm.prob = predict(lm.caravan, Caravan.test, type = "response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, lm.pred)

58/(350 + 58)
## [1] 0.1422. 
## About 14 % of people predicted to make purchase using logistic  regression actually end up making one. 
# The precision is lower than for boosting.

# Note: In applications one can use other popular performance measures (AUC,recall, ...) as discussed in the first part of the curriculum (ISL p. 147-149). 
```



##LauB RF Random Forest


```{r Random Forest }
rf_model <- train(HighratingOverallBinary ~ ., data = subtrain, method = "rf", trControl = ctrl)
summary(rf_model)
```

```{r predictions RF + Accuracy}
# Make predictions
pred_rf <- predict(rf_model, newdata = subtest)

# Evaluate the models' performance
confusionMatrix(pred_rf, subtest$HighratingOverallBinary)
```

##LauB RF Random Forest PART2

```{r}
# Random Forest
# number of features
n_features <- length(setdiff(names(baked_train), "X0"))
n_features #45
 

# train a default random forest model using ranger library
rf1 <- ranger(
  X0 ~ ., 
  data = baked_train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)

# get Out-of-bag RMSE (OOB RMSE)
(default_rmse <- sqrt(rf1$prediction.error)) 
# OOB RMSE = 0.5032%


# Tunable hyperparameters that we should consider when training a RF model (the tuneable hyperparameters are "mtry", "node.size", "sample.fraction")
# create hyperparameter grid
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .5)),
  min.node.size = c(1, 3, 5, 10, 15, 20),   #Minimum number of oberservations in each node size #Bigger node size = less computational time, but higher error.
  replace = c(TRUE, FALSE),                 #bagging with replacement or without, default is with replacement, but ranger allows both
  sample.fraction = c(.5, .63, .8),         #Decides the fraction size of each bag used
  rmse = NA                                               
)

# execute full cartesian grid search (where we assess every combination of hyperparameters of interest)
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = X0 ~ ., 
    data            = baked_train, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    num.threads     = 8,
    verbose         = TRUE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)

# Interpretation: the top 10 models have these features e.g., mtry = 2 (out of 45 predictors), 
# node-size 5 (it splits until theres 5 observation in the terminal node)
# replacement is better in half of the models here, as replace = FALSE for top 10 models (Replace = false = Replacement is best)
# sample fraction is better if we only consider 80% of the original data, 
# rmse perc_gain = percentage rmse minimized by using this model

#Train the optimal model, changing values in accordance with the best model obtained from the top 10 above
OptimalModel <- ranger(
  formula = X0 ~.,
  data = baked_train,
  num.trees = 450,
  mtry = 2,
  min.node.size = 5,
  replace = T,
  sample.fraction = 0.8,
  verbose = T,
  seed = 123
)


#Making out of sample predictions
p <- predict(OptimalModel, baked_test)
pred <- p$predictions

# rmse <- accuracy(pred, baked_test$X0) #stoppet med at virke efter første gang!? use caret package if problems. (giver det overhovedet mening at beregne RMSE for logistic problems?)
# rmse #0.7375415 Mean Error

# Feature Importance plot + comparison between impurity and permutation
# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = X0 ~ ., 
  data = baked_train, 
  num.trees = 450,  # notice the model is re-run with the optimal hyperparam identified before
  mtry = 2,
  min.node.size =5,
  sample.fraction = 0.8,
  replace = T,
  importance = "impurity",  # based on impurity (Impurity importance, also known as Gini importance, is the default importance measure used in random forests. The impurity-based importance assesses the predictive power of each predictor by measuring how much splitting on that predictor improves the overall purity or homogeneity of the resulting nodes.)
  respect.unordered.factors = "order",
  verbose = T,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = X0 ~ ., 
  data = baked_train, 
  num.trees = 450,  # notice the model is re-run with the optimal hyperparam identified before
  mtry = 2,
  min.node.size = 5,
  sample.fraction = 0.8,
  replace = T,
  importance = "permutation",  # based on permutation (It measures the decrease in model performance (e.g., accuracy or mean squared error) when the values of a predictor are randomly permuted while keeping other predictors unchanged)
  respect.unordered.factors = "order",
  verbose = T,
  seed  = 123
) 


# Typically, you will not see the same variable importance order
# between the two options; however, you will often see similar 
# variables at the top of the plots (and also the bottom).
# We use impurity, as it is the standard for RF models.

p1 <- vip::vip(rf_impurity, num_features = 45, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 45, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)

# Confusion Matrix
confusionMatrix(
  pred, baked_test$X0, positive = "yes")

#          Reference
# Prediction  no yes
#         no  199  66
#        yes  12  24

# Accuracy : 0.7409          
# 95% CI : (0.6875, 0.7894)
# No Information Rate : 0.701           
# P-Value [Acc > NIR] : 0.07242         
# Kappa : 0.2534 
# Sensitivity : 0.26667         
# Specificity : 0.94313         
# Pos Pred Value : 0.66667

# AUC
#Cant with the Ranger model apparently - use carets train() on rf model if we want AUC and RMSE
#E.g 
#X0_rf.caret <- train(X0 ~ ., data = baked_train, method = "rf", ntree = 10, trControl = ctrl)
```


##LauB Gradient Boosting Model (GBM) 

```{r}
# Gradient boosting with caret
# Set up the train control object
ctrl <- trainControl(
  method = "cv",       # Cross-validation method
  number = 3,          # Number of folds
  verboseIter = TRUE   # Print progress during training
)

# Train the gradient boosting model
gbmfit <- train(
  X0 ~ .,                    # Formula for the model
  data = baked_train,        # Training data
  method = "gbm",            # Gradient boosting method
  trControl = ctrl,          # Train control object
  verbose = FALSE            # Suppress verbose output
)

# Getting accuracy and kappa from the model trained using caret
# Obtain predictions from the trained model
gbm_pred <- predict(gbmfit, newdata = baked_test)

# Create a confusion matrix
confusion_mat <- confusionMatrix(gbm_pred, baked_test$X0)

# Extract accuracy and kappa from the confusion matrix
accuracy <- confusion_mat$overall["Accuracy"] #0.7441 
kappa <- confusion_mat$overall["Kappa"] #0.2928

#Calculate AUC

# Load the pROC package
#install.packages("pROC")
library(pROC)

#baked_train$X0 <- as.numeric(baked_train$X0)
#baked_test$X0 <- as.numeric(baked_test$X0)
#baked_train$X0 <- baked_train$X0 - 1 #subtracting 1 as R turns the 0/1 variable into a 1/2 variable when changing to numeric
#baked_test$X0 <- baked_test$X0 - 1 #subtracting 1 as R turns the 0/1 variable into a 1/2 variable when changing to numeric


# Obtain predicted probabilities from your gradient boosting model
predicted_probs <- predict(gbmfit, newdata = baked_test, type = "prob")

# Extract the predicted probabilities for the positive class
positive_probs <- predicted_probs[, "yes"]

# Create a binary outcome vector (1 for positive class, 0 for negative class)
outcome <- ifelse(baked_test$X0 == "yes", 1, 0)

# Calculate the ROC curve and AUC

roc_obj <- roc(outcome, positive_probs)
plot(roc_obj)
colAUC(predicted_probs[ , 1], as.character(baked_test$X0), plotROC = T)
#We get the probabilities from the first model we trained
#auc_value <- auc(roc_obj) #0.7659
#auc
# Area Under the Curve is 76.59%
# The ROC curve is a graphical representation of the models sensitivity and 1-specificity and to evaluate the performance of a model
# predicting a binary variable we can evaluate the Area Under the Curve which represent a number between 0 and 1, where the closer to
# 1 the better the performance of the model and an AUC of 0.5 is equal to a random guess.
# Hence the AUC of almost 76% is considered a decent performance.

# Confusion Matrix
predicted_val <- predict(gbmfit, baked_test)
confusionMatrix(
  predicted_val, baked_test$X0, positive = "yes")

#           Reference
# Prediction  no yes
#        no   194  60
#       yes   17  30

# Accuracy : 0.7442         
# 95% CI : (0.691, 0.7925)
# No Information Rate : 0.701          
# P-Value [Acc > NIR] : 0.0562         

# Kappa : 0.2929
# Sensitivity : 0.33333        
# Specificity : 0.91943

#Discussion: 
# The accuracy of the model is 74.42%, which means the model predicts the true positives or true negative correctly
# 74.42% of the time. We can compare this to the NIR of 70%, which is the ratio of Yes and No in the testing data set.
# This means that if we simply predicted "no" for all observations, we would still have an accuracy of 70%, why our goal is to
# maximize the accuracy rate above the no information baseline. The model predicts slightly better with an accuracy of 74.42

#The sensitivity (True Positive Rate) is an indication of how well the model is recalling the positive class "Yes" and is 33.33% which is a bad performance
# of recalling the positive class.

#The specificity is an indication of how well the model is predicting the negative class "No" and is 92%, which is a good performance
# of predicting the negative class

#The precision of the model (Pos Pred Value) is a value that represents the precision or the proportion of positive predictions that are 
# actually true positives which is at 63.83% meaning that 57% of the time, the model correctly predicts a subscribed client.

```

##LauB Extreme Gradient Boosting (XGBoost) 

```{r}
set.seed(123)
train.param <- trainControl(method = "cv", number = 3) 

tune.grid.xgb <- expand.grid(nrounds=300, 
                                 max_depth=c(3,6,9), 
                                 gamma=c(0,1,5,10),
                                 eta=c(0.01, 0.03, 0.1, 0.3),
                                 subsample=0.5, 
                                 colsample_bytree=0.1, 
                                 min_child_weight = 1)

xgbcaret <- train(X0 ~ ., baked_train,
                     method = "xgbTree",
                     tuneGrid = tune.grid.xgb,
                     trControl = train.param)

xgbcaret
# eta   max_depth  gamma  Accuracy   Kappa    
# 0.01  3           0     0.7468454  0.2734396
# 0.01  3           1     0.7568598  0.2965317
# 0.01  3           5     0.7568598  0.2917852
# 0.01  3          10     0.7167284  0.1090324
# 0.01  6           0     0.7511311  0.2825163
# 0.01  6           1     0.7540047  0.2952909
# 0.01  6           5     0.7525740  0.2730613
# 0.01  6          10     0.7253061  0.1502354

plot(xgbcaret)

#Feature importance
vip::vip(xgbcaret) 
#Most important features (top 5) are X18, X13, X19, X20 and X16

#Predicting
# Obtain predicted probabilities from the XG boost model
predicted_probsXG <- predict(xgbcaret, newdata = baked_test, type = "prob")

# Extract the predicted probabilities for the positive class
positive_probsXG <- predicted_probsXG[, "yes"]

# Create a binary outcome vector (1 for positive class, 0 for negative class)
outcomeXG <- ifelse(baked_test$X0 == "yes", 1, 0)

# Calculate the ROC curve and AUC
roc_objXG <- roc(outcomeXG, positive_probsXG)
roc_objXG
colAUC(predicted_probsXG[ , 1], as.character(baked_test$X0), plotROC = T) #0.7543
# Area Under the Curve is 75.43%
# The ROC curve is a graphical representation of the models sensitivity and 1-specificity and to evaluate the performance of a model
# predicting a binary variable we can evaluate the Area Under the Curve which represent a number between 0 and 1, where the closer to
# 1 the better the performance of the model and an AUC of 0.5 is equal to a random guess.
# Hence the AUC of almost 76% is considered a decent performance.

#RMSE (does not make sense to calculate on logistic?)
rmse <- RMSE(positive_probsXG, outcomeXG) #0.4126
1-rmse #0.5873

# Confusion Matrix
predicted_val <- predict(xgbcaret, baked_test)
confusionMatrix(
  predicted_val, baked_test$X0, positive = "yes")

#           Reference
#Prediction  no yes
#       no  200  65
#      yes  11  25

# Accuracy : 0.7475          
# 95% CI : (0.6945, 0.7956)
# No Information Rate : 0.701           
# P-Value [Acc > NIR] : 0.04296         
# Kappa : 0.2725
# Sensitivity : 0.27778         
# Specificity : 0.94787         
# Pos Pred Value : 0.69444

#Discussion: 
# The accuracy of the model is 74.75%, which means the model predicts the true positives or true negative correctly
# 74.42% of the time. We can compare this to the NIR of 70%, which is the ratio of Yes and No in the testing data set.
# This means that if we simply predicted "no" for all observations, we would still have an accuracy of 70%, why our goal is to
# maximize the accuracy rate above the no information baseline. The model predicts slightly better with an accuracy of 74.42

#The sensitivity (True Positive Rate) is an indication of how well the model is recalling the positive class "Yes" and is 27% which is a bad performance
# of recalling the positive class.

#The specificity is an indication of how well the model is predicting the negative class "No" and is 94%, which is a good performance
# of predicting the negative class

#The precision of the model (Pos Pred Value) is a value that represents the precision or the proportion of positive predictions that are 
# actually true positives which is at 69% meaning that 69% of the time, the model correctly predicts a subscribed client.
```




#SVM HOM

```{r helper packages}
# Helper packages
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)  # for data splitting
library(modeldata) # for data set Job attrition


# Modeling packages
library(caret)    # meta-engine for SVMs
library(kernlab)  # also for fitting SVMs 

# Model interpretability packages
library(pdp)      # for partial dependence plots, etc.
library(vip)      # for variable importance plots
```

```{r datasplit}
dim(subtrain)
dim(subtest)
prop.table(table(empFactorBinary$HighratingOverallBinary)) 
```

Tuning and fitting SVM with a radial basis kernel (C and sigma as hyperparameters)
- below we use caret’s train() function to tune and train an SVM using the radial basis 
- kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.
```{r Tune SVM - radial kernel}
set.seed(123)  
churn_svm <- train(
  HighratingOverallBinary ~ ., 
  data = subtrain,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)
```

```{r Plot SVM}
ggplot(churn_svm) + theme_light()

# Print results
churn_svm$bestTune 
churn_svm$results 
# notice the default is accuracy
```
from Plotting and results from radial kernel SVM, smaller values of cost is beneficial for the accuracy on this dataset case.
- the cost = 0.50 yields: 
- the highest accuracy measure 0.9384
- a sigma value of 0.01541844

```{r}
confusionMatrix(churn_svm)
```
On SVM without probability classes
- Accuracy: 0.9385

We calculate the probability classes to b able to obtain AUC/ROC measurements
```{r Class probabilities to obtain AUC/ROC}
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)
```

```{r rescaling DV as getting strange format error}
levels(subtrain$HighratingOverallBinary) <- make.names(levels(subtrain$HighratingOverallBinary), unique=TRUE)

levels(subtrain$HighratingOverallBinary)
```


```{r Tune SVM with ROC/AUC}
# Tune an SVM
set.seed(123)  
churn_svm_auc <- train(
  HighratingOverallBinary ~ ., 
  data = subtrain,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC", #explicitly set area under ROC curve as criteria
  trControl = ctrl,
  tuneLength = 10
)
```


```{r SVM AUC Confusionmatrix}
confusionMatrix(churn_svm_auc)
# interpret
```
On SVM with probability classes
- Accuracy: 0.9328
This Accuracy is 0.0057 less than the Accuracy obtained without class probabilities.
(0.9385 - 0.9328 = 0.0057)

```{r SVM AUC besttune + results}
churn_svm_auc$bestTune
churn_svm_auc$results
```
from results from radial kernel SVM taking class probabilities into account, smaller values of cost is beneficial for the accuracy on this dataset case.
- the cost = 0.25 yields: 
- the highest ROC measure 0.9574
- a sigma value of 0.01541844
- sensitivity (true positive rate): 0.7372727
- specificity (true negative rate): 0.9679944

This model does a better job predicting 0's compared to 1's due to the sensitivity and specificity rates.

###SVM HOM - Feature Importance

Creating a prediction wrapper function where the reference class of interest is chosen.
Evaluating whether it is preferable to focus on the employees having 
- low scores on HighratingOverallBinary = 0
- high scores on HighratingOverallBinary= 1
From a business perspective, the employees being unsatisfied is the ones we want to focus on at first: Choose 0 as reference
```{r Feature Importance - reference class}
prob_0 <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "0"]
}
```

Variance Importance Plot 
```{r VIP plot}
#install.packages("DALEX")
library(DALEX)
library(iml)
library(vip)
set.seed(123)
vip(churn_svm_auc,
    method = "permute", 
    nsim = 5, 
    train = subtrain, 
    target = "HighratingOverallBinary",
    metric = yardstick::roc_auc,
    metric_options = list(smaller_is_better = F),
    #metric = "auc", 
    reference_class = "0", 
    pred_wrapper = prob_0)
```

```{r}
importance <- varImp(churn_svm_auc)  # 'scale = TRUE' scales the importance scores

# Print the importance
print(importance)
# Plotting variable importance
plot(importance)
```

### SVM HOM - PDP 

use the pdp package to construct PDPs for the top four features according to the permutation-based (ombytning af elementer i en given mængde/kombinationsmulighed/omplacering) variable importance scores (notice we set prob = TRUE in the call to pdp::partial() so that the feature effect plots are on the probability scale; see ?pdp::partial for details). 
Additionally, since the predicted probabilities from our model come in two columns (No and Yes), we specify which.class = 2 so that our interpretation is in reference to predicting Yes:
```{r}
#install.packages("pdp")
library(pdp)
#construct PDP (feature effect plots are on the probability scale)
features <- c("ratingRecommendToFriend", "ratingWorkLifeBalance", 
              "ratingSeniorLeadership", "ratingCultureAndValues")
pdps <- lapply(features, function(x) {
  partial(churn_svm_auc, 
          pred.var = x, 
          which.class = 2, # since the predicted probabilities from our model come in two columns (No=1 and Yes=2), we specify which.class = 1 so that our interpretation is in reference to predicting No
          prob = TRUE, 
          plot = TRUE, 
          plot.engine = "ggplot2") +
    coord_flip()
})
grid.arrange(grobs = pdps,  ncol = 2)
# interpret
```


#Moving Beyond Linearity

 - go to empNumeric for GAM logistic regression

##LauB GLM Generalized Linear Model

```{r}
dim(subtrain)
dim(subtest)
glimpse(subtrain)
```


Define train CV
```{r Define train CV}
ctrl <- trainControl(method = "cv", number = 5, savePredictions = "final")
```



```{r Fit GLM }
glm_model <- train(HighratingOverallBinary ~ ., data = subtrain, method = "glm", trControl = ctrl)
```
Function Call: Unfortunately, the function call details are marked as NULL, which typically indicates missing output or that the original call wasn't captured.
MIGHT CONVERT INTO NUMERIC TO GET SOMETHING?
```{r Summary GLM }
summary(glm_model)
```

```{r predictions GLM}
# Make predictions
pred_glm <- predict(glm_model, newdata = subtest)

# Evaluate the models' performance
confusionMatrix(pred_glm, subtest$HighratingOverallBinary)
```

## HOM 7.6 MARS: Multivariate Additive Regression Spline

The MARS method and algorithm can be extended to handle classification problems and GLMs in general.

```{r datasplit}
dim(subtrain)
dim(subtest)
glimpse(subtrain)
```

Creating hypergrid
Tuning hyperparametrs:
 - the maximum degree of interactions (degree)  
 - the number of terms (i.e., hinge functions determined by the optimal number of knots across all features) (nprune)
 - performing a grid search to identify the optimal combination of hyperparameters that minimize the cv prediction error
```{r Hyper grid}
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)
hyper_grid
```



```{r CV model}
library(earth)
# cross validated model
tuned_mars <- train(
  x = subset(subtrain, select = -ratingOverall),
  y = subtrain$ratingOverall,
  method = "earth",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)
```

```{r besttune MARS}
tuned_mars$bestTune
tuned_mars$results
```

```{r Plot CV on Degree//Terms}
ggplot(tuned_mars)
```
The besttune and plot shows:
Maximum degree of interactions (degree)
- the maximum degree for optimizing accuracy is 3 degrees/interactions

Number of terms (i.e., hinge functions determined by the optimal number of knots across all features) (nprune)
- the number of terms optimizing the accuracy are 23









#From Classification_2024

```{r Ralles draft1}
#Tree model
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(ratingOverall ~., job_train,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model
```


```{r Ralles draft1 - Tree}
library(caTools)
real.pred <- job_test$ratingOverall 
tree.class.pred <- predict(tree.model, 
                           job_test, type = "raw") 
tree.scoring <- predict(tree.model, 
                        job_test, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 
tree.auc
```
Output shows the distinction between how the different classes are predicted in comparison to each other

```{r Ch 8_Lab Rcode.R}
library(tree)
# recode sales as a binary var and incorporate to the dataset
#High=ifelse(Carseats$Sales<=8,"0","1")
#Carseats=data.frame(Carseats,High)
#str(Carseats)
#Carseats$High = as.factor(Carseats$High)

# set a classsification tree
tree.carseats = tree(High ~ .-Sales,Carseats, split = c("deviance", "gini"))
summary(tree.carseats)


# plot tree, disply labels (text) and category names (pretty)
plot(tree.carseats)
text(tree.carseats,pretty=0)

# read the tree
tree.carseats

# estimate the test error 
RNGkind("L'Ecuyer-CMRG")
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuraccy: (87+56)/200 = 0.71; test error = 0.29
# it you get slightly different results, it is because of the split

# pruning the tree
RNGkind("L'Ecuyer-CMRG")
set.seed(3)
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass) # FUN: classsification error rate guides the cross-validation and pruning process; default: deviance 
names(cv.carseats)
cv.carseats
# in the output, despite the name, $dev is the cross-validation error rate
# size = numer of terminal nodes of each tree considered
# $k = cost-complexity parameter (alpha in the slides)

par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# the trees with 19 terminal nodes results in the lowest cross-validation error rate.
# this is the most complex model;  
# as an example, if we wish to prune the tree 
# let us select best = 13 

# plot pruned tree
prune.carseats=prune.misclass(tree.carseats,best=13)
plot(prune.carseats)
text(prune.carseats,pretty=0)

# estimate test error 
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# accuracy: (78+62)/200 =0.7; test error rate = 0.3
# here, pruning did not improve the accuracy.

# ps. our solution is not the same as in the textbook because of the random split.
# the instability of tree solutions when working with small datasets is acknowledged.
```

```{r Tree model}
# 3) Decision trees 
# training parameters
train.param <- trainControl(method = "cv", number = 5)

# 1) basic decision tree
tune.grid <- data.frame(.maxdepth = 3:10, 
                        .mincriterion = c(.1, .2, .3, .4))
tree.model <- train(best_seller ~., train.data,
                    method = "ctree2",
                    metric = "Kappa",
                    preProcess = c("nzv", "center", "scale"),
                    trControl = train.param,
                    tuneGrid = tune.grid) 

tree.model

# confusion matrix + KAPPA 
real.pred <- test.data$best_seller 
tree.class.pred <- predict(tree.model, 
                           test.data, type = "raw") 
tree.scoring <- predict(tree.model, 
                        test.data, 
                        type = "prob") [, "1"] 
tree.conf <- confusionMatrix(data = tree.class.pred, 
                             reference = real.pred, 
                             positive = "1", 
                             mode = "prec_recall") 

# ROC and AUC
tree.auc = colAUC(tree.scoring , real.pred, plotROC = TRUE) 

modelComparison = rbind(modelComparison, 
                        data.frame(model = 'Basic Tree', 
                                   accuracy = tree.conf[[3]][[1]], 
                                   kappa = tree.conf[[3]][[2]], 
                                   precision = tree.conf[[4]][[5]], 
                                   recall_sensitivity = tree.conf[[4]][[6]], 
                                   specificity = tree.conf[[4]][[2]], 
                                   auc = tree.auc))
```


```{r Random Forest}
# 2) random forest
rf.model <- train(ratingOverall ~ ., job_train,
                  method = "rf", 
                  ntree = 1000,
                  metric = "Kappa",
                  trControl = train.param,
                  tune.grid = expand.grid(.mtry=c(1:28)))

rf.model

# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
rfmodel.class.pred <- predict(rf.model, 
                              job_test, 
                              type = "raw") 
rfmodel.scoring <- predict(rf.model, 
                           job_test, 
                           type = "prob") [, "1"] 
rfmodel.conf <- confusionMatrix(data = rfmodel.class.pred, 
                                reference = real.pred, 
                                positive = "1", 
                                mode = "prec_recall") 

# ROC and AUC
rf.auc = colAUC(rfmodel.scoring, real.pred, plotROC = TRUE)
```


```{r Random Forest}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'RF tree', 
                                   accuracy = rfmodel.conf[[3]][[1]], 
                                   kappa = rfmodel.conf[[3]][[2]], 
                                   precision = rfmodel.conf[[4]][[5]], 
                                   recall_sensitivity = rfmodel.conf[[4]][[6]], 
                                   specificity = rfmodel.conf[[4]][[2]], 
                                   auc = rf.auc))
```



```{r XGBoost }
model.xgboost <- train(ratingOverall ~ ., job_train,
                       method = "xgbTree",
                       metric = "Kappa",
                       tuneGrid = expand.grid(max_depth=3:6,
                                              gamma=c(0, 1, 2, 3, 5), 
                                              eta=c(0.03, 0.06, 0.1, 0.2), 
                                              nrounds=300,
                                              subsample=0.5, 
                                              colsample_bytree=0.1, 
                                              min_child_weight = 1),
                       trControl = train.param)
model.xgboost



# confusion matrix + KAPPA 
real.pred <- job_test$ratingOverall 
xgb.class.pred <- predict(model.xgboost, 
                          job_test, 
                          type = "raw") 
xgb.scoring <- predict(model.xgboost, 
                       job_test, 
                       type = "prob") [, "1"] 
xgb.conf <- confusionMatrix(data = xgb.class.pred, 
                            reference = real.pred, 
                            positive = "1", 
                            mode = "prec_recall") 

# ROC and AUC
xgb.auc = colAUC(xgb.scoring, real.pred, plotROC = TRUE)
```

```{r XGBoost}
xgb.conf
```


```{r XGBoost}
#Save results
modelComparison = rbind(modelComparison, 
                        data.frame(model = 'xgb tree', 
                                   accuracy = xgb.conf[[3]][[1]], 
                                   kappa = xgb.conf[[3]][[2]], 
                                   precision = xgb.conf[[4]][[5]], 
                                   recall_sensitivity = xgb.conf[[4]][[6]], 
                                   specificity = xgb.conf[[4]][[2]], 
                                   auc = xgb.auc))
```

```{r SVM }
library(caret)
# Tune and fit an SVM with a radial basis kernel (C and sigma as hyperparameters)
# below we use caret’s train() function to tune and train an SVM using the radial basis 
# kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.

# Tune an SVM with radial basis kernel 
set.seed(1854)  
churn_svm <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# Plot results
ggplot(churn_svm) + theme_light()


# Print results
churn_svm$results 
# notice the default is accuracy
```


```{r SVM with trControl}
# RE-run with trContol to get the class probabilities for AUC/ROC     
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
set.seed(5628)  
churn_svm_auc <- train(
  ratingOverall ~ ., 
  data = job_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC",  # explicitly set area under ROC curve as criteria        
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(churn_svm_auc)
# interpret
```
