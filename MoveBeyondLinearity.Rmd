
# CH 7 EX. SOLUTIONS_ update

Chapter 7. Moving Beyond Linearity, ISL - Solutions to Applied Ex. 6-10

```{r packages}
#install.packages("ISLR")
library(ISLR)
attach(Wage)
library(boot)
```

## Ex. 6 Wage/Age GLM + Stepfunction
In this exercise, you will further analyze the Wage data set considered throughout this chapter.

### 6a Perform polynomial regression
a) Perform polynomial regression to predict wage using age. Use
cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.

Requires: polynomial regression of Wage ~ Age 
- select optimal degree using k-fold cross-validation method
- compare with ANOVA method 
- plot the polynomial fit to the data
```{r dataload}
Wage <- Wage
Data <- Wage
```

```{r GLM: Generalized Linear Model}
set.seed(123)
    
cv.error = rep(NA, 10) # vector 
  for (i in 1:10) {
    fit.i = glm(wage ~ poly(age, i), data=Wage) #the i is looping thrpugh 10 degrees of the GLModel
    cv.error[i] = cv.glm(Wage, fit.i, K=10)$delta[2] # function cv.glm()
  }
cv.error # MSE
which.min(cv.error)
min(cv.error)

# Plot cv error for better evaluation. We want to select the model with a small MSE but still parsimonius (påholdende, sparsommelig, nærig)
plot(1:10, cv.error, xlab="Degree", ylab="CV error", type="l", pch=20, lwd=2, ylim=c(1590, 1700))
# Include std dev. lines
min.point = min(cv.error)
sd.points = sd(cv.error)
abline(h=min.point + 0.2 * sd.points, col="red", lty="dashed")
abline(h=min.point - 0.2 * sd.points, col="red", lty="dashed")
legend("topright", "0.2-standard deviation lines", lty="dashed",col="red")
```
The minimum MSE: 1592.748 on the 10th cv-model
Alina Concl:    
- degree = 2 - generates a sig. decrease in MSE vs. degree 1; 
- degree = 3 - gives us a reasonably smaller error; 
- degree = 4 or 7 could also be an option.

AL concl:
- we might choose GLM with 3rd degree whereas it falls within the sd-lines, whereas we alwyas strive for the simplest model possible

An ANOVA test is performed to support the decision of the quantity of degrees in the GLModel
```{r ANOVA test as support for degree decision}
# ANOVA test to complement and decide
    fit.1 = lm(wage~poly(age, 1), data=Wage)
    fit.2 = lm(wage~poly(age, 2), data=Wage)
    fit.3 = lm(wage~poly(age, 3), data=Wage)
    fit.4 = lm(wage~poly(age, 4), data=Wage)
    fit.5 = lm(wage~poly(age, 5), data=Wage)
    fit.6 = lm(wage~poly(age, 6), data=Wage)
    fit.7 = lm(wage~poly(age, 7), data=Wage)
    fit.8 = lm(wage~poly(age, 8), data=Wage)
    fit.9 = lm(wage~poly(age, 9), data=Wage)
    fit.10 = lm(wage~poly(age, 10), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8, fit.9, fit.10)
# Concl: the decrease in RSS after d = 3 is not sig. => select degree=3
  
# Plot fit for polynomial model with d = 3
lm.fit = lm(wage~poly(age, 3), data=Wage) # fit
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2]) # the grid
lm.pred = predict(lm.fit, data.frame(age=age.grid), se = TRUE) #  predict
se.bands=cbind(lm.pred$fit+2*lm.pred$se.fit,lm.pred$fit-2*lm.pred$se.fit) # se
    
par(mfrow=c(1,1)) 
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-3 Polynomial",outer=T)
lines(age.grid,lm.pred$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# Concl: Ages between approx. 40-60 are related to highest wage.
```
ANOVA-output
- It is seen from the p-values, that the significance changes from the 3rd to the 4th degree of the GLModel: we decide on the 3rd degree GLModel

Plot 3rd-degree GLModel
- It is seen from the plot, that the highest wages are obtained from approx 35-40 until approx 60 years olds.

### 6b Step function
b) Fit a step function to predict wage using age, and perform crossvalidation to choose the optimal number of cuts. Make a plot of the fit obtained.

Requires: Step function of Wage ~ Age
- Cut Age, but how many cuts? Perform k-fold cv to select the number of cuts that minimizes CV error
- fit and plot the model

```{r Step function - Wage ~ Age }
all.cvs = rep(NA, 10)
  for (i in 2:10) {
    Wage$age.cut = cut(Wage$age, i)  #  cut() the variable in i intervals of equal length
    lm.fit = glm(wage ~ age.cut, data=Wage)
    all.cvs[i] = cv.glm(Wage, lm.fit, K=10)$delta[2]
  }

coef(summary(lm.fit)) # interpret the coefficients (hint: p. 269 ISL)
```
Intercept
- shows the dicrete bins of age intervals
- age.cut (24.2,30.4] goes from age 24.2 years until 30.4 years old
- age.cut baseline is the age less than 24.2 years old

Estimate
- yeilds the wage value, which this age.cut-interval is higher than the baseline
- e.g. age.cut (24.2,30.4] is earning 24.35 units of wage MORE than the baseline (age less than 24.2)


```{r Step function - Plot}
plot(2:10, all.cvs[-1], xlab="Number of cuts", ylab="CV error", type="l", pch=20, lwd=2)
# Concl: plot shows that cv error is minimum for 8 cuts

# Plot the data and fit the model
plot(wage~age, data=Wage, col="black")
agelims = range(Wage$age)
age.grid = seq(from=agelims[1], to=agelims[2])
lm.fit = glm(wage~cut(age, 8), data=Wage)
lm.pred = predict(lm.fit, data.frame(age=age.grid))
lines(age.grid, lm.pred, col="blue", lwd=3)
# Same conclusion: Ages between approx. 40-60 are related to highest wage.   
```
CVerror///NumberOfCuts Plot
- Decreasing CVerror until 8 cuts, which means creating more bins/cuts helps the model capturing more patterns and/or relationships between age and wage (improving accuracy)

StepFunction Age///Wage Plot
- showing the same as the 3rd degree GLModel: that the highest wages are obtained from approx 35-40 until approx 60 years olds.

## EX. 7 Wage/Maritl;JobClass;etc

### 7a 
The Wage data set contains a number of other features not explored in this chapter, such as marital status (maritl), job class (jobclass), and others. Explore the relationships between some of these other predictors and wage, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings.

Requires: Wage ~ Marital status + Jobclass  (IV are both categorical)
- explore data first
- fit a model and plot
```{r EDA MaritalStatus + JobClass}
summary(Wage$maritl)
summary(Wage$jobclass)
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 1,1), oma = c(0,0,4,0))
plot(Wage$maritl, Wage$wage, cex.axis=0.70)
plot(Wage$jobclass, Wage$wage, cex.axis=0.70)
```
Marital Status
- Married couple tend to earn more money on average than other groups. 
- Never married is having the lowest average wage across all categories

JobClass
- Informational jobs tend to be higher-wage than industrial jobs on average.


```{r Linear Regression: Wage///intercept+maritl }
# a) Step functions
# [NOTE: polynomials do not make sense for qualitative predictors]
fit0 = lm(wage ~1,data = Wage) #using no predictors, only the intercept. This is essentially modeling the mean of the wage
#deviance(fit0) THIS IS RSS FOR CALC OF R^2
    
fit1 = lm(wage ~ maritl, data = Wage)
#deviance(fit1) # here deviance = RSS (unexplained or residuals variance)
    
#(5222086-4858941)/5222086  # to get the explained variance
# [1] 0.06954022 (the R_squared)
summary(fit1)
```
Wage///Maritl
- output shows baseline and how much the other categopries earns more than the baseline. Be aware of the significance whereas some categories are not significant.

```{r Linear Regression: Wage///JobClass+maritlXjobClass }
fit2 = lm(wage ~ jobclass, data = Wage)
#deviance(fit2)
    
fit3 = lm(wage ~ maritl + jobclass, data = Wage)
#deviance(fit3)
summary(fit3)
```
Look into fit1 for interpretation

```{r ANOVA: compare fit1;2;3 }
anova (fit1, fit2, fit3) # only to compare fit3 with fit1 and fit2 - these models are nested in fit3
```
NEED EXPLANATION
BOOKMARK 16/5-24

```{r a) Step functions }
# To interpret first identify which is ref category
contrasts(Wage$maritl) # Never Married is the reference category
contrasts(Wage$jobclass) # Industrial is the reference category
    
# Concl: 
# Married and Divorced people seem to have a sig. higher wage than Never Married. 
# Widowed and Separated do not differ significantly from Never Married.
# Information jobs have sig. higher wages than industrial jobs.
```


```{r a) Step functions }
fit3$residuals
    plot(fit3)
    
     # OBS: The assumptions behind the model are: the errors are normally distributed 
     #      and the variance across groups is homogeneous.This is not fulfilled as shown below: 
    
                # Analysis of Variance
                  res.aov <- aov(wage ~ maritl, data = Wage)
                  summary(res.aov)
                # Compare the means 
                  TukeyHSD(res.aov)
                # Check assumptions
                  plot(res.aov, 2) # normality of residuals is rejected
                  library(car)
                  leveneTest(wage ~ maritl, data = Wage) #Variance homogeneity rejected
                # Applying Kruskal non-parametric test if assumptions not fulfilled
                  kruskal.test(wage ~ maritl, data = Wage)
                  kruskal.test(wage ~ jobclass, data = Wage)
                 
  # b) Splines cannot be run when we have categorical variables among the predictors
  # c) GAMs with a smoothing spline df=4 for age
    library(gam)
    fit = gam::gam(wage ~ maritl + jobclass + s(age, 4), data = Wage)
    deviance(fit)
    summary(fit)
  # There seems to be a nonlinear sig. effect of degree 4 of age on wage 
  # Plot
    par(mfrow=c(1,3))
    plot.Gam(fit, se=TRUE,col="blue",main="GAM with smooth splines for age")
  # Interpretation: 
    # We can visually assess the relative contribution of the variables 
    # as the graphs have the same vertical scale. 
    # Age seems to be the most important. 
    # Note the high variance for Widow, Divorce and Separated groups makes 95%CI very broad 
    # for those groups. We can also see there are few obs in these groups. 
    table(Wage$maritl)
```



# Ch 7_Lab 1 and 2 _Rcode

```{r}
# *************************************************************************
# LAB with discussions - Chapter 7 Non-linear Modeling -
# *************************************************************************
library(ISLR) 
attach(Wage)
dim(Wage)
str(Wage)
View(Wage)

# ***********************************************
# 7.8.1. Polynomial Regression and Step Functions
# ***********************************************

# ***
# Objective: Predict "Wage" based on Age.  
# Wage is a continuous (numerical) random var. Age is also continuous.
plot(age, wage) # the relationships does not seem to be linear
# We try a 4-degree polynomial linear regression lm()
# The poly() function:
   # * helps us to avoid typing long formulas with powers of age
   # * generates a basis for orthogonal polynomials terms (i.e. combinations of the original terms); 
   # * orthogonal means uncorrelated predictors. 
fit=lm(wage~poly(age,4),data=Wage)  
summary(fit)
# if raw = T, poly() uses the polynomials terms (age, age^2, age^3, age^4) - it does not create orthogonal terms. 
fit2=lm(wage ~ poly(age,4,raw=T),data=Wage)  
fit2$model # Check
summary(fit2)
# We get different coefficients using the two methods. Why? 
# Still, the predictions will be almost the same - see below.


# Predictions 
# Create a grid of values for age for which we want to predict salary  
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
# Generate the predicted values + standard error bands 
preds=predict(fit,newdata=list(age=age.grid),se=TRUE) # preds has two objects "preds$fit" and "preds$se.fit"
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
# Plot the data and the predictions 
par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0)) 
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# gam package will do this plot automatically (see later) 


# Other equivalent ways of fitting a polynomial model
fit2a = lm(wage ~ age + I(age^2) + I(age^3) + I(age^4),data=Wage) # I() is a wrapper function to protect ^ to be correctly interpreted
coef(fit2a) # the coeficients are similar to fit2 (non-orthogonal); but the predictions are the same, as we will see in a moment. 
fit2b = lm(wage ~ cbind(age,age^2,age^3,age^4),data=Wage)
coef(fit2b) # IDEM 


# Let us use fit2a to predict 
preds2a = predict(fit2a,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2a$fit)) # the maximum difference btw. predictions fit fit and fit2a is very very small
# Concl:  the fitted values obtained in either case are almost identical

# Let us use fit2b to predict 
preds2b = predict(fit2b,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2b$fit))
mean(preds$fit-preds2b$fit)
# Concl:  the fitted values obtained in either case are almost identical
# Final note: here we do not interpret each beta coefficient separately; 
# rather we are interested in the form of the overall function. How age determines salary, overall? 



# ****************************************************************************************
# When applying polynomial, we need to decide on the DEGREE of the polynomial 

# a.) One way to do this is using hypothesis tests such as Analysis-of-variance (anova), 
# where we look at the decrease in RSS between models evaluated 
# H0: is that a model M1 is sufficient to explain the data
# H1: is that a more complex model M2 is required
# anova can be used only with nested models: meaning the predictors in M1 must be a subset of the predictors in M2
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
# Concl: Model 3 (maybe 4) is sufficient. The decrease in RSS is not anymore significant at 5% for model 4 and 5
coef(summary(fit.5)) # In fit.5 model, the p-value for the age^ 5 also reflects that. 


# Including education in the model
fit.1=lm(wage~education+age,data=Wage)
fit.2=lm(wage~education+poly(age,2),data=Wage)
fit.3=lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)
# Let us conclude


# b.) We can use cross-validation (ch. 5) to select the best model degree, where we aim to choose the model with the lowest test MSE (mean square error); MSE = RSS/n.
# K = 10-fold cross-validation 
library(boot)
set.seed(19)
cv.error = rep (0, 5)
for (i in 1:5)
  {
  fit.i=glm(wage~poly(age,i),data=Wage)  # notice glm here in conjunction with cv.glm function
  cv.error[i]=cv.glm(Wage, fit.i, K=10)$delta[1]
}
cv.error # the CV errors of the five polynomials models
# Concl: A 5 order model is not justified.



# ****************************************************************************************
# Polynomial logistic regression (glm())
# Background: Plotting the wage data it seems there are two subpopulations we can distingush: 
# Low earners (<250.000) and High earners (>250.000)
# hist(wage)
# Objective: set a model to predict if an individual earns more than $250.000 per year

fit = glm(I(wage>250) ~ poly(age,4),data=Wage,family=binomial) # function I() creates automatically the binary variable Wage 
preds = predict(fit,newdata=list(age=age.grid),se=T) # predict 
preds # note the predictions; they are negative values ; they represent "logits" of wage - not probabilities - because we used the default (type = "link")
# we need to tranform logits into probabilities - see formula p. 291 ISL
pfit=exp(preds$fit)/(1+exp(preds$fit)) 
se.bands.logit = cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit) # add the the bands +-2*SE with cbind
se.bands = exp(se.bands.logit)/(1+exp(se.bands.logit)) # transform them into probabilities

# Alternatively, if we explicitly set type = "response" in the predict() function, we could directly compute the probabilities 
# preds=predict(fit,newdata=list(age=age.grid),type="response",se=T) 
# But we cannot get the 95% CI in terms of probabilities in this case.
# In the online video, the author shows another method.
# Now let us plot the predicted vs the actual (Figure 7.1 right hand from ISL) 
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,.2))
points(jitter(age), I((wage>250)/5),cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2, col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# be patient ....
# Grey marks (top and bottom) are ages of high earners and low earners; blue line represent pfit (fitted probabilities). 



# ****************************************************************************************
# Step functions 
# Discretize the independent variables and fit a constant in each bin 
table(cut(age,4)) # cut() function discretize the variable in 4 intervals of equal length 
fit = lm(wage ~ cut(age,4), data = Wage)  
# one category us the reference category ; in this case age < 33.5 is the ref category
contrasts (cut(age,4)) # ?contrasts to learn about other contrasts and how to set them. 
round(coef(summary(fit)),2)
# $94.16 is the avg. wage for the ref category; the rest are interpreted as ...see full explanation in ISL p.292



# ***********************
# 7.8.2 Splines
# ***********************
library(splines)
# a) Regression splines
# Choosing 3 knots manually: 25, 40, 60 
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage) # bs() function 
pred=predict(fit,newdata=list(age=age.grid),se=T)
# Plot
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")

# Set knots at uniform quatiles of the data by setting the degrees of freedom, df() 
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots") # the corresponding knots are 33.75, 42.0 and 51.0
# ...and re-fit the model
fit=lm(wage~bs(age,df=6),data=Wage) 
pred=predict(fit,newdata=list(age=age.grid),se=T)
# ...and plot 
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")



# b) Natural splines 
# e.g. for df = 4  (or we can specify the knots directly using the knots() function as before)
dim(ns(age,df=4))
attr(ns(age,df=4),"knots") 
fit2=lm(wage~ns(age,df=4),data=Wage)# using ns() function  
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
lines(age.grid, pred2$fit,col="red",lwd=2) # we plot on the top of the regression splines so we can compare



# c) Smoothing splines 
# they have a smoothing parameter, lamda, which can be specified using degrees of freedon (df)
fit=smooth.spline(age,wage,df=16) # we select subjectively 16 df
fit2=smooth.spline(age,wage,cv=TRUE) # the software selects automatically df by using cross validation (cv = TRUE); 
# the default in most spline software is either leave-one-out CV
fit2
# PRESS is the “prediction sum of squares”, i.e., the sum of the squared leave- one-out prediction errors.
fit2$df # selected df based on cv is 6.79 
fit2$lambda # selected lambda
# λ → ∞, having any curvature at all becomes infinitely penalized, and only linear functions are allowed
# as λ → 0, we decide that we don’t care about curvature
# we select λ by cross-validation but smooth.spline does not let us control λ directly
# check also: $x component, re-arranged in increasing order
# a $y component of fitted values, 
# a $yin component of original values, etc. 
# See help(smooth.spline) for more



# Plot 
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Smoothing Spline")
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("red","blue"),lty=1,lwd=2,cex=.8)



# d) Local linear regression with loess () 
fit=loess(wage~age,span=.2,data=Wage) # span = 0.2 meanind neighbourhood consists of 20% of the obs
fit2=loess(wage~age,span=.5,data=Wage) # span = 0.5 meanind neighbourhood consists of 50% of the obs
# ...and plot
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Local Regression")
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
# note that the larger the span the smoother the fit



# ****************************************************
# 7.8.3 GAMs model - # more than one predictor 
# ****************************************************
# Objective: to predict Wage using functions of Year and Age, and 
# treating Education as qualitative with n levels 
# (it will be converted in n-1 dummy variables)

# A GAM using a natural spline 
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
summary(gam1)

# A GAM using a smooth spline - called s() in gam library
library(gam) # make sure it is installed
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage) 
summary(gam.m3)
# plot
par(mfrow=c(1,3))
plot(gam.m3, se=TRUE,col="blue",main="GAM  using smooth splines" )

# plot also gam1
par(mfrow=c(1,3))
plot.Gam(gam1, se=TRUE, col="red", main="GAM  using natural splines") 

# Now, which model is best? Let us run two more models and use anova to compare them
gam.m1=gam(wage ~ s(age,5)+education,data=Wage) # GAM that excludes year (called, M1)
gam.m2=gam(wage ~ year+s(age,5)+education,data=Wage) # GAM that includes a linear function of year (M2)
anova(gam.m1,gam.m2,gam.m3,test="F")
# M2 is preferred. 
# A GAM with a linear function of year is better that a GAM that excludes the year. A GAM with a nonlinear function of year is not needed. 
summary(gam.m3) # looking at the p-values in the ANOVA for Nonparametric Effects, it reinforces our previous conclusion 
# Let us make predictions using predict () employing our best model 
preds=predict(gam.m2,newdata=Wage) # note here we make predictions using the actual training data set; ideally split it before training.


# Optional 
# Using GAM with local regression lo() 
#gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
#plot.Gam(gam.lo, se=TRUE, col="green")
# Using GAM with lo() and interaction : year X age
#gam.lo.i = gam(wage~lo(year,age,span=0.5)+education,data=Wage)
#library(akima) # install it first
#par(mfrow=c(1,1))
#plot(gam.lo.i)



# ****************************************************************************************** 
# Fitting a logistic regression GAM (if Y is binary)
# We fit a GAM to the wage data to predict the probability that an individual exceeds $250.000 per year 
gam.lr = gam(I(wage>250) ~ year + s(age,df=5) + education,family=binomial,data = Wage) #  a smooth splines with df = 5 for age and a step function for education  
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green") # last plot looks suspicios 
table(education,I(wage>250)) # there are no high earners in the HS category

# Refit the model using all except this category of education
gam.lr.s = gam(I(wage>250) ~ year + s(age,df=5) + education,family=binomial,data = Wage,subset=(education!="1. < HS Grad"))
plot(gam.lr.s,se=T,col="green")
# As all plots have an identical scale, we can assess the relative contributions of the 3 variables: 
# Age and education have a relatively larger effect on the probability of earning more than 250.000 per year (p. 287 ISL)


# Do we need a nonlinear term for year? 
# Use anova for comparing the previous model with a model that includes a smooth spline of year with df=4
gam.y.s = gam(I(wage>250) ~ s(year, 4) + s(age,5) + education,family=binomial,data = Wage,subset=(education!="1. < HS Grad")) 
anova(gam.lr.s,gam.y.s, test="Chisq") #  Chi-square test as Dep variable is categorical 
# We do not need a non-linear term for year.

# End 
# In addition to the textbook, there are good discussions of splines in:
# 1) Simonoff (1996, ch.5), Smoothing Methods in Statistics. Berlin: Springer- Verlag.
# 2) Hastie et al. (2009, ch.5) The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Berlin: Springer, 
#    2nd edn. URL http://www-stat.stanford.edu/~tibs/ ElemStatLearn/.
# 3) Wasserman (2006, ch.5.5). All of Nonparametric Statistics. Berlin: Springer- Verlag.
```







# Ch 7 HOM _ s

```{r}
######################################################################################################
# Multivariate Adaptive Regression Splines (MARS)
# Reference: Ch.7, HOM with R
# Trevor Hastie, Stephen Milborrow. Derived from mda:mars by 
# Trevor Hastie and Rob Tibshirani. Uses Alan Miller’s Fortran utilities with 
# Thomas Lumley’s leaps wrapper. 2019. Earth: Multivariate Adaptive Regression Splines. 
# https://CRAN.R-project.org/package=earth.
# Advantages: automating tuning process, generalized cross-validation procedure
# automatic (GCV), automated feature selection, interaction effects and feature importance 
# Last update: 30 Jan 2023
######################################################################################################

# Case description: we use the ames data (see description in ch.1) 
# problem type: supervised regression
# response variable: Sale_Price (i.e., $195,000, $215,000)
# features: 80
# observations: 2,930
# objective: use property attributes to predict the sale price of a house
# access: provided by the AmesHousing package (Kuhn 2017a)
# more details: See ?AmesHousing::ames_raw

# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting
library(rsample)     # for sample split

# Modeling packages
library(earth)     # for fitting MARS models
library(caret)     # for automating the tuning process

# Model interpretability packages
library(vip)       # for variable importance
library(pdp)       # for variable relationships


# Stratified sampling with the rsample package
ames <- AmesHousing::make_ames()
set.seed(123)
split <- initial_split(ames, prop = 0.7, 
                       strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)


# Fit a basic MARS model
mars1 <- earth(
  Sale_Price ~ .,  
  data = ames_train   
)

# Print model summary
print(mars1)
# Discuss output
# at this stage pruning is based only on an approximation of cv model 
# performance on the training data (based on an expected change 
# in R-sq of less than 0.001)

# check out the first 10 coefficient terms
summary(mars1) %>% .$coefficients %>% head(10)

# plot
plot(mars1, which = 1)
# Discuss output




# Fit a basic MARS model with interactions (see degree = 2)
mars2 <- earth(
  Sale_Price ~ .,  
  data = ames_train,
  degree = 2
)

# check out the first 10 coefficient terms
summary(mars2) %>% .$coefficients %>% head(10)


# Tuning hyperparametrs:
 # the maximum degree of interactions (degree)  
 # the number of terms (i.e., hinge functions determined by the 
 # optimal number of knots across all features) (nprune)
 # we perform a grid search to identify the optimal combination of 
 # hyperparameters that minimize the cv prediction error (cv-RMSE)

hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)
hyper_grid


# Cross-validated model (it takes 5 min to compute)
set.seed(123) 
cv_mars <- train(
  x = subset(ames_train, select = -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# View results
cv_mars$bestTune
# interpret

cv_mars$results %>%
  filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)
# interpret

ggplot(cv_mars)
# interpret

# Note: optimal nrpune around 56. As a next step, we could perform a grid search 
# that focuses in on a refined grid space for nprune (e.g., comparing 45–65 terms retained).

          hyper_grid_beta <- expand.grid(
            degree = 1:3, 
            nprune = 45:50
          )
          hyper_grid
          
          set.seed(123) 
          cv_mars_beta <- train(
            x = subset(ames_train, select = -Sale_Price),
            y = ames_train$Sale_Price,
            method = "earth",
            metric = "RMSE",
            trControl = trainControl(method = "cv", number = 10),
            tuneGrid = hyper_grid_beta
          )


cv_mars$resample
# interpret

# Variable importance plots
p1 <- vip(cv_mars, num_features = 40, geom = "point", value = "gcv") + ggtitle("GCV")
p2 <- vip(cv_mars, num_features = 40, geom = "point", value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, ncol = 2)
# interpret variable importance based on impact to GCV (left) and RSS (right)
# note: it measures the importance of original feature; it does not measure the impact 
# for particular hinge functions created for a given feature

# see coef.
cv_mars$finalModel %>% coef()
# filter for interaction terms (if any)
cv_mars$finalModel %>%
  coef() %>%  
  broom::tidy() %>%  
  filter(stringr::str_detect(names, "\\*")) 
# if interactions are not identified, then => A tibble: 0 × 2
# if interactions identified, it is difficult to give meaning 
# to the numbers because the coef. depend on the scale


# use partial dependence plots (PDPs) to better understand them
  # PDPs for individual feature
  # PDPs for two features to understand the interactions 

# Construct partial dependence plots
p1 <- partial(cv_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% 
  autoplot()
p2 <- partial(cv_mars, pred.var = "Year_Built", grid.resolution = 10) %>% 
  autoplot()
p3 <- partial(cv_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), 
              grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, 
              screen = list(z = -20, x = -60))

# Display plots side by side
gridExtra::grid.arrange(p1, p2,  ncol = 2)
gridExtra::grid.arrange(p3, ncol = 1)


######################################################################################################
# MARS with binary outcome.
# Consider the dataset attrition, where DV is categorical.
# Implement a similar code to predict employee attrition. 
# Case description: 
# problem type: supervised binomial classification
# response variable: Attrition (i.e., “Yes”, “No”)
# features: 30
# observations: 1,470
# objective: use employee attributes to predict if they will attrit (leave the company)
# access: provided by the rsample package (Kuhn and Wickham 2019)
# more details: See ?rsample::attrition
######################################################################################################

# access data
library(modeldata)
data(attrition)
str(attrition)

set.seed(123)
churn_split <- initial_split(attrition, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
```


