---
title: "SVM"
output: html_document
date: "2024-03-01"
---

##Ch9.CaseStudySVM_update


# CaseStudy 9.10 Predict Student Admission

Background
Admission to the university is an important topic. Effective university admission prediction is needed to help students choose the right university college and assure their tertiary education's performance. How a student chooses a university, and conversely how a university chooses a student, determines both sides' success in carrying through the education. However, due to the vast number of students required to attend the university every year, this decision-making process became a complex problem. Universities admissions are faced annually with a tremendous quantity of student applicants—the size of the applicant pool taxes the admissions staff's resources. Therefore, university admission prediction methods are used for categorizing student applicants (Ragab et al., 2012, 12th International Conference of Intelligent Systems Design and Applications).

Case study (Business Understanding Phase)
Suppose you are the university department administrator, and you want to determine each applicant's chance of admission based on their results on two exams. To address this prediction problem, you collect historical data from previous applicants you can use as a training set for the support vector machine model. For each training example, you have the applicant's scores on two exams and the admissions decision. Your task is to build a classification model that estimates an applicant's probability of admission based on those two exams' scores.


The data (Data Understanding Phase)
There is one dataset for this problem. The dataset (Admission.csv) consists of data for n = 100 observations (students). Each observation contains information on three variables:
- The two independent variables (V1 and V2) are numerical values of two different undergraduate grades
- The dependent variable (V3) is an integer, representing the admission decision (0=rejected; 1=accepted)
Our study's primary goal is to correctly predict whether students will be admitted or not at the university. This means that we are facing a predictive data mining task.
```{r}
#install.packages("dplyr")
```


```{r dataload}
library(dplyr)
library(readr)
Admission <- read_csv("C:/ML1ny/ML2/Admission.csv")
#View(Admission)
data1 <- Admission

glimpse(Admission)
```

Task 2: Inspect your data and do the required variable adaptations and transformations 
```{r data summary}
#  * Support Vector models have no distributional assumptions
#  * Outliers do not (generally) influence the SVM models
#  * Data exploration is done 
#      - to identify possible errors in the data 
#      - to adapt the variables type
#      - to evaluate and treat the missing values
#      - to better understand the data

# renaming the variables (optional)
# library(tidyverse)
# Admission <- Admission %>% 
#              rename(Grade1 = V1, Grade2 = V2, DV = V3)
summary(Admission)
options(repr.plot.width=4, repr.plot.height=4)
plot_histogram(Admission)
```

```{r Transform V3 into factor}
# encode the reponse as a factor variable
Admission$V3=as.factor(Admission$V3)
plot_bar(Admission)
levels(Admission$V3) <- c("Yes", "No") # svm does not accept 0/1 labels
# concl: this data is not too unbalanced 
# if the distribution of classes (0/1) is extremely unbalanced, there are statistical methods that can be used to improve the data and directly the algorithm performance (see Supplementary below)
```
Supplementary information on handling unbalanced datasets
- it applies to all classifiers 
- there are some alternatives to handle the unbalanced datasets
- they are available in the library ROSE 
- There are two main functions: ROSE and ovun.sample. 
- ROSE is based on bootstrapping
- Ovun.sample is the one I am referring to in particular. The parameter “method”  allows to do:

 1) OVER-SAMPLING the minority class: generating synthetic data that tries to randomly generate a sample of the attributes from observations in the minority class.
 2) UNDER-SAMPLING the majority class: randomly delete some of the observations from the majority class in order to match the numbers with the minority class
 3) BOTH: a combination of over- and under- sampling (method=”both”)
References about this topic: https://cran.r-project.org/web/packages/ROSE/ROSE.pdf
Lunardon, N., Menardi, G., and Torelli, N. (2014). ROSE: a Package for Binary Imbalanced Learning. R Journal, 6:82--92.
Menardi, G. and Torelli, N. (2014). Training and assessing classification rules with imbalanced data. Data Mining and Knowledge Discovery, 28:92--122.

```{r Missings}
# evaluate the missing data
library(DataExplorer)
plot_missing(Admission)
```
No missing values in this dataset
For many other exploratory techniques, including dealing with missing values, consider the previous case studies discussed.

Task 3: Build one or several predictive models and evaluate their performance 
The main goal of this case study is to obtain accurate predictions for a student being admitted or rejected at the university. Given that the DV is categorical, we are facing a classification task. In a data mining project, we would try to compare several classifiers. For the purpose of this exercise, we focus on support vector methods.

```{r DV Plot}
library(ggplot2)
qplot(
  x = V1,
  y = V2,
  data = Admission,
  color = V3 # color by factor 
)
```
The output shows that a linear classifier might be sufficient
This plot is handy when working with only two predictors
Next we run a Support Vector Classifier with kernel=linear

```{r Train Testset}
# Splitting into train (for tuning parameters) and test (for performance evaluation)
set.seed(1)
train <- sample(nrow(Admission), 80) #creates a random sample of row indices from the Admission dataset. nrow(Admission) gives the total number of rows in the Admission dataset, and sample() is used to randomly select 80 of these row numbers.
Admission.train <- Admission[train, ]
Admission.test <- Admission[-train, ]
```

```{r Linear SVM - Tuning cost parameter}
# To run a SV machines you need to establish the parameter "cost"
# use tune() to search for the best cost among a range of possible values
library (e1071)
set.seed(1)
tune.linear <- tune(svm, V3 ~ ., data = Admission.train, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.linear)
```
The output shows the best cost is 0.1

```{r Fit Linear SVM with best-tune}
#  train the model with the tuned cost
svm.linear <- svm(V3 ~ ., kernel = "linear", data = Admission.train, cost = tune.linear$best.parameter$cost)

# ask for plot 
plot(svm.linear, Admission.train) 

# ask for support vectors 
svm.linear$index 

# get summary info 
summary(svm.linear)
```
Observations marked with x are the support vectors
The observations that are not support vectors are indicated as circles

Model Evaluation: Several criteria to evaluate the tuned model e.g.error rate, recall, precision, AUC. These criteria should be evaluated on the testing data set 
```{r ROC AUC Evaluation}
# ROC and AUC
library(caTools) 
# re-train the same model with probability = TRUE
svm.linear <- svm(V3 ~ ., kernel = "linear", data = Admission.train, cost = tune.linear$best.parameter$cost, probability=TRUE)
test.pred = predict(svm.linear, Admission.test, probability = TRUE)
  
colAUC(attributes(test.pred)$probabilities[,1], Admission.test$V3, plotROC = TRUE) # extract the corresponding probabilities from the list 
```
The AUC=0.8571429 is close to 1, revealing that our model is very good in predicting whether a particular student will be admitted or not.

```{r Confusion Matrix}
# confusion matrix, recall, specificity
library(caret)
confusionMatrix(factor(ifelse(attributes(test.pred)$probabilities[,1] > 0.5, "Yes", "No")),factor(Admission.test$V3), positive = "Yes")
```

Task 4: Reflect on implications and recommendations.
a.) The main goal of our study was to correctly predict whether students will be admitted or not at the university. This can help the universities to select the best students and at the same time reduce the effort and time involved into evaluating the students applications.  Our model (SVM linear) managed to predict well with an overall error rate of ... when the cutoff was .... SVM linear model with a cost of 0.1 was related with a high ROC curve and an AUC = 0.85. Sensitivity or recall (1-Type error II) was ... meaning that the model is able to predict ... the Yes´s. 

b.) Discuss the seriousness of different type of errors. Which of the two types of error is less serious in this context?
- The False Positives (Type 1 error) is less serious, as these students may have been predicted a yes and got a Uni-permission but may have dropped out.
The exact cost from a False Positive may be high, as the student gets onboarded but ends up chasing another career or fails the exams.
- The False Negatives (Type2error) is more serious, as these students ended yp applying for the Uni-program but did not got permission.

Alina: Current university policy may favour more sensitive models (high true positive rate, that is, high (1-Type error II), minimizing Type error II (fn = true Yes predicted as being No)); or, more specific models (low false positive rate, that is minimizing Type I error (fp = true No predicted as being Yes)).
Perhaps in this context, minimizing the false positives (fp) would be more important because one would like to avoid that "unprepared" students get admitted to a particular program. The cost of false positives might be much higher than the price of false negatives. Accepting students who are not prepared to follow that program (i.e., fp) may increase students' abandoning the program later, failing at the exams, or pursuing a second career later). Comparatively, the cost of false negatives could be lower. The program will miss the opportunity to accept some of the "prepared" students in the program. Still, as long as this number is not too high, the society can assume this cost. 

The model is acceptable, but one can impose a higher cost for fp, if desired (see a similar discussion in "Ch 9 Solutions Ex 8_update.R") and get the required cutoff.

```{r Minimize False Positives (Type1error)}
# get the cutoff probability corresponding to the minimum total cost, given that cost.fp = 2*cost.fn 
library(ROCR)
pred = prediction(attributes(test.pred)$probabilities[,1], Admission.test$V3, label.ordering = NULL)
cost.perf = performance(pred, "cost", cost.fp = 2, cost.fn = 1) #evaluates the performance of the predictions in terms of cost, specifying that the cost of a false positive (cost.fp) is twice that of a false negative (cost.fn). This cost function is crucial in scenarios where the consequences of false positives and false negatives are different.
pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])] # identifies the cutoff probability that minimizes the total cost. pred@cutoffs[[1]] contains the possible cutoff values, and cost.perf@y.values[[1]] contains the total cost for each cutoff. which.min finds the index of the minimum cost, and this index is used to select the corresponding optimal cutoff from pred@cutoffs.

# 0.7533893 - it may be different for your run
```


```{r Test False Positives (Type1error)}
# now check 
library(caret)
confusionMatrix(factor(ifelse(attributes(test.pred)$probabilities[,1] > 0.7578759, "Yes", "No")),factor(Admission.test$V3), positive = "Yes") 
```
The minimization of the Cutoff-value gained a specificity-rate of 100%.
The model does not make any False Positives, and saves money for the Universitu due to drop-outs and University-seats being occupied by "not-ready" students.

# CaseStudy 9.10 Predict Product Quality
RMD: Ch9.CaseStudySVM_update

Background 
Quality assurance (QA) is a way of preventing mistakes and defects in manufactured products and avoiding problems when delivering products or services to customers. QA serves as the foundation for customer satisfaction and continuous improvement in all aspects of operation. During Quality Assurance (QA), each microchip goes through various tests to ensure it is functioning correctly.

Case study (Business Understanding Phase)
Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a classifier. Again, in a full DM project, you would compare several classifiers. For the purpose of this lecture, let us focus on Support Vector models.

The data (Data Understanding Phase)
There is one dataset for this problem. 
- The dataset (Microchips.csv) consists of data for n = 118 observations (microchips). Each observation contains information on 3 variables:
- The two independent variables (V1 and V2) are numerical values of two different tests of microchips
- The dependent variable (V3) is integer, representing the tester decision (0=reject; 1=accept)
In this case study, you will implement a support vector classifier to predict whether microchips from a fabrication plant passes quality assurance (QA).

```{r dataload}
library(dplyr)
library(readr)
Microchips <- read_csv("C:/ML1ny/ML2/Microchips.csv")
#View(Microchips)
data1 <- Microchips

glimpse(Microchips)
```

Task 2: Inspect your data and do the required variable adaptations and transformations 
```{r data summary}
#  * Support Vector models have no distributional assumptions
#  * Outliers do not (generally) influence the SVM models
#  * Data exploration is done 
#      - to identify possible errors in the data 
#      - to adapt the variables type
#      - to evaluate and treat the missing values
#      - to better understand the data

# renaming the variables (optional)
# library(tidyverse)
# Admission <- Admission %>% 
#              rename(Grade1 = V1, Grade2 = V2, DV = V3)
summary(Microchips)
options(repr.plot.width=4, repr.plot.height=4)
plot_histogram(Microchips)
```

```{r Transform V3 into factor}
# encode the reponse as a factor variable
Microchips$V3=as.factor(Microchips$V3)
levels(Microchips$V3) <- c("No", "Yes")
# plot it
plot_bar(Microchips) # svm does not accept 0/1 labels
# concl: this data is not too unbalanced 
# if the distribution of classes (0/1) is extremely unbalanced, there are statistical methods that can be used to improve the data and directly the algorithm performance (see Supplementary below)
```
Supplementary information on handling unbalanced datasets
- it applies to all classifiers 
- there are some alternatives to handle the unbalanced datasets
- they are available in the library ROSE 
- There are two main functions: ROSE and ovun.sample. 
- ROSE is based on bootstrapping
- Ovun.sample is the one I am referring to in particular. The parameter “method”  allows to do:

 1) OVER-SAMPLING the minority class: generating synthetic data that tries to randomly generate a sample of the attributes from observations in the minority class.
 2) UNDER-SAMPLING the majority class: randomly delete some of the observations from the majority class in order to match the numbers with the minority class
 3) BOTH: a combination of over- and under- sampling (method=”both”)
References about this topic: https://cran.r-project.org/web/packages/ROSE/ROSE.pdf
Lunardon, N., Menardi, G., and Torelli, N. (2014). ROSE: a Package for Binary Imbalanced Learning. R Journal, 6:82--92.
Menardi, G. and Torelli, N. (2014). Training and assessing classification rules with imbalanced data. Data Mining and Knowledge Discovery, 28:92--122.

```{r Missings}
# evaluate the missing data
library(DataExplorer)
plot_missing(Microchips)
```
No missing values in this dataset
For many other exploratory techniques, including dealing with missing values, consider the previous case studies discussed.

Task 3: Build one or several predictive models and evaluate their performance. For the purpose of this exercise, we focus on support vector methods.

```{r DV Plot}
library(ggplot2)
qplot(
  x = V1,
  y = V2,
  data = Microchips,
  color = V3 # color by factor 
)
```
The output suggests a nonlinear classifier
Next, we run a Support Vector Machines with kernel nonlinear; as an exercise, you may later try a polynomial kernel.

```{r Train Testset}
# Splitting into train (for tuning parameters) and test (for performance evaluation)
set.seed(1)
train <- sample(nrow(Microchips), 95) #creates a random sample of row indices from the Admission dataset. nrow(Admission) gives the total number of rows in the Admission dataset, and sample() is used to randomly select 80 of these row numbers.
Microchips.train <- Microchips[train, ]
Microchips.test <- Microchips[-train, ]
```

```{r Radial SVM - Tuning cost parameter}
# To run a SV machines you need to establish the parameter "cost" and gamma
# use tune() to search for the best cost among a range of possible values
library (e1071)
set.seed(1)
tune.nonlinear <- tune(svm, V3 ~ ., data = Microchips.train, kernel = "radial", ranges = list(cost = 10^seq(-2, 1, by = 0.25), gamma=c(0.5,1,2,3,4)))
summary(tune.nonlinear)
# Alina: best cost is 1.778279 and best gamma is 0.5
```
The output shows the best cost is 0.5623413 and gamma = 4.

###BOOKMARK

```{r Fit Linear SVM with best-tune}
#  train the model with the tuned cost
svm.linear <- svm(V3 ~ ., kernel = "linear", data = Admission.train, cost = tune.linear$best.parameter$cost)

# ask for plot 
plot(svm.linear, Admission.train) 

# ask for support vectors 
svm.linear$index 

# get summary info 
summary(svm.linear)
```
Observations marked with x are the support vectors
The observations that are not support vectors are indicated as circles

Model Evaluation: Several criteria to evaluate the tuned model e.g.error rate, recall, precision, AUC. These criteria should be evaluated on the testing data set 
```{r ROC AUC Evaluation}
# ROC and AUC
library(caTools) 
# re-train the same model with probability = TRUE
svm.linear <- svm(V3 ~ ., kernel = "linear", data = Admission.train, cost = tune.linear$best.parameter$cost, probability=TRUE)
test.pred = predict(svm.linear, Admission.test, probability = TRUE)
  
colAUC(attributes(test.pred)$probabilities[,1], Admission.test$V3, plotROC = TRUE) # extract the corresponding probabilities from the list 
```
The AUC=0.8571429 is close to 1, revealing that our model is very good in predicting whether a particular student will be admitted or not.

```{r Confusion Matrix}
# confusion matrix, recall, specificity
library(caret)
confusionMatrix(factor(ifelse(attributes(test.pred)$probabilities[,1] > 0.5, "Yes", "No")),factor(Admission.test$V3), positive = "Yes")
```

Task 4: Reflect on implications and recommendations.
a.) The main goal of our study was to correctly predict whether students will be admitted or not at the university. This can help the universities to select the best students and at the same time reduce the effort and time involved into evaluating the students applications.  Our model (SVM linear) managed to predict well with an overall error rate of ... when the cutoff was .... SVM linear model with a cost of 0.1 was related with a high ROC curve and an AUC = 0.85. Sensitivity or recall (1-Type error II) was ... meaning that the model is able to predict ... the Yes´s. 

b.) Discuss the seriousness of different type of errors. Which of the two types of error is less serious in this context?
- The False Positives (Type 1 error) is less serious, as these students may have been predicted a yes and got a Uni-permission but may have dropped out.
The exact cost from a False Positive may be high, as the student gets onboarded but ends up chasing another career or fails the exams.
- The False Negatives (Type2error) is more serious, as these students ended yp applying for the Uni-program but did not got permission.

Alina: Current university policy may favour more sensitive models (high true positive rate, that is, high (1-Type error II), minimizing Type error II (fn = true Yes predicted as being No)); or, more specific models (low false positive rate, that is minimizing Type I error (fp = true No predicted as being Yes)).
Perhaps in this context, minimizing the false positives (fp) would be more important because one would like to avoid that "unprepared" students get admitted to a particular program. The cost of false positives might be much higher than the price of false negatives. Accepting students who are not prepared to follow that program (i.e., fp) may increase students' abandoning the program later, failing at the exams, or pursuing a second career later). Comparatively, the cost of false negatives could be lower. The program will miss the opportunity to accept some of the "prepared" students in the program. Still, as long as this number is not too high, the society can assume this cost. 

The model is acceptable, but one can impose a higher cost for fp, if desired (see a similar discussion in "Ch 9 Solutions Ex 8_update.R") and get the required cutoff.

```{r Minimize False Positives (Type1error)}
# get the cutoff probability corresponding to the minimum total cost, given that cost.fp = 2*cost.fn 
library(ROCR)
pred = prediction(attributes(test.pred)$probabilities[,1], Admission.test$V3, label.ordering = NULL)
cost.perf = performance(pred, "cost", cost.fp = 2, cost.fn = 1) #evaluates the performance of the predictions in terms of cost, specifying that the cost of a false positive (cost.fp) is twice that of a false negative (cost.fn). This cost function is crucial in scenarios where the consequences of false positives and false negatives are different.
pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])] # identifies the cutoff probability that minimizes the total cost. pred@cutoffs[[1]] contains the possible cutoff values, and cost.perf@y.values[[1]] contains the total cost for each cutoff. which.min finds the index of the minimum cost, and this index is used to select the corresponding optimal cutoff from pred@cutoffs.

# 0.7533893 - it may be different for your run
```


```{r Test False Positives (Type1error)}
# now check 
library(caret)
confusionMatrix(factor(ifelse(attributes(test.pred)$probabilities[,1] > 0.7578759, "Yes", "No")),factor(Admission.test$V3), positive = "Yes") 
```


##CH 9 SOLUTIONS APPLIED EX4_7_update
```{r CH 9 SOLUTIONS APPLIED EX4_7_update }
# *************************************************************************
# # Chapter 9 SVM Solutions to Applied Ex. 4-7 
#  (with explanations AAT)
# *************************************************************************
library(ISLR) # for dataset
library(e1071) # for implementing SVMs

# Ex.4 

# We begin by creating a data set with non-linear separation between the two classes.
set.seed(1)
x <- rnorm(100)
y <- 4 * x^2 + 1 + rnorm(100)
plot(x,y)

set.seed(1)
class <- sample(100, 50)
y[class] <- y[class] + 3
y[-class] <- y[-class] - 3

plot(x[class], y[class], col = "red", xlab = "X", ylab = "Y", ylim = c(-6, 30))
points(x[-class], y[-class], col = "blue")

# Now, we fit a support vector classifier on the training data.
# create the dependent variable as -1 and 1 
z <- rep(-1, 100)
z[class] <- 1
# build my data
data <- data.frame(y = y,x = x, z = as.factor(z))


# Split the dataset
set.seed(123)
train <- sample(100, 50)
data.train <- data[train, ]
data.test <- data[-train, ]

# Apply the model
svm.linear <- svm(z ~ ., data = data.train, kernel = "linear", cost = 10)
plot(svm.linear, data.train, ylim = c(-6, 25), xlim = c(-2,1.90) )
# In the plot, points that are represented by an “x” are the support vectors
# or the points that directly affect the classification line
# The points marked with an “o” are the other points, which don’t affect the calculation of the line.

# Check prediction
table(predict = predict(svm.linear, data.train), truth = data.train$z)
# Concl: The support vector classifier makes 6 errors on the training data.
# (one may get another output if the seed is not preserved)


# Now let us fit a support vector machine with a polynomial kernel.
svm.poly <- svm(z ~ ., data = data.train, kernel = "polynomial", cost = 10)
plot(svm.poly, data.train)
table(predict = predict(svm.poly, data.train), truth = data.train$z)
#Concl: 10 errors on the training data


# Now let us fit a support vector machine with a radial kernel.
svm.rad <- svm(z ~ ., data = data.train, kernel = "radial", cost = 10)
plot(svm.rad, data.train)
table(predict = predict(svm.rad, data.train), truth = data.train$z)
#Concl: 0 training errors


# Let us evaluate the performance of the 3 models in the testing data
table(predict = predict(svm.linear, data.test), truth = data.test$z)
table(predict = predict(svm.poly, data.test), truth = data.test$z)
table(predict = predict(svm.rad, data.test), truth = data.test$z)
# Concl: linear, polynomial and radial support vector machines 
# classify respectively 7, 12 and 5 observations incorrectly. 
# in my run, radial kernel performs best on the test data.


# visualize 
print(svm.rad)
plot(svm.rad, data.test, ylim = c(-6, 25), xlim = c(-2.1,2.5)) 
# notice the incorrectly classified points that were "1"
# but classified as "-1" 


      # An alternative with package "kernlab" 
      # library(kernlab)
      # kernfit <- ksvm(z ~ ., data = data.train, kernel = 'vanilladot', C = 10) # vanilladot = linear kernel
      # table(predict = predict(kernfit, data.test), truth = data.test$z)
      # print(kernfit)
      # plot(kernfit,data=data.test, ylim = c(-6, 25), xlim = c(-2.1,2.5))
        # the color gradient that indicates how confidently a new point 
        # would be classified based on its features;
        # in this package, the support vectors are marked as filled-in points, 
        # while the classes are denoted by different shapes.
  


# Ex.5 
# LR using non-linear transformations of the features 

set.seed(1)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1 * (x1^2 - x2^2 > 0)

#b)
plot(x1, x2, col = (2-y), xlab = "X1", ylab = "X2", pch=(3-y))

#c)
logit.fit <- glm(y ~ x1 + x2, family = "binomial")
summary(logit.fit)


#d)
data <- data.frame(x1 = x1, x2 = x2, y = y) 
probs <- predict(logit.fit, data, type = "response") # consider "data" as training data
preds <- rep(0, 500) #initialize the vector with 500 observations
preds[probs > 0.50] <- 1 # cutoff at 0.50 and I assigned a label of 1 to all prediction that surpass that cutoff
plot(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = (4 - 1), pch = (3 - 1), xlab = "X1", ylab = "X2",ylim = c(-0.5, 0.5), xlim = c(-0.5,0.5) )
points(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = (4 - 0), pch = (3 - 0))
# notice the decision boundary is linear; 
# this is normal, as we have discussed that a logistic model is in essence linear. 


#e) apply a polynomial LR model
logitnl.fit <- glm(y ~ poly(x1, 2) + poly(x2, 2) + I(x1 * x2), family = "binomial")
summary(logitnl.fit)


#f)
probs <- predict(logitnl.fit, data, type = "response")
preds <- rep(0, 500)
preds[probs > 0.50] <- 1
plot(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = "red", pch = 0, xlab = "X1", ylab = "X2")
points(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = "blue", pch = 2)
# The non-linear decision boundary is similar to the true decision boundary 
# (here we know the true decision boundary because we simulated the data).


#g) apply a svm model LINEAR
data$y <- as.factor(data$y) # do not forget to set y as a factor 
svm.fit <- svm(y ~ x1 + x2, data, kernel = "linear", cost = 0.01)
plot(svm.fit, data)
preds <- predict(svm.fit, data)
table(predict = predict(svm.fit, data), truth = data$y)
# This support vector classifier (even with low cost) classifies all points to a single class. 
# svm classified all points as zeros

plot(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = "red", pch = 0, xlab = "X1", ylab = "X2")
points(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = "blue", pch = 2) 
# points is a generic function to draw a sequence of points at the specified coordinates. 
# The specified character(s) are plotted, centered at the coordinates.
# as seen, there was no "1" predicted so the nothing is visualized (svm classfied all points as zeros) 


#g) apply a svm model RADIAL
data$y <- as.factor(data$y) # do not forget to set y as a factor
svmnl.fit <- svm(y ~ x1 + x2, data, kernel = "radial", gamma = 1) # gamma can be tuned; cost is here the default
plot(svmnl.fit, data)
table(predict = predict(svmnl.fit, data), truth = data$y)
preds <- predict(svmnl.fit, data)

#plot(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = "red", pch = 0, xlab = "X1", ylab = "X2")
#points(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col ="blue" , pch = 2) 
# the non-linear decision boundary is similar to the true decision boundary.


#i) Comment on your results.

# We may conclude that:
# 1. SVM with non-linear kernel and logistic regression (LR) with interaction terms 
#    are equally very powerful for finding non-linear decision boundaries. 
# 2. SVM with linear kernel and LR without any interaction terms
#    are very bad when it comes to finding non-linear decision boundaries. 
# 3. An argument in favor of using SVM instead of LR, is that LR requires 
#    some manual tuning to find the right interaction terms, 
#    while when using SVM we only need to tune the gamma.




# Ex. 6
# a.) # We randomly generate 1000 points and scatter them across line x=y with wide margin. 
# We also create noisy points along the line. 
# These points make the classes barely separable and also shift the maximum margin classifier.

set.seed(1)
x.one <- runif(500, 0, 90)
y.one <- runif(500, x.one + 10, 100)
x.one.noise <- runif(50, 20, 80)
y.one.noise <- 5/4 * (x.one.noise - 10) + 0.1

x.zero <- runif(500, 10, 100)
y.zero <- runif(500, 0, x.zero - 10)
x.zero.noise <- runif(50, 20, 80)
y.zero.noise <- 5/4 * (x.zero.noise - 10) - 0.1

class.one <- seq(1, 550)
x <- c(x.one, x.one.noise, x.zero, x.zero.noise)
y <- c(y.one, y.one.noise, y.zero, y.zero.noise)

plot(x[class.one], y[class.one], col = "green", pch = "+", ylim = c(0, 100))
points(x[-class.one], y[-class.one], col = "orange", pch = 4)




#b) 
set.seed(2)
z <- rep(0, 1100)
z[class.one] <- 1
data <- data.frame(x = x, y = y, z = as.factor(z))
dim(data)

# tune () function is using 10-fold cross validation (default)
tune.out <- tune(svm, z ~ ., data = data, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)))
summary(tune.out)
# notice in terms of cost, the lowest cv-error is corresponding to a cost of 10000
# we may calculate and display the numbers as 
data.frame(cost = tune.out$performance$cost, misclass = tune.out$performance$error * 1100)



# c) Testing with another dataset
x.test <- runif(1000, 0, 100)
class.one <- sample(1000, 500)
y.test <- rep(NA, 1000)
# Set y > x for class.one
for (i in class.one) {
  y.test[i] <- runif(1, x.test[i], 100)
}
# set y < x for class.zero
for (i in setdiff(1:1000, class.one)) {
  y.test[i] <- runif(1, 0, x.test[i])
}
plot(x.test[class.one], y.test[class.one], col = "blue", pch = "+")
points(x.test[-class.one], y.test[-class.one], col = "red", pch = 4)

set.seed(3)
z.test <- rep(0, 1000)
z.test[class.one] <- 1
data.test <- data.frame(x = x.test, y = y.test, z = as.factor(z.test))

costs <- c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)

train.err <- rep(NA, length(costs))
test.err <- rep(NA, length(costs))
for (i in 1:length(costs)) {
  svm.fit <- svm(z ~ ., data = data, kernel = "linear", cost = costs[i])
  predtrain <- predict(svm.fit, data) # train error
  pred <- predict(svm.fit, data.test) # test error
  train.err[i] <- sum(predtrain != data$z)
  test.err[i] <- sum(pred != data.test$z)
}
options(scipen=999)
data.frame(cost = costs, misclass_train = train.err)
data.frame(cost = costs, misclass_test = test.err)



#d) We observe an overfitting phenomenon for linear kernel. 
# A large cost (meaning a narrower band in the svm() function - see below) tries to fit 
# correctly noisy-points and hence overfits the train data. 
# A small cost, however, makes a few errors on the noisy training points 
# and performs better on test data.


#  NOTE: explanation link to the above conclusion, based on the question I received in class:  
#  it is reported in the LAB section that svm() function interprets the C parameter 
#  in the opposite way than used in the theory
#  p.359: svm() uses a slightly different formulation from (9.14) and (9.25)
#   C in svm() function means the cost of a violation to the margin
#   - when C (the cost) is small, that margin will be wide
#   - when C is large, than the margin will be narrow

# in the theory, C is presented as budget (p.347). in this perspective the interpretation is reversed: 
#   - as C decreses, we are less tolerant to violations to the margin, so the margin narrows
#   - as C increses, we are more tolerant, so the margin is wider

# hope this helps.




# Ex.7 

# a) 
View(Auto)
str(Auto)
# Discretize the variable mpg into two groups
var <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto$mpglevel <- as.factor(var) # for svm library make sure dep variable is factor

Auto <- Auto[, -1] # remove the original mpg
str(Auto)

# b) 
set.seed(111)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
# Lowest cv-error is obtained for c=1;
# best performance: ~ 0.08
# one can also unselect variable "name" (factor with 304 levels)

# c) 
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
summary(tune.out)
# For a polynomial kernel, the lowest cv-error is obtained 
# for a degree of 2 and a cost of 100.
# best performance: ~ 0.31

set.seed(1)
tune.out <- tune(svm,mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
# For a radial kernel, the lowest cv-error is obtained for a gamma 0.1 and a cost of 10.
# best performance: ~0.07

# d)
# run the models with the tuned parameters (best models)
svm.linear <- svm(mpglevel ~ ., data = Auto, kernel = "linear", cost = 1)
svm.poly <- svm(mpglevel ~ ., data = Auto, kernel = "polynomial", cost = 100, degree = 2)
svm.radial <- svm(mpglevel ~ ., data = Auto, kernel = "radial", cost = 10, gamma = 0.1)

# because the models have more than two input variables, we
# need to fix 2 selected dimensions; an example:
plot(svm.linear, Auto, weight ~ acceleration)
plot(svm.poly, Auto, weight  ~ acceleration)
plot(svm.radial, Auto, weight ~ acceleration)
# for more informative plots, see HOM, ch.14
```


##Ch9. Lab - Support Vector Machines
```{r Ch9. Lab - Support Vector Machines }

#  Chapter 9 Lab: Support Vector Machines in R                 
##### using svm() funcion from e1071 library   

#Note 1: This file contains the exact code from the book with explanations        
#Note 2: The videos contain some deviations from this code + extra commands for model visualization  

# 9.6.1. Support Vector Classifier

# Generating some data. In practice, the data will be given. 

set.seed(1) 
x=matrix(rnorm(20*2), ncol=2) # set up a matrix x of 20 obs, in two classes and 2 variables 
plot(x)

y=c(rep(-1,10), rep(1,10)) # generates the y variable (-1 and + 1) (there are 10 obs in each class)
x[y==1,]=x[y==1,] + 1 # for class +1, we move the mean from 0 to 1 in each of the coordinates
plot(x, col=(3-y)) # plot x by coloring the classes
# As we can see, the obs generated are not linearly separable



# Create the data and encode the reponse as a factor variable:

dat=data.frame(x=x, y=as.factor(y))



# Fit the model, specifying the ´cost´ parameter and linear kernel

library(e1071)
svmfit = svm(y~., data=dat, kernel="linear", cost=10,scale = FALSE) # scale = FALSE means the variables are not standardized
plot(svmfit, dat) # the plot
# support vectors
svmfit$index 
# summary info
summary(svmfit)



##     * Extra code for better visualization     

# * The plot above is fine but it can be improved.
# * The svm() function in e1071 library does not explicitly output neither the coefficients nor the width of the margin 
# * This can be done manually
# * The folowing code is based on the Lab1: SVM for classification, available on Blackboard.

#    - create a grid of values for X1 and x2; we write a function for it. 
#    - it uses the function expand.grid and produces coordinates of ´n*n´ points on a lattice covering the domain of x
#    - once we have the lattice, we make a prediction at each point on the lattice
#    - we then plot the lattice, color-coded according to the classification
#    - now we can see the decision boundary
#    - the support points (points on the margin, or on the wrong side of the margin) are indexed in the $index component of the fit

make.grid = function(x, n=75){
  grange=apply(x,2,range)
  x1=seq(from=grange[1,1], to=grange[2,1], length=n)
  x2=seq(from=grange[1,2], to=grange[2,2], length=n)
  expand.grid(x.1=x1, x.2=x2)
}

xgrid=make.grid(x)
ygrid=predict(svmfit,xgrid)
plot(xgrid, col=c("red", "blue")[as.numeric(ygrid)], pch=20, cex=.2)
points(x,col=3-y, pch=19)
points(x[svmfit$index,], pch=5, cex=2) # support vectors are highlighted

#     - Extracting the linear coefficients that describe the lienar bounday to be able to build the margin 
#     - Full details on this function is available on Ch. 12 ESL "Elements of Statistical Learning

beta=drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0=svmfit$rho
plot(xgrid, col=c("red", "blue")[as.numeric(ygrid)],pch=20, cex=0.2)
points(x, col=3-y, pch=19)
points(x[svmfit$index,], pch=5, cex=2) 
abline(beta0/beta[2], -beta[1]/beta[2])
abline((beta0-1)/beta[2], -beta[1]/beta[2], lty=2)
abline((beta0+1)/beta[2], -beta[1]/beta[2], lty=2)

# the output differs a bit from the Lab1 video because of the way we generated the data. 
# Lab 2 shows an example with nonlinear kernel.


##

# We continue now with the code from the book: 

#    - Change the cost of violating the margins to 0.1 => allow wide margins

svmfit=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE)
plot(svmfit, dat)
svmfit$index

#     - Cross-validation for a range of models where we vary the cost

set.seed(1)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
# Concl: the best model is for c=0.1; we can access it by: 
bestmod = tune.out$best.model
summary(bestmod)


# Let us test the best model using a Testing data set

#   - Creating a test data
xtest=matrix(rnorm(20*2), ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
plot(xtest, col=(3-ytest)) #visualize the testing data
testdat=data.frame(x=xtest, y=as.factor(ytest))

ypred=predict(bestmod,testdat) # make predictions
table(predict=ypred, truth=testdat$y) # contingency table


#   - for cost c=0.01:

svmfit=svm(y~., data=dat, kernel="linear", cost=.01,scale=FALSE)
ypred=predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)




# Now let us consider a sit. when the classes are linearly separable

#  - Generate the data 
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)

# - We set a very high cost C, and fit the model
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit, dat)

# We can see that there are only 3 support vectors; 
# We also see there were no training errors, as the classes are separable
# BUT, we also notice the observations that are not support vectors - indicated as circles --->
# --> are very close to the decision boundary. 
# This indicates that the model might perform bad on a testing data


#  - Let us try a smaller cost (c=1) - thus allowing a wider margin

svmfit=svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit,dat)

# Concl: in this model we misclassify an observation (see the plotted figure) 
# However, we also obtain a wider margin and make use of seven support vectors
# This model might perform better than the previous one; we shall see later. 




# 9.6.2. Support Vector Machine
###########################################################################################################################

# - Generate data

set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x, col=y) # it is clear that the data is not linerly separable in the original space

train=sample(200,100) # randomly select 100 numbers out of 200 to be used for splitting the data into training and testing

# - Fit the model

svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1) #note we set kernel = radial; gamma and the cost;
plot(svmfit, dat[train,]) # the plot reveals a non-linear boundary between classes; it also reveals there are a fair number of training errors; 
summary(svmfit)

# - Let us increase the cost

svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])

# - A cross validation model to tune the parameter ´cost´ 

set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)

# Concl: Check in the output the best model is for cost=1 and gamma =2
# Now, let us use this model to make predictions in the testing data

table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newdata=dat[-train,]))
# 10 observations out of 100 are missclassfied. 



# 9.6.3. ROC Curves
###########################################################################################################################

library(ROCR)

rocplot=function(pred, truth, ...){
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf,...)}

svmfit.opt=svm(y~., data=dat[train,], kernel="radial",gamma=2, cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")

# increasing gamma to 50

svmfit.flex=svm(y~., data=dat[train,], kernel="radial",gamma=50, cost=1, decision.values=T)
fitted=attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
rocplot(fitted,dat[train,"y"],add=T,col="red")

# doing the same for testing data

fitted=attributes(predict(svmfit.opt,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")
fitted=attributes(predict(svmfit.flex,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],add=T,col="red")

# Concl: the model with gamma 2 appear to give the most accurate results. 



# 9.6.4 SVM with Multiple Classes (one-versus-one approach)
###########################################################################################################################

set.seed(1)
x=rbind(x, matrix(rnorm(50*2), ncol=2)) # adds 50 obs more with raw bind
y=c(y, rep(0,50))
x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x, y=as.factor(y))
par(mfrow=c(1,1))
plot(x,col=(y+1))
svmfit=svm(y~., data=dat, kernel="radial", cost=10, gamma=1)
plot(svmfit, dat)


# 9.6.5 Application to Gene Expression Data
###########################################################################################################################

library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain)) # a lot of features and few obs. 

# fit the svm
out=svm(y~., data=dat, kernel="linear",cost=10, scale=FALSE)
plot(out,dat) # we cannot plot this because of multiple dimensions; The plot() will only run automatically if your data= argument has exactly three columns (one of which is a response)
plot(out, dat, x.1~x.3)
summary(out)
table(out$fitted, dat$y) # check contingency table

# test the model
dat.te=data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te=predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
```


##SVM_HOM
```{r SVM_HOM }
##############################################################
# Support Vector Machines (SVM)
# Ch.14, HOM with R
# Adv: guarantee to find a global optimum; robust to outliers
# Disadv: slow to train n tall data n >> p
##############################################################
# Data: Attrition data
# Problem type: supervised binomial classification
# Response variable: Attrition (i.e., “Yes”, “No”)
# Features: 30
# Observations: 1,470
# Objective: use employee attributes to predict if they will attrit (leave the company)
# Access: provided by the rsample package (Kuhn and Wickham 2019)
# More details: See ?rsample::attrition


# Note: In the ISL, implementation using library(e1071)
# New here: implementing SVMs in library (caret)


# Helper packages
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
library(rsample)  # for data splitting
library(modeldata) # for data set Job attrition


# Modeling packages
library(caret)    # meta-engine for SVMs
library(kernlab)  # also for fitting SVMs 

# Model interpretability packages
library(pdp)      # for partial dependence plots, etc.
library(vip)      # for variable importance plots



# Access data
data(attrition)
dim(attrition)
## [1] 1470   31

head(attrition$Attrition)
prop.table(table(attrition$Attrition)) 
#No       Yes 
#0.8387755 0.1612245 # unbalanced

str(attrition) # evaluate variables' types 
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE) 
str(df)


# Create training (70%) and test (30%) sets
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = 0.7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)


# Tune and fit an SVM with a radial basis kernel (C and sigma as hyperparameters)
# below we use caret’s train() function to tune and train an SVM using the radial basis 
# kernel function with autotuning for the sigma parameter (i.e., "svmRadialSigma") and 10-fold CV.

# Tune an SVM with radial basis kernel 
set.seed(1854)  
churn_svm <- train(
  Attrition ~ ., 
  data = churn_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)


# Plot results
ggplot(churn_svm) + theme_light()


# Print results
churn_svm$results 
# notice the default is accuracy


# RE-run with trContol to get the class probabilities for AUC/ROC     
ctrl <- trainControl(
  method = "cv", 
  number = 10, 
  classProbs = TRUE,             
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
set.seed(5628)  
churn_svm_auc <- train(
  Attrition ~ ., 
  data = churn_train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  metric = "ROC",  # explicitly set area under ROC curve as criteria        
  trControl = ctrl,
  tuneLength = 10
)

confusionMatrix(churn_svm_auc)
# interpret


# Feature importance 
# Create a wrapper to retain the predicted class probabilities for the class of interest (in this case, Yes)
prob_yes <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "Yes"]
}
#variable importance plot
set.seed(2827)  # for reproducibility
vip(churn_svm_auc, method = "permute", nsim = 5, train = churn_train, 
    target = "Attrition", metric = "auc", reference_class = "Yes", 
    pred_wrapper = prob_yes)

#construct PDP (feature effect plots are on the probability scale)
features <- c("OverTime", "WorkLifeBalance", 
              "JobSatisfaction", "JobRole")
pdps <- lapply(features, function(x) {
  partial(churn_svm_auc, pred.var = x, which.class = 2,        # since the predicted probabilities from our model come in two columns (No and Yes), we specify which.class = 2 so that our interpretation is in reference to predicting Yes
          prob = TRUE, plot = TRUE, plot.engine = "ggplot2") +
    coord_flip()
})
grid.arrange(grobs = pdps,  ncol = 2)
# interpret
```



